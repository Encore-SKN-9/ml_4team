{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: lightgbm in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from lightgbm) (2.2.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from lightgbm) (1.15.1)\n"
     ]
    }
   ],
   "source": [
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn\n",
    "# !pip install sklearn\n",
    "# !pip install imbalanced-learn\n",
    "!pip install lightgbm\n",
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, precision_score, recall_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "df = pd.read_csv('./data/data.csv')\n",
    "# df.head(3)\n",
    "\n",
    "X = df.drop(['id','hire_state','b022', 'b023', 'b036', 'b038', 'b039', 'b040'],axis=1)\n",
    "y = df['hire_state']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 모델 돌려봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "의사결정나무 정확도: 0.7015742642026009 0.6918142160636387\n"
     ]
    }
   ],
   "source": [
    "# 의사결정 나무\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=0, max_depth=3)   # max_depth : 가지치기 (최대 깊이 지정)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"의사결정나무 정확도:\", dt_clf.score(X_train, y_train), dt_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 포레스트 정확도: 1.0 0.7151655119322555\n"
     ]
    }
   ],
   "source": [
    "# 랜덤 포레스트\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_features='sqrt',  # 특성의 일부만 사용\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_clf.fit(X_train, y_train)\n",
    "print(\"랜덤 포레스트 정확도:\", rf_clf.score(X_train, y_train), rf_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost 정확도: 0.7396475017111568 0.7244033872209392\n"
     ]
    }
   ],
   "source": [
    "# xgboost\n",
    "xgb_clf = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = xgb_clf.predict(X_train)\n",
    "y_pred_test = xgb_clf.predict(X_test)\n",
    "\n",
    "print(\"xgboost 정확도:\", accuracy_score(y_train, y_pred_train), accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 및 정확도 데이터\n",
    "models = ['DecisionTree', 'RandomForest', 'XGBoost']\n",
    "train_acc = [0.7016, 1.0000, 0.7375]  # Train Accuracy\n",
    "test_acc = [0.6918, 0.7208, 0.7275]   # Test Accuracy\n",
    "\n",
    "# x축 위치 설정\n",
    "x = np.arange(len(models))\n",
    "width = 0.3  # 막대 너비\n",
    "\n",
    "# 그래프 크기 설정\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# 바 그래프 추가\n",
    "plt.bar(x - width/2, train_acc, width, label='Train Accuracy', color='skyblue')\n",
    "plt.bar(x + width/2, test_acc, width, label='Test Accuracy', color='salmon')\n",
    "\n",
    "# 그래프 제목 및 라벨\n",
    "plt.title('Train vs Test Accuracy for Models')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(x, models)  # x축 눈금 설정\n",
    "plt.ylim(0, 1.05)  # y축 범위 설정\n",
    "plt.legend()\n",
    "\n",
    "# 그래프 출력\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 274,  925],\n",
       "       [ 276, 2422]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dt_clf.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred, labels=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 277,  922],\n",
       "       [ 188, 2510]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rf_clf.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred, labels=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 283,  916],\n",
       "       [ 158, 2540]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = xgb_clf.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred, labels=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# DecisionTree 예측값 기반 혼동 행렬 생성\n",
    "conf_matrix = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
    "\n",
    "# 시각화\n",
    "plt.figure(figsize=(6, 5))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', cbar=True,  # 색상 바 추가\n",
    "            xticklabels=['Predicted 0', 'Predicted 1'], \n",
    "            yticklabels=['Actual 0', 'Actual 1'])\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('DecisionTree Confusion Matrix')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70%대의 성능이 나오는 이유는 전부 y=1(취직함)으로 예측하기 때문으로 y값의 비율 차이로 발생한다고 추정됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### oversampleing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 10732, 0: 4853})\n",
      "Counter({0: 10732, 1: 10732})\n",
      "Counter({0: 10732, 1: 10732})\n"
     ]
    }
   ],
   "source": [
    "counter = Counter(y)\n",
    "print(counter)\n",
    "\n",
    "smt = SMOTE(random_state=42)\n",
    "X_new, y_new = smt.fit_resample(X, y)\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_res, y_res = ros.fit_resample(X, y)\n",
    "\n",
    "counter = Counter(y_new)\n",
    "print(counter)\n",
    "counter = Counter(y_res)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_26916\\4203188219.py:7: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.countplot(x=y, ax=axes[0], palette=\"pastel\")\n",
      "C:\\Users\\Playdata\\AppData\\Local\\Temp\\ipykernel_26916\\4203188219.py:11: FutureWarning: \n",
      "\n",
      "Passing `palette` without assigning `hue` is deprecated and will be removed in v0.14.0. Assign the `x` variable to `hue` and set `legend=False` for the same effect.\n",
      "\n",
      "  sns.countplot(x=y_res, ax=axes[1], palette=\"dark\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1800x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(18, 5))\n",
    "\n",
    "# 원본 데이터 클래스 분포\n",
    "sns.countplot(x=y, ax=axes[0], palette=\"pastel\")\n",
    "axes[0].set_title(\"Original Data\")\n",
    "\n",
    "# RandomOverSampler 적용 후 클래스 분포\n",
    "sns.countplot(x=y_res, ax=axes[1], palette=\"dark\")\n",
    "axes[1].set_title(\"After RandomOverSampler\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE는 가장 가까운 값 사이에 직성을 만들고 그 안에서 새로운 값을 뽑는 방식으로 범주형 변수에 맞지 않아서 RandomOverSampler 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6087712759348988 0.5972791651136787\n"
     ]
    }
   ],
   "source": [
    "dt_clf = DecisionTreeClassifier(random_state=0, max_depth=3)   # max_depth : 가지치기 (최대 깊이 지정)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "print(dt_clf.score(X_train, y_train), dt_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1348, 1367],\n",
       "       [ 794, 1857]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dt_clf.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred, labels=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리드서치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최적의 하이퍼 파라미터를 선택하여 높은 정확도의 모델을 만들기 위해 그리드 서치 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m params \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m12\u001b[39m,\u001b[38;5;241m13\u001b[39m,\u001b[38;5;241m14\u001b[39m,\u001b[38;5;241m15\u001b[39m,\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m17\u001b[39m,\u001b[38;5;241m18\u001b[39m,\u001b[38;5;241m19\u001b[39m,\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m21\u001b[39m,\u001b[38;5;241m22\u001b[39m,\u001b[38;5;241m23\u001b[39m,\u001b[38;5;241m24\u001b[39m,\u001b[38;5;241m25\u001b[39m,\u001b[38;5;241m26\u001b[39m]\n\u001b[0;32m      5\u001b[0m }\n\u001b[0;32m      6\u001b[0m gscv_tree \u001b[38;5;241m=\u001b[39m GridSearchCV (dt_clf, params, scoring \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, cv \u001b[38;5;241m=\u001b[39m skf)\n\u001b[1;32m----> 7\u001b[0m \u001b[43mgscv_tree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(gscv_tree\u001b[38;5;241m.\u001b[39mbest_estimator_)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# pd.DataFrame(gscv_tree.cv_results_)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\pystudy_env\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\pystudy_env\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1024\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m   1019\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m   1020\u001b[0m     )\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m-> 1024\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m   1027\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m   1028\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\pystudy_env\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1571\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1571\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\pystudy_env\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:970\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    962\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    963\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    964\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    965\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    966\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    967\u001b[0m         )\n\u001b[0;32m    968\u001b[0m     )\n\u001b[1;32m--> 970\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    971\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    972\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    973\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    974\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    975\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    976\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    977\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    978\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    979\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    980\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    981\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    982\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    984\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplitter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    985\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    986\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    989\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    990\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    991\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    992\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    993\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\pystudy_env\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     72\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     73\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     74\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     76\u001b[0m )\n\u001b[1;32m---> 77\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\pystudy_env\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\pystudy_env\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\pystudy_env\\Lib\\site-packages\\sklearn\\utils\\parallel.py:139\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\pystudy_env\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:866\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    864\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    865\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 866\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\pystudy_env\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1382\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1384\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1385\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1386\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1387\u001b[0m     )\n\u001b[0;32m   1388\u001b[0m ):\n\u001b[1;32m-> 1389\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\pystudy_env\\Lib\\site-packages\\sklearn\\tree\\_classes.py:1024\u001b[0m, in \u001b[0;36mDecisionTreeClassifier.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    994\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    995\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree classifier from the training set (X, y).\u001b[39;00m\n\u001b[0;32m    996\u001b[0m \n\u001b[0;32m    997\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1025\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1026\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1030\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\pystudy_env\\Lib\\site-packages\\sklearn\\tree\\_classes.py:257\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    251\u001b[0m check_y_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    252\u001b[0m X, y \u001b[38;5;241m=\u001b[39m validate_data(\n\u001b[0;32m    253\u001b[0m     \u001b[38;5;28mself\u001b[39m, X, y, validate_separately\u001b[38;5;241m=\u001b[39m(check_X_params, check_y_params)\n\u001b[0;32m    254\u001b[0m )\n\u001b[0;32m    256\u001b[0m missing_values_in_feature_mask \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 257\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compute_missing_values_in_feature_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    258\u001b[0m )\n\u001b[0;32m    259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(X):\n\u001b[0;32m    260\u001b[0m     X\u001b[38;5;241m.\u001b[39msort_indices()\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\pystudy_env\\Lib\\site-packages\\sklearn\\tree\\_classes.py:218\u001b[0m, in \u001b[0;36mBaseDecisionTree._compute_missing_values_in_feature_mask\u001b[1;34m(self, X, estimator_name)\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(over\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 218\u001b[0m     overall_sum \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    220\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39misfinite(overall_sum):\n\u001b[0;32m    221\u001b[0m     \u001b[38;5;66;03m# Raise a ValueError in case of the presence of an infinite element.\u001b[39;00m\n\u001b[0;32m    222\u001b[0m     _assert_all_finite_element_wise(X, xp\u001b[38;5;241m=\u001b[39mnp, allow_nan\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcommon_kwargs)\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\pystudy_env\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:2466\u001b[0m, in \u001b[0;36msum\u001b[1;34m(a, axis, dtype, out, keepdims, initial, where)\u001b[0m\n\u001b[0;32m   2463\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   2464\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\n\u001b[1;32m-> 2466\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2467\u001b[0m \u001b[43m    \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msum\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2468\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\n\u001b[0;32m   2469\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Playdata\\AppData\\Local\\anaconda3\\envs\\pystudy_env\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:86\u001b[0m, in \u001b[0;36m_wrapreduction\u001b[1;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis\u001b[38;5;241m=\u001b[39maxis, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpasskwargs)\n\u001b[1;32m---> 86\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "dt_clf = DecisionTreeClassifier(random_state=0)\n",
    "params = {\n",
    "    \"max_depth\": [12,13,14,15,16,17,18,19,20,21,22,23,24,25,26]\n",
    "}\n",
    "gscv_tree = GridSearchCV (dt_clf, params, scoring ='accuracy', cv = skf)\n",
    "gscv_tree.fit(X_train, y_train)\n",
    "print(gscv_tree.best_estimator_)\n",
    "# pd.DataFrame(gscv_tree.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.8583674990682072\n"
     ]
    }
   ],
   "source": [
    "# 랜덤 포레스트\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    random_state=42\n",
    ")\n",
    "rf_clf.fit(X_train, y_train)\n",
    "print(\"Random Forest Accuracy:\", rf_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_depth=34, n_estimators=550, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "rf_clf = RandomForestClassifier(random_state=0)\n",
    "# params = {\n",
    "#     \"max_depth\": [5,10,15,20,25,30,35],\n",
    "#     \"n_estimators\": [100,200,300,400,500,800,1000]\n",
    "# }\n",
    "params = {\n",
    "    \"max_depth\": [26,27,28,29,30,31,32,33,34],\n",
    "    \"n_estimators\": [450,500,550,600,650,700]\n",
    "}\n",
    "gscv_rf = GridSearchCV (rf_clf, params, scoring ='accuracy', cv = skf)\n",
    "gscv_rf.fit(X_train, y_train)\n",
    "print(gscv_rf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.411443</td>\n",
       "      <td>0.010121</td>\n",
       "      <td>0.020357</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 100}</td>\n",
       "      <td>0.648758</td>\n",
       "      <td>0.646894</td>\n",
       "      <td>0.643789</td>\n",
       "      <td>0.641504</td>\n",
       "      <td>0.631873</td>\n",
       "      <td>0.642564</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.792946</td>\n",
       "      <td>0.021246</td>\n",
       "      <td>0.036564</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 200}</td>\n",
       "      <td>0.648758</td>\n",
       "      <td>0.644410</td>\n",
       "      <td>0.648447</td>\n",
       "      <td>0.643057</td>\n",
       "      <td>0.634048</td>\n",
       "      <td>0.643744</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.167702</td>\n",
       "      <td>0.023728</td>\n",
       "      <td>0.054850</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 300}</td>\n",
       "      <td>0.653416</td>\n",
       "      <td>0.642547</td>\n",
       "      <td>0.648758</td>\n",
       "      <td>0.640572</td>\n",
       "      <td>0.634358</td>\n",
       "      <td>0.643930</td>\n",
       "      <td>0.006605</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.598454</td>\n",
       "      <td>0.036300</td>\n",
       "      <td>0.079807</td>\n",
       "      <td>0.010289</td>\n",
       "      <td>5</td>\n",
       "      <td>400</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 400}</td>\n",
       "      <td>0.651863</td>\n",
       "      <td>0.642547</td>\n",
       "      <td>0.646894</td>\n",
       "      <td>0.639640</td>\n",
       "      <td>0.634669</td>\n",
       "      <td>0.643123</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.003466</td>\n",
       "      <td>0.041828</td>\n",
       "      <td>0.092758</td>\n",
       "      <td>0.005912</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 500}</td>\n",
       "      <td>0.653106</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.648447</td>\n",
       "      <td>0.639329</td>\n",
       "      <td>0.632495</td>\n",
       "      <td>0.643247</td>\n",
       "      <td>0.007146</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.200616</td>\n",
       "      <td>0.073565</td>\n",
       "      <td>0.146300</td>\n",
       "      <td>0.002856</td>\n",
       "      <td>5</td>\n",
       "      <td>800</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 800}</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.643478</td>\n",
       "      <td>0.644720</td>\n",
       "      <td>0.641504</td>\n",
       "      <td>0.632495</td>\n",
       "      <td>0.642439</td>\n",
       "      <td>0.005714</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.961413</td>\n",
       "      <td>0.052801</td>\n",
       "      <td>0.178615</td>\n",
       "      <td>0.002159</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 1000}</td>\n",
       "      <td>0.648137</td>\n",
       "      <td>0.644099</td>\n",
       "      <td>0.645342</td>\n",
       "      <td>0.642746</td>\n",
       "      <td>0.631563</td>\n",
       "      <td>0.642377</td>\n",
       "      <td>0.005692</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.758636</td>\n",
       "      <td>0.036442</td>\n",
       "      <td>0.033649</td>\n",
       "      <td>0.002227</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 100}</td>\n",
       "      <td>0.708696</td>\n",
       "      <td>0.694410</td>\n",
       "      <td>0.707453</td>\n",
       "      <td>0.694315</td>\n",
       "      <td>0.685306</td>\n",
       "      <td>0.698036</td>\n",
       "      <td>0.008847</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.515033</td>\n",
       "      <td>0.138325</td>\n",
       "      <td>0.067491</td>\n",
       "      <td>0.003222</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 200}</td>\n",
       "      <td>0.704969</td>\n",
       "      <td>0.697516</td>\n",
       "      <td>0.711801</td>\n",
       "      <td>0.694936</td>\n",
       "      <td>0.694004</td>\n",
       "      <td>0.700645</td>\n",
       "      <td>0.006777</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.150137</td>\n",
       "      <td>0.072125</td>\n",
       "      <td>0.096304</td>\n",
       "      <td>0.005918</td>\n",
       "      <td>10</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 300}</td>\n",
       "      <td>0.709006</td>\n",
       "      <td>0.699068</td>\n",
       "      <td>0.713043</td>\n",
       "      <td>0.697111</td>\n",
       "      <td>0.691519</td>\n",
       "      <td>0.701950</td>\n",
       "      <td>0.007917</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.048869</td>\n",
       "      <td>0.263477</td>\n",
       "      <td>0.138487</td>\n",
       "      <td>0.006993</td>\n",
       "      <td>10</td>\n",
       "      <td>400</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 400}</td>\n",
       "      <td>0.711180</td>\n",
       "      <td>0.700932</td>\n",
       "      <td>0.708075</td>\n",
       "      <td>0.694004</td>\n",
       "      <td>0.693072</td>\n",
       "      <td>0.701453</td>\n",
       "      <td>0.007273</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.696555</td>\n",
       "      <td>0.241600</td>\n",
       "      <td>0.162404</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>10</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 500}</td>\n",
       "      <td>0.714907</td>\n",
       "      <td>0.696584</td>\n",
       "      <td>0.706522</td>\n",
       "      <td>0.696800</td>\n",
       "      <td>0.693383</td>\n",
       "      <td>0.701639</td>\n",
       "      <td>0.007962</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.440744</td>\n",
       "      <td>0.034421</td>\n",
       "      <td>0.248233</td>\n",
       "      <td>0.008386</td>\n",
       "      <td>10</td>\n",
       "      <td>800</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 800}</td>\n",
       "      <td>0.711801</td>\n",
       "      <td>0.696273</td>\n",
       "      <td>0.710248</td>\n",
       "      <td>0.697732</td>\n",
       "      <td>0.694004</td>\n",
       "      <td>0.702012</td>\n",
       "      <td>0.007470</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.673083</td>\n",
       "      <td>0.140280</td>\n",
       "      <td>0.303600</td>\n",
       "      <td>0.004587</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 1000}</td>\n",
       "      <td>0.711180</td>\n",
       "      <td>0.696894</td>\n",
       "      <td>0.710559</td>\n",
       "      <td>0.700528</td>\n",
       "      <td>0.693072</td>\n",
       "      <td>0.702447</td>\n",
       "      <td>0.007273</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.938587</td>\n",
       "      <td>0.024200</td>\n",
       "      <td>0.046389</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 15, 'n_estimators': 100}</td>\n",
       "      <td>0.790373</td>\n",
       "      <td>0.772671</td>\n",
       "      <td>0.774224</td>\n",
       "      <td>0.775707</td>\n",
       "      <td>0.782852</td>\n",
       "      <td>0.779165</td>\n",
       "      <td>0.006599</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.828865</td>\n",
       "      <td>0.008322</td>\n",
       "      <td>0.087851</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>15</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 15, 'n_estimators': 200}</td>\n",
       "      <td>0.787578</td>\n",
       "      <td>0.773292</td>\n",
       "      <td>0.776708</td>\n",
       "      <td>0.773532</td>\n",
       "      <td>0.778813</td>\n",
       "      <td>0.777985</td>\n",
       "      <td>0.005219</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.812448</td>\n",
       "      <td>0.026993</td>\n",
       "      <td>0.131371</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>15</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 15, 'n_estimators': 300}</td>\n",
       "      <td>0.788199</td>\n",
       "      <td>0.774534</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.776639</td>\n",
       "      <td>0.780056</td>\n",
       "      <td>0.780407</td>\n",
       "      <td>0.004783</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.759496</td>\n",
       "      <td>0.039398</td>\n",
       "      <td>0.183108</td>\n",
       "      <td>0.004382</td>\n",
       "      <td>15</td>\n",
       "      <td>400</td>\n",
       "      <td>{'max_depth': 15, 'n_estimators': 400}</td>\n",
       "      <td>0.788820</td>\n",
       "      <td>0.775466</td>\n",
       "      <td>0.780435</td>\n",
       "      <td>0.776639</td>\n",
       "      <td>0.778503</td>\n",
       "      <td>0.779972</td>\n",
       "      <td>0.004734</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.627345</td>\n",
       "      <td>0.063624</td>\n",
       "      <td>0.222309</td>\n",
       "      <td>0.009132</td>\n",
       "      <td>15</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 15, 'n_estimators': 500}</td>\n",
       "      <td>0.790994</td>\n",
       "      <td>0.776398</td>\n",
       "      <td>0.783230</td>\n",
       "      <td>0.775085</td>\n",
       "      <td>0.780056</td>\n",
       "      <td>0.781152</td>\n",
       "      <td>0.005688</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7.536066</td>\n",
       "      <td>0.095466</td>\n",
       "      <td>0.370397</td>\n",
       "      <td>0.006364</td>\n",
       "      <td>15</td>\n",
       "      <td>800</td>\n",
       "      <td>{'max_depth': 15, 'n_estimators': 800}</td>\n",
       "      <td>0.791304</td>\n",
       "      <td>0.777640</td>\n",
       "      <td>0.783851</td>\n",
       "      <td>0.777260</td>\n",
       "      <td>0.778192</td>\n",
       "      <td>0.781649</td>\n",
       "      <td>0.005392</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>9.456755</td>\n",
       "      <td>0.102391</td>\n",
       "      <td>0.471243</td>\n",
       "      <td>0.008593</td>\n",
       "      <td>15</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 15, 'n_estimators': 1000}</td>\n",
       "      <td>0.790062</td>\n",
       "      <td>0.776398</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.776328</td>\n",
       "      <td>0.775707</td>\n",
       "      <td>0.780842</td>\n",
       "      <td>0.005920</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.067395</td>\n",
       "      <td>0.011992</td>\n",
       "      <td>0.052982</td>\n",
       "      <td>0.002482</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 100}</td>\n",
       "      <td>0.815528</td>\n",
       "      <td>0.798758</td>\n",
       "      <td>0.807764</td>\n",
       "      <td>0.801802</td>\n",
       "      <td>0.795899</td>\n",
       "      <td>0.803950</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.114477</td>\n",
       "      <td>0.010998</td>\n",
       "      <td>0.100454</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>20</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 200}</td>\n",
       "      <td>0.819565</td>\n",
       "      <td>0.801553</td>\n",
       "      <td>0.808385</td>\n",
       "      <td>0.811432</td>\n",
       "      <td>0.801180</td>\n",
       "      <td>0.808423</td>\n",
       "      <td>0.006824</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.188295</td>\n",
       "      <td>0.027905</td>\n",
       "      <td>0.153440</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>20</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 300}</td>\n",
       "      <td>0.821118</td>\n",
       "      <td>0.802484</td>\n",
       "      <td>0.810870</td>\n",
       "      <td>0.817024</td>\n",
       "      <td>0.803976</td>\n",
       "      <td>0.811094</td>\n",
       "      <td>0.007218</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.280581</td>\n",
       "      <td>0.057216</td>\n",
       "      <td>0.202270</td>\n",
       "      <td>0.007443</td>\n",
       "      <td>20</td>\n",
       "      <td>400</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 400}</td>\n",
       "      <td>0.822360</td>\n",
       "      <td>0.806211</td>\n",
       "      <td>0.815528</td>\n",
       "      <td>0.815781</td>\n",
       "      <td>0.803044</td>\n",
       "      <td>0.812585</td>\n",
       "      <td>0.007015</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.394105</td>\n",
       "      <td>0.112039</td>\n",
       "      <td>0.251276</td>\n",
       "      <td>0.001950</td>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 500}</td>\n",
       "      <td>0.823292</td>\n",
       "      <td>0.809317</td>\n",
       "      <td>0.814596</td>\n",
       "      <td>0.814849</td>\n",
       "      <td>0.801180</td>\n",
       "      <td>0.812647</td>\n",
       "      <td>0.007274</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8.850856</td>\n",
       "      <td>0.054284</td>\n",
       "      <td>0.410372</td>\n",
       "      <td>0.008391</td>\n",
       "      <td>20</td>\n",
       "      <td>800</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 800}</td>\n",
       "      <td>0.822050</td>\n",
       "      <td>0.809938</td>\n",
       "      <td>0.818012</td>\n",
       "      <td>0.812364</td>\n",
       "      <td>0.802423</td>\n",
       "      <td>0.812957</td>\n",
       "      <td>0.006763</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>11.015884</td>\n",
       "      <td>0.124539</td>\n",
       "      <td>0.516344</td>\n",
       "      <td>0.014216</td>\n",
       "      <td>20</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 1000}</td>\n",
       "      <td>0.821739</td>\n",
       "      <td>0.808696</td>\n",
       "      <td>0.818323</td>\n",
       "      <td>0.811743</td>\n",
       "      <td>0.801802</td>\n",
       "      <td>0.812460</td>\n",
       "      <td>0.007053</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.100297</td>\n",
       "      <td>0.014386</td>\n",
       "      <td>0.056506</td>\n",
       "      <td>0.002138</td>\n",
       "      <td>25</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 25, 'n_estimators': 100}</td>\n",
       "      <td>0.816149</td>\n",
       "      <td>0.811491</td>\n",
       "      <td>0.825466</td>\n",
       "      <td>0.820441</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.816872</td>\n",
       "      <td>0.005528</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.240584</td>\n",
       "      <td>0.053113</td>\n",
       "      <td>0.108751</td>\n",
       "      <td>0.003234</td>\n",
       "      <td>25</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 25, 'n_estimators': 200}</td>\n",
       "      <td>0.820186</td>\n",
       "      <td>0.813043</td>\n",
       "      <td>0.822050</td>\n",
       "      <td>0.825412</td>\n",
       "      <td>0.811432</td>\n",
       "      <td>0.818425</td>\n",
       "      <td>0.005346</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3.374285</td>\n",
       "      <td>0.019244</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.002406</td>\n",
       "      <td>25</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 25, 'n_estimators': 300}</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.811801</td>\n",
       "      <td>0.824534</td>\n",
       "      <td>0.823237</td>\n",
       "      <td>0.811121</td>\n",
       "      <td>0.819356</td>\n",
       "      <td>0.006513</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4.516173</td>\n",
       "      <td>0.051738</td>\n",
       "      <td>0.220194</td>\n",
       "      <td>0.008681</td>\n",
       "      <td>25</td>\n",
       "      <td>400</td>\n",
       "      <td>{'max_depth': 25, 'n_estimators': 400}</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.813665</td>\n",
       "      <td>0.822050</td>\n",
       "      <td>0.821994</td>\n",
       "      <td>0.805530</td>\n",
       "      <td>0.817865</td>\n",
       "      <td>0.007375</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5.747314</td>\n",
       "      <td>0.071435</td>\n",
       "      <td>0.275002</td>\n",
       "      <td>0.002815</td>\n",
       "      <td>25</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 25, 'n_estimators': 500}</td>\n",
       "      <td>0.824845</td>\n",
       "      <td>0.811180</td>\n",
       "      <td>0.822360</td>\n",
       "      <td>0.817335</td>\n",
       "      <td>0.808015</td>\n",
       "      <td>0.816747</td>\n",
       "      <td>0.006398</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>9.305920</td>\n",
       "      <td>0.066621</td>\n",
       "      <td>0.436677</td>\n",
       "      <td>0.004358</td>\n",
       "      <td>25</td>\n",
       "      <td>800</td>\n",
       "      <td>{'max_depth': 25, 'n_estimators': 800}</td>\n",
       "      <td>0.825466</td>\n",
       "      <td>0.812422</td>\n",
       "      <td>0.822981</td>\n",
       "      <td>0.817956</td>\n",
       "      <td>0.804908</td>\n",
       "      <td>0.816747</td>\n",
       "      <td>0.007419</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>11.487637</td>\n",
       "      <td>0.154556</td>\n",
       "      <td>0.544431</td>\n",
       "      <td>0.012129</td>\n",
       "      <td>25</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 25, 'n_estimators': 1000}</td>\n",
       "      <td>0.826398</td>\n",
       "      <td>0.813975</td>\n",
       "      <td>0.822360</td>\n",
       "      <td>0.818888</td>\n",
       "      <td>0.806462</td>\n",
       "      <td>0.817616</td>\n",
       "      <td>0.006913</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.132784</td>\n",
       "      <td>0.018518</td>\n",
       "      <td>0.056671</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>30</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 100}</td>\n",
       "      <td>0.814596</td>\n",
       "      <td>0.805280</td>\n",
       "      <td>0.818323</td>\n",
       "      <td>0.819199</td>\n",
       "      <td>0.810500</td>\n",
       "      <td>0.813579</td>\n",
       "      <td>0.005167</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2.288341</td>\n",
       "      <td>0.024397</td>\n",
       "      <td>0.111456</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>30</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 200}</td>\n",
       "      <td>0.818944</td>\n",
       "      <td>0.816149</td>\n",
       "      <td>0.820497</td>\n",
       "      <td>0.822305</td>\n",
       "      <td>0.813607</td>\n",
       "      <td>0.818300</td>\n",
       "      <td>0.003096</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3.441381</td>\n",
       "      <td>0.073761</td>\n",
       "      <td>0.164334</td>\n",
       "      <td>0.003188</td>\n",
       "      <td>30</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 300}</td>\n",
       "      <td>0.827329</td>\n",
       "      <td>0.816149</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.822616</td>\n",
       "      <td>0.810500</td>\n",
       "      <td>0.819605</td>\n",
       "      <td>0.005777</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4.560962</td>\n",
       "      <td>0.068064</td>\n",
       "      <td>0.220080</td>\n",
       "      <td>0.002651</td>\n",
       "      <td>30</td>\n",
       "      <td>400</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 400}</td>\n",
       "      <td>0.826398</td>\n",
       "      <td>0.816770</td>\n",
       "      <td>0.822360</td>\n",
       "      <td>0.820441</td>\n",
       "      <td>0.812364</td>\n",
       "      <td>0.819667</td>\n",
       "      <td>0.004793</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>5.629390</td>\n",
       "      <td>0.022191</td>\n",
       "      <td>0.270758</td>\n",
       "      <td>0.005593</td>\n",
       "      <td>30</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 500}</td>\n",
       "      <td>0.827640</td>\n",
       "      <td>0.818012</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.823548</td>\n",
       "      <td>0.811432</td>\n",
       "      <td>0.820412</td>\n",
       "      <td>0.005468</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>9.229974</td>\n",
       "      <td>0.144019</td>\n",
       "      <td>0.458021</td>\n",
       "      <td>0.010275</td>\n",
       "      <td>30</td>\n",
       "      <td>800</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 800}</td>\n",
       "      <td>0.830124</td>\n",
       "      <td>0.813354</td>\n",
       "      <td>0.822981</td>\n",
       "      <td>0.824480</td>\n",
       "      <td>0.808947</td>\n",
       "      <td>0.819977</td>\n",
       "      <td>0.007717</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>11.806583</td>\n",
       "      <td>0.071645</td>\n",
       "      <td>0.574887</td>\n",
       "      <td>0.009825</td>\n",
       "      <td>30</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 1000}</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.815528</td>\n",
       "      <td>0.822981</td>\n",
       "      <td>0.823858</td>\n",
       "      <td>0.810500</td>\n",
       "      <td>0.820288</td>\n",
       "      <td>0.006435</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.157506</td>\n",
       "      <td>0.024636</td>\n",
       "      <td>0.057062</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>35</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 35, 'n_estimators': 100}</td>\n",
       "      <td>0.822360</td>\n",
       "      <td>0.810559</td>\n",
       "      <td>0.813975</td>\n",
       "      <td>0.817335</td>\n",
       "      <td>0.801491</td>\n",
       "      <td>0.813144</td>\n",
       "      <td>0.007009</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2.266299</td>\n",
       "      <td>0.015416</td>\n",
       "      <td>0.109291</td>\n",
       "      <td>0.002863</td>\n",
       "      <td>35</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 35, 'n_estimators': 200}</td>\n",
       "      <td>0.821739</td>\n",
       "      <td>0.812422</td>\n",
       "      <td>0.822360</td>\n",
       "      <td>0.821994</td>\n",
       "      <td>0.807704</td>\n",
       "      <td>0.817244</td>\n",
       "      <td>0.006053</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3.378601</td>\n",
       "      <td>0.023720</td>\n",
       "      <td>0.163736</td>\n",
       "      <td>0.003706</td>\n",
       "      <td>35</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 35, 'n_estimators': 300}</td>\n",
       "      <td>0.826398</td>\n",
       "      <td>0.812112</td>\n",
       "      <td>0.821118</td>\n",
       "      <td>0.824790</td>\n",
       "      <td>0.812675</td>\n",
       "      <td>0.819418</td>\n",
       "      <td>0.005989</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4.473022</td>\n",
       "      <td>0.012409</td>\n",
       "      <td>0.220085</td>\n",
       "      <td>0.009588</td>\n",
       "      <td>35</td>\n",
       "      <td>400</td>\n",
       "      <td>{'max_depth': 35, 'n_estimators': 400}</td>\n",
       "      <td>0.827329</td>\n",
       "      <td>0.813665</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.821994</td>\n",
       "      <td>0.814228</td>\n",
       "      <td>0.819729</td>\n",
       "      <td>0.005154</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5.645208</td>\n",
       "      <td>0.046911</td>\n",
       "      <td>0.279050</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>35</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 35, 'n_estimators': 500}</td>\n",
       "      <td>0.827640</td>\n",
       "      <td>0.814286</td>\n",
       "      <td>0.825155</td>\n",
       "      <td>0.821994</td>\n",
       "      <td>0.810500</td>\n",
       "      <td>0.819915</td>\n",
       "      <td>0.006508</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>9.163636</td>\n",
       "      <td>0.080356</td>\n",
       "      <td>0.444956</td>\n",
       "      <td>0.004991</td>\n",
       "      <td>35</td>\n",
       "      <td>800</td>\n",
       "      <td>{'max_depth': 35, 'n_estimators': 800}</td>\n",
       "      <td>0.828882</td>\n",
       "      <td>0.816149</td>\n",
       "      <td>0.823292</td>\n",
       "      <td>0.824169</td>\n",
       "      <td>0.805840</td>\n",
       "      <td>0.819666</td>\n",
       "      <td>0.008023</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>11.609316</td>\n",
       "      <td>0.147602</td>\n",
       "      <td>0.575605</td>\n",
       "      <td>0.009593</td>\n",
       "      <td>35</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 35, 'n_estimators': 1000}</td>\n",
       "      <td>0.827329</td>\n",
       "      <td>0.817081</td>\n",
       "      <td>0.823602</td>\n",
       "      <td>0.823858</td>\n",
       "      <td>0.806772</td>\n",
       "      <td>0.819729</td>\n",
       "      <td>0.007276</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.411443      0.010121         0.020357        0.000597   \n",
       "1        0.792946      0.021246         0.036564        0.002976   \n",
       "2        1.167702      0.023728         0.054850        0.001679   \n",
       "3        1.598454      0.036300         0.079807        0.010289   \n",
       "4        2.003466      0.041828         0.092758        0.005912   \n",
       "5        3.200616      0.073565         0.146300        0.002856   \n",
       "6        3.961413      0.052801         0.178615        0.002159   \n",
       "7        0.758636      0.036442         0.033649        0.002227   \n",
       "8        1.515033      0.138325         0.067491        0.003222   \n",
       "9        2.150137      0.072125         0.096304        0.005918   \n",
       "10       3.048869      0.263477         0.138487        0.006993   \n",
       "11       3.696555      0.241600         0.162404        0.014600   \n",
       "12       5.440744      0.034421         0.248233        0.008386   \n",
       "13       6.673083      0.140280         0.303600        0.004587   \n",
       "14       0.938587      0.024200         0.046389        0.001654   \n",
       "15       1.828865      0.008322         0.087851        0.002631   \n",
       "16       2.812448      0.026993         0.131371        0.001208   \n",
       "17       3.759496      0.039398         0.183108        0.004382   \n",
       "18       4.627345      0.063624         0.222309        0.009132   \n",
       "19       7.536066      0.095466         0.370397        0.006364   \n",
       "20       9.456755      0.102391         0.471243        0.008593   \n",
       "21       1.067395      0.011992         0.052982        0.002482   \n",
       "22       2.114477      0.010998         0.100454        0.000832   \n",
       "23       3.188295      0.027905         0.153440        0.002004   \n",
       "24       4.280581      0.057216         0.202270        0.007443   \n",
       "25       5.394105      0.112039         0.251276        0.001950   \n",
       "26       8.850856      0.054284         0.410372        0.008391   \n",
       "27      11.015884      0.124539         0.516344        0.014216   \n",
       "28       1.100297      0.014386         0.056506        0.002138   \n",
       "29       2.240584      0.053113         0.108751        0.003234   \n",
       "30       3.374285      0.019244         0.163400        0.002406   \n",
       "31       4.516173      0.051738         0.220194        0.008681   \n",
       "32       5.747314      0.071435         0.275002        0.002815   \n",
       "33       9.305920      0.066621         0.436677        0.004358   \n",
       "34      11.487637      0.154556         0.544431        0.012129   \n",
       "35       1.132784      0.018518         0.056671        0.000907   \n",
       "36       2.288341      0.024397         0.111456        0.001591   \n",
       "37       3.441381      0.073761         0.164334        0.003188   \n",
       "38       4.560962      0.068064         0.220080        0.002651   \n",
       "39       5.629390      0.022191         0.270758        0.005593   \n",
       "40       9.229974      0.144019         0.458021        0.010275   \n",
       "41      11.806583      0.071645         0.574887        0.009825   \n",
       "42       1.157506      0.024636         0.057062        0.001632   \n",
       "43       2.266299      0.015416         0.109291        0.002863   \n",
       "44       3.378601      0.023720         0.163736        0.003706   \n",
       "45       4.473022      0.012409         0.220085        0.009588   \n",
       "46       5.645208      0.046911         0.279050        0.002752   \n",
       "47       9.163636      0.080356         0.444956        0.004991   \n",
       "48      11.609316      0.147602         0.575605        0.009593   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "0                5                100   \n",
       "1                5                200   \n",
       "2                5                300   \n",
       "3                5                400   \n",
       "4                5                500   \n",
       "5                5                800   \n",
       "6                5               1000   \n",
       "7               10                100   \n",
       "8               10                200   \n",
       "9               10                300   \n",
       "10              10                400   \n",
       "11              10                500   \n",
       "12              10                800   \n",
       "13              10               1000   \n",
       "14              15                100   \n",
       "15              15                200   \n",
       "16              15                300   \n",
       "17              15                400   \n",
       "18              15                500   \n",
       "19              15                800   \n",
       "20              15               1000   \n",
       "21              20                100   \n",
       "22              20                200   \n",
       "23              20                300   \n",
       "24              20                400   \n",
       "25              20                500   \n",
       "26              20                800   \n",
       "27              20               1000   \n",
       "28              25                100   \n",
       "29              25                200   \n",
       "30              25                300   \n",
       "31              25                400   \n",
       "32              25                500   \n",
       "33              25                800   \n",
       "34              25               1000   \n",
       "35              30                100   \n",
       "36              30                200   \n",
       "37              30                300   \n",
       "38              30                400   \n",
       "39              30                500   \n",
       "40              30                800   \n",
       "41              30               1000   \n",
       "42              35                100   \n",
       "43              35                200   \n",
       "44              35                300   \n",
       "45              35                400   \n",
       "46              35                500   \n",
       "47              35                800   \n",
       "48              35               1000   \n",
       "\n",
       "                                     params  split0_test_score  \\\n",
       "0     {'max_depth': 5, 'n_estimators': 100}           0.648758   \n",
       "1     {'max_depth': 5, 'n_estimators': 200}           0.648758   \n",
       "2     {'max_depth': 5, 'n_estimators': 300}           0.653416   \n",
       "3     {'max_depth': 5, 'n_estimators': 400}           0.651863   \n",
       "4     {'max_depth': 5, 'n_estimators': 500}           0.653106   \n",
       "5     {'max_depth': 5, 'n_estimators': 800}           0.650000   \n",
       "6    {'max_depth': 5, 'n_estimators': 1000}           0.648137   \n",
       "7    {'max_depth': 10, 'n_estimators': 100}           0.708696   \n",
       "8    {'max_depth': 10, 'n_estimators': 200}           0.704969   \n",
       "9    {'max_depth': 10, 'n_estimators': 300}           0.709006   \n",
       "10   {'max_depth': 10, 'n_estimators': 400}           0.711180   \n",
       "11   {'max_depth': 10, 'n_estimators': 500}           0.714907   \n",
       "12   {'max_depth': 10, 'n_estimators': 800}           0.711801   \n",
       "13  {'max_depth': 10, 'n_estimators': 1000}           0.711180   \n",
       "14   {'max_depth': 15, 'n_estimators': 100}           0.790373   \n",
       "15   {'max_depth': 15, 'n_estimators': 200}           0.787578   \n",
       "16   {'max_depth': 15, 'n_estimators': 300}           0.788199   \n",
       "17   {'max_depth': 15, 'n_estimators': 400}           0.788820   \n",
       "18   {'max_depth': 15, 'n_estimators': 500}           0.790994   \n",
       "19   {'max_depth': 15, 'n_estimators': 800}           0.791304   \n",
       "20  {'max_depth': 15, 'n_estimators': 1000}           0.790062   \n",
       "21   {'max_depth': 20, 'n_estimators': 100}           0.815528   \n",
       "22   {'max_depth': 20, 'n_estimators': 200}           0.819565   \n",
       "23   {'max_depth': 20, 'n_estimators': 300}           0.821118   \n",
       "24   {'max_depth': 20, 'n_estimators': 400}           0.822360   \n",
       "25   {'max_depth': 20, 'n_estimators': 500}           0.823292   \n",
       "26   {'max_depth': 20, 'n_estimators': 800}           0.822050   \n",
       "27  {'max_depth': 20, 'n_estimators': 1000}           0.821739   \n",
       "28   {'max_depth': 25, 'n_estimators': 100}           0.816149   \n",
       "29   {'max_depth': 25, 'n_estimators': 200}           0.820186   \n",
       "30   {'max_depth': 25, 'n_estimators': 300}           0.826087   \n",
       "31   {'max_depth': 25, 'n_estimators': 400}           0.826087   \n",
       "32   {'max_depth': 25, 'n_estimators': 500}           0.824845   \n",
       "33   {'max_depth': 25, 'n_estimators': 800}           0.825466   \n",
       "34  {'max_depth': 25, 'n_estimators': 1000}           0.826398   \n",
       "35   {'max_depth': 30, 'n_estimators': 100}           0.814596   \n",
       "36   {'max_depth': 30, 'n_estimators': 200}           0.818944   \n",
       "37   {'max_depth': 30, 'n_estimators': 300}           0.827329   \n",
       "38   {'max_depth': 30, 'n_estimators': 400}           0.826398   \n",
       "39   {'max_depth': 30, 'n_estimators': 500}           0.827640   \n",
       "40   {'max_depth': 30, 'n_estimators': 800}           0.830124   \n",
       "41  {'max_depth': 30, 'n_estimators': 1000}           0.828571   \n",
       "42   {'max_depth': 35, 'n_estimators': 100}           0.822360   \n",
       "43   {'max_depth': 35, 'n_estimators': 200}           0.821739   \n",
       "44   {'max_depth': 35, 'n_estimators': 300}           0.826398   \n",
       "45   {'max_depth': 35, 'n_estimators': 400}           0.827329   \n",
       "46   {'max_depth': 35, 'n_estimators': 500}           0.827640   \n",
       "47   {'max_depth': 35, 'n_estimators': 800}           0.828882   \n",
       "48  {'max_depth': 35, 'n_estimators': 1000}           0.827329   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.646894           0.643789           0.641504   \n",
       "1            0.644410           0.648447           0.643057   \n",
       "2            0.642547           0.648758           0.640572   \n",
       "3            0.642547           0.646894           0.639640   \n",
       "4            0.642857           0.648447           0.639329   \n",
       "5            0.643478           0.644720           0.641504   \n",
       "6            0.644099           0.645342           0.642746   \n",
       "7            0.694410           0.707453           0.694315   \n",
       "8            0.697516           0.711801           0.694936   \n",
       "9            0.699068           0.713043           0.697111   \n",
       "10           0.700932           0.708075           0.694004   \n",
       "11           0.696584           0.706522           0.696800   \n",
       "12           0.696273           0.710248           0.697732   \n",
       "13           0.696894           0.710559           0.700528   \n",
       "14           0.772671           0.774224           0.775707   \n",
       "15           0.773292           0.776708           0.773532   \n",
       "16           0.774534           0.782609           0.776639   \n",
       "17           0.775466           0.780435           0.776639   \n",
       "18           0.776398           0.783230           0.775085   \n",
       "19           0.777640           0.783851           0.777260   \n",
       "20           0.776398           0.785714           0.776328   \n",
       "21           0.798758           0.807764           0.801802   \n",
       "22           0.801553           0.808385           0.811432   \n",
       "23           0.802484           0.810870           0.817024   \n",
       "24           0.806211           0.815528           0.815781   \n",
       "25           0.809317           0.814596           0.814849   \n",
       "26           0.809938           0.818012           0.812364   \n",
       "27           0.808696           0.818323           0.811743   \n",
       "28           0.811491           0.825466           0.820441   \n",
       "29           0.813043           0.822050           0.825412   \n",
       "30           0.811801           0.824534           0.823237   \n",
       "31           0.813665           0.822050           0.821994   \n",
       "32           0.811180           0.822360           0.817335   \n",
       "33           0.812422           0.822981           0.817956   \n",
       "34           0.813975           0.822360           0.818888   \n",
       "35           0.805280           0.818323           0.819199   \n",
       "36           0.816149           0.820497           0.822305   \n",
       "37           0.816149           0.821429           0.822616   \n",
       "38           0.816770           0.822360           0.820441   \n",
       "39           0.818012           0.821429           0.823548   \n",
       "40           0.813354           0.822981           0.824480   \n",
       "41           0.815528           0.822981           0.823858   \n",
       "42           0.810559           0.813975           0.817335   \n",
       "43           0.812422           0.822360           0.821994   \n",
       "44           0.812112           0.821118           0.824790   \n",
       "45           0.813665           0.821429           0.821994   \n",
       "46           0.814286           0.825155           0.821994   \n",
       "47           0.816149           0.823292           0.824169   \n",
       "48           0.817081           0.823602           0.823858   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.631873         0.642564        0.005900               47  \n",
       "1            0.634048         0.643744        0.005333               44  \n",
       "2            0.634358         0.643930        0.006605               43  \n",
       "3            0.634669         0.643123        0.005908               46  \n",
       "4            0.632495         0.643247        0.007146               45  \n",
       "5            0.632495         0.642439        0.005714               48  \n",
       "6            0.631563         0.642377        0.005692               49  \n",
       "7            0.685306         0.698036        0.008847               42  \n",
       "8            0.694004         0.700645        0.006777               41  \n",
       "9            0.691519         0.701950        0.007917               38  \n",
       "10           0.693072         0.701453        0.007273               40  \n",
       "11           0.693383         0.701639        0.007962               39  \n",
       "12           0.694004         0.702012        0.007470               37  \n",
       "13           0.693072         0.702447        0.007273               36  \n",
       "14           0.782852         0.779165        0.006599               34  \n",
       "15           0.778813         0.777985        0.005219               35  \n",
       "16           0.780056         0.780407        0.004783               32  \n",
       "17           0.778503         0.779972        0.004734               33  \n",
       "18           0.780056         0.781152        0.005688               30  \n",
       "19           0.778192         0.781649        0.005392               29  \n",
       "20           0.775707         0.780842        0.005920               31  \n",
       "21           0.795899         0.803950        0.007000               28  \n",
       "22           0.801180         0.808423        0.006824               27  \n",
       "23           0.803976         0.811094        0.007218               26  \n",
       "24           0.803044         0.812585        0.007015               24  \n",
       "25           0.801180         0.812647        0.007274               23  \n",
       "26           0.802423         0.812957        0.006763               22  \n",
       "27           0.801802         0.812460        0.007053               25  \n",
       "28           0.810811         0.816872        0.005528               17  \n",
       "29           0.811432         0.818425        0.005346               12  \n",
       "30           0.811121         0.819356        0.006513               11  \n",
       "31           0.805530         0.817865        0.007375               14  \n",
       "32           0.808015         0.816747        0.006398               18  \n",
       "33           0.804908         0.816747        0.007419               19  \n",
       "34           0.806462         0.817616        0.006913               15  \n",
       "35           0.810500         0.813579        0.005167               20  \n",
       "36           0.813607         0.818300        0.003096               13  \n",
       "37           0.810500         0.819605        0.005777                9  \n",
       "38           0.812364         0.819667        0.004793                7  \n",
       "39           0.811432         0.820412        0.005468                1  \n",
       "40           0.808947         0.819977        0.007717                3  \n",
       "41           0.810500         0.820288        0.006435                2  \n",
       "42           0.801491         0.813144        0.007009               21  \n",
       "43           0.807704         0.817244        0.006053               16  \n",
       "44           0.812675         0.819418        0.005989               10  \n",
       "45           0.814228         0.819729        0.005154                5  \n",
       "46           0.810500         0.819915        0.006508                4  \n",
       "47           0.805840         0.819666        0.008023                8  \n",
       "48           0.806772         0.819729        0.007276                6  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gscv_rf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6741210088209716\n",
      "0.6634364517331346\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.63      0.65      2715\n",
      "           1       0.65      0.70      0.67      2651\n",
      "\n",
      "    accuracy                           0.66      5366\n",
      "   macro avg       0.66      0.66      0.66      5366\n",
      "weighted avg       0.66      0.66      0.66      5366\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1707, 1008],\n",
       "       [ 798, 1853]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xgboost\n",
    "xgb_clf = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = xgb_clf.predict(X_train)\n",
    "y_pred_test = xgb_clf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_train, y_pred_train))\n",
    "print(accuracy_score(y_test, y_pred_test))\n",
    "# print(classification_report(y_train, y_pred_train))\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "confusion_matrix(y_test, y_pred_test, labels=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.05, max_bin=256, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=20, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=500,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, ...)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "xgb_clf = XGBClassifier(random_state=0)\n",
    "params = {\n",
    "    \"max_depth\": [3,5,7,10,15,20,25],\n",
    "    \"n_estimators\": [100, 300, 500],\n",
    "    'learning_rate' : [0.01,0.05,0.1]\n",
    "}\n",
    "gscv_xg = GridSearchCV (xgb_clf, params, scoring ='accuracy', cv = skf)\n",
    "gscv_xg.fit(X_train, y_train)\n",
    "print(gscv_xg.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.05, max_bin=256, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=23, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=700,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, ...)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "xgb_clf = XGBClassifier(random_state=0)\n",
    "params = {\n",
    "    \"max_depth\": [18,19,20,21,22,23],\n",
    "    \"n_estimators\": [400,450, 500,550,600,700],\n",
    "    'learning_rate' : [0.05]\n",
    "}\n",
    "gscv_xg = GridSearchCV (xgb_clf, params, scoring ='accuracy', cv = skf)\n",
    "gscv_xg.fit(X_train, y_train)\n",
    "print(gscv_xg.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.05, max_bin=256, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=23, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=800,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, ...)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "xgb_clf = XGBClassifier(random_state=0)\n",
    "params = {\n",
    "    \"max_depth\": [23,24,25],\n",
    "    \"n_estimators\": [650,700,800,1000],\n",
    "    'learning_rate' : [0.05]\n",
    "}\n",
    "gscv_xg = GridSearchCV (xgb_clf, params, scoring ='accuracy', cv = skf)\n",
    "gscv_xg.fit(X_train, y_train)\n",
    "print(gscv_xg.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.180436</td>\n",
       "      <td>0.006167</td>\n",
       "      <td>0.005074</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 3, 'n_est...</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.631677</td>\n",
       "      <td>0.622981</td>\n",
       "      <td>0.624107</td>\n",
       "      <td>0.622554</td>\n",
       "      <td>0.626351</td>\n",
       "      <td>0.003895</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.518502</td>\n",
       "      <td>0.022714</td>\n",
       "      <td>0.005011</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 3, 'n_est...</td>\n",
       "      <td>0.655901</td>\n",
       "      <td>0.644099</td>\n",
       "      <td>0.641925</td>\n",
       "      <td>0.644921</td>\n",
       "      <td>0.633737</td>\n",
       "      <td>0.644117</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.828115</td>\n",
       "      <td>0.065859</td>\n",
       "      <td>0.005504</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 3, 'n_est...</td>\n",
       "      <td>0.660870</td>\n",
       "      <td>0.653727</td>\n",
       "      <td>0.645963</td>\n",
       "      <td>0.652066</td>\n",
       "      <td>0.645853</td>\n",
       "      <td>0.651696</td>\n",
       "      <td>0.005575</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.283682</td>\n",
       "      <td>0.020203</td>\n",
       "      <td>0.005204</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 5, 'n_est...</td>\n",
       "      <td>0.655901</td>\n",
       "      <td>0.631988</td>\n",
       "      <td>0.650932</td>\n",
       "      <td>0.641504</td>\n",
       "      <td>0.637154</td>\n",
       "      <td>0.643496</td>\n",
       "      <td>0.008784</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.782597</td>\n",
       "      <td>0.035522</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 5, 'n_est...</td>\n",
       "      <td>0.666149</td>\n",
       "      <td>0.659006</td>\n",
       "      <td>0.663665</td>\n",
       "      <td>0.667599</td>\n",
       "      <td>0.659211</td>\n",
       "      <td>0.663126</td>\n",
       "      <td>0.003514</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>3.673918</td>\n",
       "      <td>0.069603</td>\n",
       "      <td>0.016322</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>300</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 20, 'n_est...</td>\n",
       "      <td>0.806522</td>\n",
       "      <td>0.806832</td>\n",
       "      <td>0.805590</td>\n",
       "      <td>0.807083</td>\n",
       "      <td>0.797142</td>\n",
       "      <td>0.804634</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5.443889</td>\n",
       "      <td>0.077368</td>\n",
       "      <td>0.024132</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 20, 'n_est...</td>\n",
       "      <td>0.807143</td>\n",
       "      <td>0.807453</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.797453</td>\n",
       "      <td>0.804199</td>\n",
       "      <td>0.003604</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1.964846</td>\n",
       "      <td>0.148565</td>\n",
       "      <td>0.008242</td>\n",
       "      <td>0.001166</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 25, 'n_est...</td>\n",
       "      <td>0.808696</td>\n",
       "      <td>0.804658</td>\n",
       "      <td>0.803106</td>\n",
       "      <td>0.800870</td>\n",
       "      <td>0.791550</td>\n",
       "      <td>0.801776</td>\n",
       "      <td>0.005716</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>4.189157</td>\n",
       "      <td>0.066287</td>\n",
       "      <td>0.018003</td>\n",
       "      <td>0.002367</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25</td>\n",
       "      <td>300</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 25, 'n_est...</td>\n",
       "      <td>0.806832</td>\n",
       "      <td>0.804037</td>\n",
       "      <td>0.801863</td>\n",
       "      <td>0.805530</td>\n",
       "      <td>0.799006</td>\n",
       "      <td>0.803454</td>\n",
       "      <td>0.002770</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>6.141532</td>\n",
       "      <td>0.097087</td>\n",
       "      <td>0.023187</td>\n",
       "      <td>0.002394</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25</td>\n",
       "      <td>500</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 25, 'n_est...</td>\n",
       "      <td>0.807453</td>\n",
       "      <td>0.802795</td>\n",
       "      <td>0.801242</td>\n",
       "      <td>0.806151</td>\n",
       "      <td>0.793725</td>\n",
       "      <td>0.802273</td>\n",
       "      <td>0.004823</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.180436      0.006167         0.005074        0.000435   \n",
       "1        0.518502      0.022714         0.005011        0.000315   \n",
       "2        0.828115      0.065859         0.005504        0.000552   \n",
       "3        0.283682      0.020203         0.005204        0.000675   \n",
       "4        0.782597      0.035522         0.005908        0.000585   \n",
       "..            ...           ...              ...             ...   \n",
       "58       3.673918      0.069603         0.016322        0.002087   \n",
       "59       5.443889      0.077368         0.024132        0.002773   \n",
       "60       1.964846      0.148565         0.008242        0.001166   \n",
       "61       4.189157      0.066287         0.018003        0.002367   \n",
       "62       6.141532      0.097087         0.023187        0.002394   \n",
       "\n",
       "   param_learning_rate param_max_depth param_n_estimators  \\\n",
       "0                 0.01               3                100   \n",
       "1                 0.01               3                300   \n",
       "2                 0.01               3                500   \n",
       "3                 0.01               5                100   \n",
       "4                 0.01               5                300   \n",
       "..                 ...             ...                ...   \n",
       "58                 0.1              20                300   \n",
       "59                 0.1              20                500   \n",
       "60                 0.1              25                100   \n",
       "61                 0.1              25                300   \n",
       "62                 0.1              25                500   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'learning_rate': 0.01, 'max_depth': 3, 'n_est...           0.630435   \n",
       "1   {'learning_rate': 0.01, 'max_depth': 3, 'n_est...           0.655901   \n",
       "2   {'learning_rate': 0.01, 'max_depth': 3, 'n_est...           0.660870   \n",
       "3   {'learning_rate': 0.01, 'max_depth': 5, 'n_est...           0.655901   \n",
       "4   {'learning_rate': 0.01, 'max_depth': 5, 'n_est...           0.666149   \n",
       "..                                                ...                ...   \n",
       "58  {'learning_rate': 0.1, 'max_depth': 20, 'n_est...           0.806522   \n",
       "59  {'learning_rate': 0.1, 'max_depth': 20, 'n_est...           0.807143   \n",
       "60  {'learning_rate': 0.1, 'max_depth': 25, 'n_est...           0.808696   \n",
       "61  {'learning_rate': 0.1, 'max_depth': 25, 'n_est...           0.806832   \n",
       "62  {'learning_rate': 0.1, 'max_depth': 25, 'n_est...           0.807453   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.631677           0.622981           0.624107   \n",
       "1            0.644099           0.641925           0.644921   \n",
       "2            0.653727           0.645963           0.652066   \n",
       "3            0.631988           0.650932           0.641504   \n",
       "4            0.659006           0.663665           0.667599   \n",
       "..                ...                ...                ...   \n",
       "58           0.806832           0.805590           0.807083   \n",
       "59           0.807453           0.804348           0.804598   \n",
       "60           0.804658           0.803106           0.800870   \n",
       "61           0.804037           0.801863           0.805530   \n",
       "62           0.802795           0.801242           0.806151   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.622554         0.626351        0.003895               63  \n",
       "1            0.633737         0.644117        0.007100               61  \n",
       "2            0.645853         0.651696        0.005575               59  \n",
       "3            0.637154         0.643496        0.008784               62  \n",
       "4            0.659211         0.663126        0.003514               54  \n",
       "..                ...              ...             ...              ...  \n",
       "58           0.797142         0.804634        0.003780                4  \n",
       "59           0.797453         0.804199        0.003604                6  \n",
       "60           0.791550         0.801776        0.005716               14  \n",
       "61           0.799006         0.803454        0.002770                8  \n",
       "62           0.793725         0.802273        0.004823               12  \n",
       "\n",
       "[63 rows x 16 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gscv_xg.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[996]\tvalid_0's binary_logloss: 0.471651\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002468 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[996]\tvalid_0's binary_logloss: 0.466149\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000766 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[979]\tvalid_0's binary_logloss: 0.455821\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000844 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[994]\tvalid_0's binary_logloss: 0.467889\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000759 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[999]\tvalid_0's binary_logloss: 0.46422\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000921 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1140]\tvalid_0's binary_logloss: 0.469779\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000808 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1306]\tvalid_0's binary_logloss: 0.460941\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000842 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1283]\tvalid_0's binary_logloss: 0.452066\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000752 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1054]\tvalid_0's binary_logloss: 0.467248\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000770 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1281]\tvalid_0's binary_logloss: 0.461524\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1140]\tvalid_0's binary_logloss: 0.469779\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002133 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1306]\tvalid_0's binary_logloss: 0.460941\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1283]\tvalid_0's binary_logloss: 0.452066\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000665 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1054]\tvalid_0's binary_logloss: 0.467248\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000813 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1281]\tvalid_0's binary_logloss: 0.461524\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000738 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[731]\tvalid_0's binary_logloss: 0.46689\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000774 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[801]\tvalid_0's binary_logloss: 0.466443\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000925 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[757]\tvalid_0's binary_logloss: 0.460734\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000674 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[739]\tvalid_0's binary_logloss: 0.469217\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000772 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[774]\tvalid_0's binary_logloss: 0.468469\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000821 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[731]\tvalid_0's binary_logloss: 0.46689\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000864 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[801]\tvalid_0's binary_logloss: 0.466443\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001075 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[757]\tvalid_0's binary_logloss: 0.460734\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001141 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[739]\tvalid_0's binary_logloss: 0.469217\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[774]\tvalid_0's binary_logloss: 0.468469\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000809 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[731]\tvalid_0's binary_logloss: 0.46689\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000935 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[801]\tvalid_0's binary_logloss: 0.466443\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000795 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[757]\tvalid_0's binary_logloss: 0.460734\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[739]\tvalid_0's binary_logloss: 0.469217\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[774]\tvalid_0's binary_logloss: 0.468469\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[626]\tvalid_0's binary_logloss: 0.473792\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001147 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[573]\tvalid_0's binary_logloss: 0.474005\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001057 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[662]\tvalid_0's binary_logloss: 0.465093\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000799 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[576]\tvalid_0's binary_logloss: 0.473279\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000759 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[557]\tvalid_0's binary_logloss: 0.47704\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000691 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[626]\tvalid_0's binary_logloss: 0.473792\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000873 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[573]\tvalid_0's binary_logloss: 0.474005\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000863 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[662]\tvalid_0's binary_logloss: 0.465093\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000845 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[576]\tvalid_0's binary_logloss: 0.473279\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001527 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[557]\tvalid_0's binary_logloss: 0.47704\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[626]\tvalid_0's binary_logloss: 0.473792\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000831 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[573]\tvalid_0's binary_logloss: 0.474005\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000933 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[662]\tvalid_0's binary_logloss: 0.465093\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000970 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[576]\tvalid_0's binary_logloss: 0.473279\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000766 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[557]\tvalid_0's binary_logloss: 0.47704\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000843 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[186]\tvalid_0's binary_logloss: 0.522903\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[254]\tvalid_0's binary_logloss: 0.523243\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[204]\tvalid_0's binary_logloss: 0.519538\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000775 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[137]\tvalid_0's binary_logloss: 0.533446\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000831 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[237]\tvalid_0's binary_logloss: 0.525805\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000851 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[186]\tvalid_0's binary_logloss: 0.522903\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002700 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[254]\tvalid_0's binary_logloss: 0.523243\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000887 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[204]\tvalid_0's binary_logloss: 0.519538\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000805 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[137]\tvalid_0's binary_logloss: 0.533446\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000790 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[237]\tvalid_0's binary_logloss: 0.525805\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000794 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[186]\tvalid_0's binary_logloss: 0.522903\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000797 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[254]\tvalid_0's binary_logloss: 0.523243\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[204]\tvalid_0's binary_logloss: 0.519538\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001917 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[137]\tvalid_0's binary_logloss: 0.533446\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002365 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[237]\tvalid_0's binary_logloss: 0.525805\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 8081, number of negative: 8017\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000965 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 867\n",
      "[LightGBM] [Info] Number of data points in the train set: 16098, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501988 -> initscore=0.007951\n",
      "[LightGBM] [Info] Start training from score 0.007951\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1499]\tvalid_0's binary_logloss: 0.405889\n",
      "LGBMClassifier(early_stopping_rounds=100, n_estimators=1500, random_state=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.765491</td>\n",
       "      <td>0.021304</td>\n",
       "      <td>0.031073</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 1000}</td>\n",
       "      <td>0.786646</td>\n",
       "      <td>0.786025</td>\n",
       "      <td>0.790373</td>\n",
       "      <td>0.792793</td>\n",
       "      <td>0.787512</td>\n",
       "      <td>0.788670</td>\n",
       "      <td>0.002543</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.991288</td>\n",
       "      <td>0.086752</td>\n",
       "      <td>0.039980</td>\n",
       "      <td>0.005023</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1500</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 1500}</td>\n",
       "      <td>0.787888</td>\n",
       "      <td>0.789130</td>\n",
       "      <td>0.791615</td>\n",
       "      <td>0.789376</td>\n",
       "      <td>0.792171</td>\n",
       "      <td>0.790036</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.989509</td>\n",
       "      <td>0.081493</td>\n",
       "      <td>0.039817</td>\n",
       "      <td>0.004039</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2000</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 2000}</td>\n",
       "      <td>0.787888</td>\n",
       "      <td>0.789130</td>\n",
       "      <td>0.791615</td>\n",
       "      <td>0.789376</td>\n",
       "      <td>0.792171</td>\n",
       "      <td>0.790036</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.630683</td>\n",
       "      <td>0.019075</td>\n",
       "      <td>0.022881</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'learning_rate': 0.15, 'n_estimators': 1000}</td>\n",
       "      <td>0.786957</td>\n",
       "      <td>0.789441</td>\n",
       "      <td>0.792547</td>\n",
       "      <td>0.794967</td>\n",
       "      <td>0.783162</td>\n",
       "      <td>0.789415</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.646376</td>\n",
       "      <td>0.018817</td>\n",
       "      <td>0.023720</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1500</td>\n",
       "      <td>{'learning_rate': 0.15, 'n_estimators': 1500}</td>\n",
       "      <td>0.786957</td>\n",
       "      <td>0.789441</td>\n",
       "      <td>0.792547</td>\n",
       "      <td>0.794967</td>\n",
       "      <td>0.783162</td>\n",
       "      <td>0.789415</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.685742</td>\n",
       "      <td>0.052163</td>\n",
       "      <td>0.024275</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2000</td>\n",
       "      <td>{'learning_rate': 0.15, 'n_estimators': 2000}</td>\n",
       "      <td>0.786957</td>\n",
       "      <td>0.789441</td>\n",
       "      <td>0.792547</td>\n",
       "      <td>0.794967</td>\n",
       "      <td>0.783162</td>\n",
       "      <td>0.789415</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.532945</td>\n",
       "      <td>0.027392</td>\n",
       "      <td>0.019730</td>\n",
       "      <td>0.003197</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'learning_rate': 0.2, 'n_estimators': 1000}</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.783851</td>\n",
       "      <td>0.788820</td>\n",
       "      <td>0.790308</td>\n",
       "      <td>0.775707</td>\n",
       "      <td>0.784880</td>\n",
       "      <td>0.005116</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.529747</td>\n",
       "      <td>0.029764</td>\n",
       "      <td>0.017322</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1500</td>\n",
       "      <td>{'learning_rate': 0.2, 'n_estimators': 1500}</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.783851</td>\n",
       "      <td>0.788820</td>\n",
       "      <td>0.790308</td>\n",
       "      <td>0.775707</td>\n",
       "      <td>0.784880</td>\n",
       "      <td>0.005116</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.524308</td>\n",
       "      <td>0.022411</td>\n",
       "      <td>0.020594</td>\n",
       "      <td>0.005851</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2000</td>\n",
       "      <td>{'learning_rate': 0.2, 'n_estimators': 2000}</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.783851</td>\n",
       "      <td>0.788820</td>\n",
       "      <td>0.790308</td>\n",
       "      <td>0.775707</td>\n",
       "      <td>0.784880</td>\n",
       "      <td>0.005116</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.236598</td>\n",
       "      <td>0.029743</td>\n",
       "      <td>0.007111</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'learning_rate': 0.5, 'n_estimators': 1000}</td>\n",
       "      <td>0.779503</td>\n",
       "      <td>0.783851</td>\n",
       "      <td>0.771118</td>\n",
       "      <td>0.752097</td>\n",
       "      <td>0.767940</td>\n",
       "      <td>0.770902</td>\n",
       "      <td>0.010992</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.239081</td>\n",
       "      <td>0.027058</td>\n",
       "      <td>0.006714</td>\n",
       "      <td>0.001962</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1500</td>\n",
       "      <td>{'learning_rate': 0.5, 'n_estimators': 1500}</td>\n",
       "      <td>0.779503</td>\n",
       "      <td>0.783851</td>\n",
       "      <td>0.771118</td>\n",
       "      <td>0.752097</td>\n",
       "      <td>0.767940</td>\n",
       "      <td>0.770902</td>\n",
       "      <td>0.010992</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.245046</td>\n",
       "      <td>0.027113</td>\n",
       "      <td>0.007468</td>\n",
       "      <td>0.001694</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2000</td>\n",
       "      <td>{'learning_rate': 0.5, 'n_estimators': 2000}</td>\n",
       "      <td>0.779503</td>\n",
       "      <td>0.783851</td>\n",
       "      <td>0.771118</td>\n",
       "      <td>0.752097</td>\n",
       "      <td>0.767940</td>\n",
       "      <td>0.770902</td>\n",
       "      <td>0.010992</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.765491      0.021304         0.031073        0.001533   \n",
       "1        0.991288      0.086752         0.039980        0.005023   \n",
       "2        0.989509      0.081493         0.039817        0.004039   \n",
       "3        0.630683      0.019075         0.022881        0.001632   \n",
       "4        0.646376      0.018817         0.023720        0.002500   \n",
       "5        0.685742      0.052163         0.024275        0.001837   \n",
       "6        0.532945      0.027392         0.019730        0.003197   \n",
       "7        0.529747      0.029764         0.017322        0.001141   \n",
       "8        0.524308      0.022411         0.020594        0.005851   \n",
       "9        0.236598      0.029743         0.007111        0.001011   \n",
       "10       0.239081      0.027058         0.006714        0.001962   \n",
       "11       0.245046      0.027113         0.007468        0.001694   \n",
       "\n",
       "   param_learning_rate param_n_estimators  \\\n",
       "0                  0.1               1000   \n",
       "1                  0.1               1500   \n",
       "2                  0.1               2000   \n",
       "3                 0.15               1000   \n",
       "4                 0.15               1500   \n",
       "5                 0.15               2000   \n",
       "6                  0.2               1000   \n",
       "7                  0.2               1500   \n",
       "8                  0.2               2000   \n",
       "9                  0.5               1000   \n",
       "10                 0.5               1500   \n",
       "11                 0.5               2000   \n",
       "\n",
       "                                           params  split0_test_score  \\\n",
       "0    {'learning_rate': 0.1, 'n_estimators': 1000}           0.786646   \n",
       "1    {'learning_rate': 0.1, 'n_estimators': 1500}           0.787888   \n",
       "2    {'learning_rate': 0.1, 'n_estimators': 2000}           0.787888   \n",
       "3   {'learning_rate': 0.15, 'n_estimators': 1000}           0.786957   \n",
       "4   {'learning_rate': 0.15, 'n_estimators': 1500}           0.786957   \n",
       "5   {'learning_rate': 0.15, 'n_estimators': 2000}           0.786957   \n",
       "6    {'learning_rate': 0.2, 'n_estimators': 1000}           0.785714   \n",
       "7    {'learning_rate': 0.2, 'n_estimators': 1500}           0.785714   \n",
       "8    {'learning_rate': 0.2, 'n_estimators': 2000}           0.785714   \n",
       "9    {'learning_rate': 0.5, 'n_estimators': 1000}           0.779503   \n",
       "10   {'learning_rate': 0.5, 'n_estimators': 1500}           0.779503   \n",
       "11   {'learning_rate': 0.5, 'n_estimators': 2000}           0.779503   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.786025           0.790373           0.792793   \n",
       "1            0.789130           0.791615           0.789376   \n",
       "2            0.789130           0.791615           0.789376   \n",
       "3            0.789441           0.792547           0.794967   \n",
       "4            0.789441           0.792547           0.794967   \n",
       "5            0.789441           0.792547           0.794967   \n",
       "6            0.783851           0.788820           0.790308   \n",
       "7            0.783851           0.788820           0.790308   \n",
       "8            0.783851           0.788820           0.790308   \n",
       "9            0.783851           0.771118           0.752097   \n",
       "10           0.783851           0.771118           0.752097   \n",
       "11           0.783851           0.771118           0.752097   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.787512         0.788670        0.002543                6  \n",
       "1            0.792171         0.790036        0.001608                1  \n",
       "2            0.792171         0.790036        0.001608                1  \n",
       "3            0.783162         0.789415        0.004142                3  \n",
       "4            0.783162         0.789415        0.004142                3  \n",
       "5            0.783162         0.789415        0.004142                3  \n",
       "6            0.775707         0.784880        0.005116                7  \n",
       "7            0.775707         0.784880        0.005116                7  \n",
       "8            0.775707         0.784880        0.005116                7  \n",
       "9            0.767940         0.770902        0.010992               10  \n",
       "10           0.767940         0.770902        0.010992               10  \n",
       "11           0.767940         0.770902        0.010992               10  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LightGBM\n",
    "from lightgbm import LGBMClassifier\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "lgbm_clf = LGBMClassifier(random_state=0, early_stopping_rounds=100)\n",
    "params = {\n",
    "    \"n_estimators\": [1000,1500,2000],\n",
    "    'learning_rate' : [0.1,0.15,0.2,0.5]\n",
    "}\n",
    "gscv_lgbm = GridSearchCV (lgbm_clf, params, scoring ='accuracy', cv = skf)\n",
    "gscv_lgbm.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
    "print(gscv_lgbm.best_estimator_)\n",
    "pd.DataFrame(gscv_lgbm.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier(early_stopping_rounds=100, n_estimators=1500, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(gscv_lgbm.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 4가지 모델의 최적 파라미터를 사용하여 5-foldvalidation을 이용하여 정확도, 정밀도, 재현율을 비교하여 최종 모델을 선정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8586, number of negative: 8585\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4431\n",
      "[LightGBM] [Info] Number of data points in the train set: 17171, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500029 -> initscore=0.000116\n",
      "[LightGBM] [Info] Start training from score 0.000116\n",
      "[LightGBM] [Info] Number of positive: 8586, number of negative: 8585\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002336 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4412\n",
      "[LightGBM] [Info] Number of data points in the train set: 17171, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500029 -> initscore=0.000116\n",
      "[LightGBM] [Info] Start training from score 0.000116\n",
      "[LightGBM] [Info] Number of positive: 8585, number of negative: 8586\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001978 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4199\n",
      "[LightGBM] [Info] Number of data points in the train set: 17171, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499971 -> initscore=-0.000116\n",
      "[LightGBM] [Info] Start training from score -0.000116\n",
      "[LightGBM] [Info] Number of positive: 8585, number of negative: 8586\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002023 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4166\n",
      "[LightGBM] [Info] Number of data points in the train set: 17171, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499971 -> initscore=-0.000116\n",
      "[LightGBM] [Info] Start training from score -0.000116\n",
      "[LightGBM] [Info] Number of positive: 8586, number of negative: 8586\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003462 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4336\n",
      "[LightGBM] [Info] Number of data points in the train set: 17172, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "의사결정나무 정확도: 0.6995951709679751\n",
      "의사결정나무 정밀도: 0.7512877969279211\n",
      "의사결정나무 재현율: 0.6994063543009145\n",
      "랜덤포레스트 정확도: 0.7757249550569872\n",
      "랜덤포레스트 정밀도: 0.7861506110046139\n",
      "랜덤포레스트 재현율: 0.88166643588162\n",
      "xgboost 정확도: 0.756156308119006\n",
      "xgboost 정밀도: 0.7844645569174908\n",
      "xgboost 재현율: 0.8393627120527526\n",
      "lightGBM 정확도: 0.7478630766963016\n",
      "lightGBM 정밀도: 0.7819028697159789\n",
      "lightGBM 재현율: 0.8310703810470927\n"
     ]
    }
   ],
   "source": [
    "# 최종 모델 비교\n",
    "dt_clf = DecisionTreeClassifier(max_depth=25, random_state=0)\n",
    "rf_clf = RandomForestClassifier(max_depth=34, n_estimators=550, random_state=0)\n",
    "xgb_clf = XGBClassifier(learning_rate=0.05, max_depth=23, n_estimators=800, random_state=0)\n",
    "lgbm_clf = LGBMClassifier(n_estimators=1500, learning_rate=0.1, random_state=0)\n",
    "\n",
    "stratified_kfold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# k번 반복하면서 평가한 정확도를 저장할 배열\n",
    "dt_accuracy = []\n",
    "dt_precision = []\n",
    "dt_recall = []\n",
    "rf_accuracy = []\n",
    "rf_precision = []\n",
    "rf_recall = []\n",
    "xgb_accuracy = []\n",
    "xgb_precision = []\n",
    "xgb_recall = []\n",
    "lgbm_accuracy = []\n",
    "lgbm_precision = []\n",
    "lgbm_recall = []\n",
    "\n",
    "for train_index, val_index in stratified_kfold.split(X_new, y_new):\n",
    "    X_train, y_train = X_new.iloc[train_index], y_new.iloc[train_index]\n",
    "    X_val, y_val = X_new.iloc[val_index], y_new.iloc[val_index]\n",
    "\n",
    "    dt_clf.fit(X_train, y_train)\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "    xgb_clf.fit(X_train, y_train)\n",
    "    lgbm_clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_dt = dt_clf.predict(X_val)   # 검증 데이터로 예측\n",
    "    y_pred_rf = rf_clf.predict(X_val)   # 검증 데이터로 예측\n",
    "    y_pred_xgb = xgb_clf.predict(X_val)   # 검증 데이터로 예측\n",
    "    y_pred_lgbm = lgbm_clf.predict(X_val)   # 검증 데이터로 예측\n",
    "\n",
    "    dt_accuracy.append(accuracy_score(y_val, y_pred_dt)) \n",
    "    rf_accuracy.append(accuracy_score(y_val, y_pred_rf)) \n",
    "    xgb_accuracy.append(accuracy_score(y_val, y_pred_xgb)) \n",
    "    lgbm_accuracy.append(accuracy_score(y_val, y_pred_lgbm)) \n",
    "    \n",
    "    dt_precision.append(precision_score(y_val, y_pred_dt)) \n",
    "    rf_precision.append(precision_score(y_val, y_pred_rf)) \n",
    "    xgb_precision.append(precision_score(y_val, y_pred_xgb)) \n",
    "    lgbm_precision.append(precision_score(y_val, y_pred_lgbm)) \n",
    "    \n",
    "    dt_recall.append(recall_score(y_val, y_pred_dt)) \n",
    "    rf_recall.append(recall_score(y_val, y_pred_rf)) \n",
    "    xgb_recall.append(recall_score(y_val, y_pred_xgb)) \n",
    "    lgbm_recall.append(recall_score(y_val, y_pred_lgbm)) \n",
    "\n",
    "\n",
    "print(\"의사결정나무 정확도:\", np.mean(dt_accuracy))\n",
    "print(\"의사결정나무 정밀도:\", np.mean(dt_precision))\n",
    "print(\"의사결정나무 재현율:\", np.mean(dt_recall))\n",
    "\n",
    "print(\"랜덤포레스트 정확도:\", np.mean(rf_accuracy))\n",
    "print(\"랜덤포레스트 정밀도:\", np.mean(rf_precision))\n",
    "print(\"랜덤포레스트 재현율:\", np.mean(rf_recall))\n",
    "\n",
    "print(\"xgboost 정확도:\", np.mean(xgb_accuracy))\n",
    "print(\"xgboost 정밀도:\", np.mean(xgb_precision))\n",
    "print(\"xgboost 재현율:\", np.mean(xgb_recall))\n",
    "\n",
    "print(\"lightGBM 정확도:\", np.mean(lgbm_accuracy))\n",
    "print(\"lightGBM 정밀도:\", np.mean(lgbm_precision))\n",
    "print(\"lightGBM 재현율:\", np.mean(lgbm_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "랜덤포레스트 최종모델로 선정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "추가적인 모델 사용\n",
    "\n",
    "- 범주형 변수를 라벨인코딩을 하면 로지스틱과 svm과 같은 수리적인 모델에서는 사용하면 안되지만 그냥 한번 해봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7064231581562928 0.689340290719344\n"
     ]
    }
   ],
   "source": [
    "# 로지스틱\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(lr_clf.score(X_train_scaled, y_train), lr_clf.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5663436451733135, 0.5566530003727171)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(kernel=\"rbf\")   # kernel 기본값 \"rbf\"\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_train, y_train), model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7083488632128214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1873,  779],\n",
       "       [ 786, 1928]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(100,100,100,100,100),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=5000,\n",
    "    random_state=42\n",
    ")\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "pred = mlp.predict(X_test)\n",
    "print(accuracy_score(y_test, pred))\n",
    "confusion_matrix(y_test, pred, labels=[0, 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
