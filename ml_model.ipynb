{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Downloading numpy-1.21.6-cp37-cp37m-win_amd64.whl (14.0 MB)\n",
      "     ---------------------------------------- 14.0/14.0 MB 9.2 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-1.21.6\n",
      "Collecting pandas\n",
      "  Downloading pandas-1.3.5-cp37-cp37m-win_amd64.whl (10.0 MB)\n",
      "     ---------------------------------------- 10.0/10.0 MB 7.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\kwj\\anaconda3\\envs\\temp_env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2017.3\n",
      "  Downloading pytz-2025.1-py2.py3-none-any.whl (507 kB)\n",
      "     -------------------------------------- 507.9/507.9 kB 4.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\kwj\\anaconda3\\envs\\temp_env\\lib\\site-packages (from pandas) (1.21.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kwj\\anaconda3\\envs\\temp_env\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.17.0)\n",
      "Installing collected packages: pytz, pandas\n",
      "Successfully installed pandas-1.3.5 pytz-2025.1\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.5.3-cp37-cp37m-win_amd64.whl (7.2 MB)\n",
      "     ---------------------------------------- 7.2/7.2 MB 10.7 MB/s eta 0:00:00\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting pillow>=6.2.0\n",
      "  Downloading Pillow-9.5.0-cp37-cp37m-win_amd64.whl (2.5 MB)\n",
      "     ---------------------------------------- 2.5/2.5 MB 11.5 MB/s eta 0:00:00\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.38.0-py3-none-any.whl (965 kB)\n",
      "     ------------------------------------- 965.4/965.4 kB 10.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kwj\\anaconda3\\envs\\temp_env\\lib\\site-packages (from matplotlib) (1.21.6)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.5-cp37-cp37m-win_amd64.whl (55 kB)\n",
      "     ---------------------------------------- 55.8/55.8 kB ? eta 0:00:00\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\kwj\\anaconda3\\envs\\temp_env\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Collecting pyparsing>=2.2.1\n",
      "  Downloading pyparsing-3.1.4-py3-none-any.whl (104 kB)\n",
      "     -------------------------------------- 104.1/104.1 kB 6.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kwj\\anaconda3\\envs\\temp_env\\lib\\site-packages (from matplotlib) (24.0)\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kwj\\anaconda3\\envs\\temp_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Installing collected packages: typing-extensions, pyparsing, pillow, fonttools, cycler, kiwisolver, matplotlib\n",
      "Successfully installed cycler-0.11.0 fonttools-4.38.0 kiwisolver-1.4.5 matplotlib-3.5.3 pillow-9.5.0 pyparsing-3.1.4 typing-extensions-4.7.1\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "     -------------------------------------- 293.3/293.3 kB 4.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: typing_extensions in c:\\users\\kwj\\anaconda3\\envs\\temp_env\\lib\\site-packages (from seaborn) (4.7.1)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.17 in c:\\users\\kwj\\anaconda3\\envs\\temp_env\\lib\\site-packages (from seaborn) (1.21.6)\n",
      "Requirement already satisfied: pandas>=0.25 in c:\\users\\kwj\\anaconda3\\envs\\temp_env\\lib\\site-packages (from seaborn) (1.3.5)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in c:\\users\\kwj\\anaconda3\\envs\\temp_env\\lib\\site-packages (from seaborn) (3.5.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kwj\\anaconda3\\envs\\temp_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (24.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kwj\\anaconda3\\envs\\temp_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\kwj\\anaconda3\\envs\\temp_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.38.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\kwj\\anaconda3\\envs\\temp_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\kwj\\anaconda3\\envs\\temp_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.5.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\kwj\\anaconda3\\envs\\temp_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.1.4)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\kwj\\anaconda3\\envs\\temp_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.5)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\kwj\\anaconda3\\envs\\temp_env\\lib\\site-packages (from pandas>=0.25->seaborn) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kwj\\anaconda3\\envs\\temp_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.17.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.12.2\n"
     ]
    }
   ],
   "source": [
    "# !pip install numpy\n",
    "# !pip install pandas\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn\n",
    "# !pip install sklearn\n",
    "# !pip install imbalanced-learn\n",
    "# !pip install lightgbm\n",
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, precision_score, recall_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "df = pd.read_csv('./data/data.csv')\n",
    "# df.head(3)\n",
    "\n",
    "X = df.drop(['id','hire_state','b022', 'b023', 'b036', 'b038', 'b039', 'b040'],axis=1)\n",
    "y = df['hire_state']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 모델 돌려봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "의사결정나무 정확도: 0.7015742642026009 0.6918142160636387\n"
     ]
    }
   ],
   "source": [
    "# 의사결정 나무\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=0, max_depth=3)   # max_depth : 가지치기 (최대 깊이 지정)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "print(\"의사결정나무 정확도:\", dt_clf.score(X_train, y_train), dt_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "랜덤 포레스트 정확도: 1.0 0.7208108801642289\n"
     ]
    }
   ],
   "source": [
    "# 랜덤 포레스트\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_features='sqrt',  # 특성의 일부만 사용\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "rf_clf.fit(X_train, y_train)\n",
    "print(\"랜덤 포레스트 정확도:\", rf_clf.score(X_train, y_train), rf_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xgboost 정확도: 0.7375085557837098 0.7274826789838337\n"
     ]
    }
   ],
   "source": [
    "# xgboost\n",
    "xgb_clf = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = xgb_clf.predict(X_train)\n",
    "y_pred_test = xgb_clf.predict(X_test)\n",
    "\n",
    "print(\"xgboost 정확도:\", accuracy_score(y_train, y_pred_train), accuracy_score(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 274,  925],\n",
       "       [ 276, 2422]], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dt_clf.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred, labels=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "70%대의 성능이 나오는 이유는 전부 y=1(취직함)으로 예측하기 때문으로 y값의 비율 차이로 발생한다고 추정됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### oversampleing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 10732, 0: 4853})\n",
      "Counter({0: 10732, 1: 10732})\n",
      "Counter({0: 10732, 1: 10732})\n"
     ]
    }
   ],
   "source": [
    "counter = Counter(y)\n",
    "print(counter)\n",
    "\n",
    "smt = SMOTE(random_state=42)\n",
    "X_new, y_new = smt.fit_resample(X, y)\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_res, y_res = ros.fit_resample(X, y)\n",
    "\n",
    "counter = Counter(y_new)\n",
    "print(counter)\n",
    "counter = Counter(y_res)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE는 가장 가까운 값 사이에 직성을 만들고 그 안에서 새로운 값을 뽑는 방식으로 범주형 변수에 맞지 않아서 RandomOverSampler 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6087712759348988 0.5972791651136787\n"
     ]
    }
   ],
   "source": [
    "dt_clf = DecisionTreeClassifier(random_state=0, max_depth=3)   # max_depth : 가지치기 (최대 깊이 지정)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "print(dt_clf.score(X_train, y_train), dt_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1348, 1367],\n",
       "       [ 794, 1857]], dtype=int64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = dt_clf.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred, labels=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "정확도는 떨어졌지만 예측을 고르게 하는 것으로 문제가 해결됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리드서치"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최적의 하이퍼 파라미터를 선택하여 높은 정확도의 모델을 만들기 위해 그리드 서치 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=25, random_state=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.055659</td>\n",
       "      <td>0.002224</td>\n",
       "      <td>0.002398</td>\n",
       "      <td>0.000373</td>\n",
       "      <td>12</td>\n",
       "      <td>{'max_depth': 12}</td>\n",
       "      <td>0.674534</td>\n",
       "      <td>0.662112</td>\n",
       "      <td>0.653727</td>\n",
       "      <td>0.657036</td>\n",
       "      <td>0.672569</td>\n",
       "      <td>0.663996</td>\n",
       "      <td>0.008270</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.060052</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>0.001793</td>\n",
       "      <td>0.000737</td>\n",
       "      <td>13</td>\n",
       "      <td>{'max_depth': 13}</td>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.668634</td>\n",
       "      <td>0.662112</td>\n",
       "      <td>0.673812</td>\n",
       "      <td>0.679404</td>\n",
       "      <td>0.672506</td>\n",
       "      <td>0.006467</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.061553</td>\n",
       "      <td>0.001092</td>\n",
       "      <td>0.002089</td>\n",
       "      <td>0.000136</td>\n",
       "      <td>14</td>\n",
       "      <td>{'max_depth': 14}</td>\n",
       "      <td>0.696584</td>\n",
       "      <td>0.679814</td>\n",
       "      <td>0.674534</td>\n",
       "      <td>0.683442</td>\n",
       "      <td>0.686238</td>\n",
       "      <td>0.684122</td>\n",
       "      <td>0.007358</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.065847</td>\n",
       "      <td>0.003863</td>\n",
       "      <td>0.001902</td>\n",
       "      <td>0.000491</td>\n",
       "      <td>15</td>\n",
       "      <td>{'max_depth': 15}</td>\n",
       "      <td>0.704658</td>\n",
       "      <td>0.684472</td>\n",
       "      <td>0.683851</td>\n",
       "      <td>0.696179</td>\n",
       "      <td>0.690587</td>\n",
       "      <td>0.691949</td>\n",
       "      <td>0.007782</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.064722</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>0.001901</td>\n",
       "      <td>0.000198</td>\n",
       "      <td>16</td>\n",
       "      <td>{'max_depth': 16}</td>\n",
       "      <td>0.711801</td>\n",
       "      <td>0.696894</td>\n",
       "      <td>0.693789</td>\n",
       "      <td>0.703324</td>\n",
       "      <td>0.703945</td>\n",
       "      <td>0.701951</td>\n",
       "      <td>0.006246</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.069801</td>\n",
       "      <td>0.004666</td>\n",
       "      <td>0.002002</td>\n",
       "      <td>0.000632</td>\n",
       "      <td>17</td>\n",
       "      <td>{'max_depth': 17}</td>\n",
       "      <td>0.718323</td>\n",
       "      <td>0.702484</td>\n",
       "      <td>0.701553</td>\n",
       "      <td>0.712022</td>\n",
       "      <td>0.713265</td>\n",
       "      <td>0.709530</td>\n",
       "      <td>0.006492</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.072629</td>\n",
       "      <td>0.004881</td>\n",
       "      <td>0.002000</td>\n",
       "      <td>0.000002</td>\n",
       "      <td>18</td>\n",
       "      <td>{'max_depth': 18}</td>\n",
       "      <td>0.726087</td>\n",
       "      <td>0.712422</td>\n",
       "      <td>0.708075</td>\n",
       "      <td>0.716682</td>\n",
       "      <td>0.713265</td>\n",
       "      <td>0.715306</td>\n",
       "      <td>0.006048</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.075809</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.002707</td>\n",
       "      <td>0.000684</td>\n",
       "      <td>19</td>\n",
       "      <td>{'max_depth': 19}</td>\n",
       "      <td>0.725776</td>\n",
       "      <td>0.723292</td>\n",
       "      <td>0.713354</td>\n",
       "      <td>0.726934</td>\n",
       "      <td>0.717925</td>\n",
       "      <td>0.721456</td>\n",
       "      <td>0.005102</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.074808</td>\n",
       "      <td>0.002949</td>\n",
       "      <td>0.003002</td>\n",
       "      <td>0.000546</td>\n",
       "      <td>20</td>\n",
       "      <td>{'max_depth': 20}</td>\n",
       "      <td>0.730124</td>\n",
       "      <td>0.726398</td>\n",
       "      <td>0.718944</td>\n",
       "      <td>0.720721</td>\n",
       "      <td>0.720721</td>\n",
       "      <td>0.723381</td>\n",
       "      <td>0.004205</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.078510</td>\n",
       "      <td>0.003090</td>\n",
       "      <td>0.002667</td>\n",
       "      <td>0.000533</td>\n",
       "      <td>21</td>\n",
       "      <td>{'max_depth': 21}</td>\n",
       "      <td>0.732919</td>\n",
       "      <td>0.732919</td>\n",
       "      <td>0.727640</td>\n",
       "      <td>0.731904</td>\n",
       "      <td>0.730040</td>\n",
       "      <td>0.731085</td>\n",
       "      <td>0.002018</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.074178</td>\n",
       "      <td>0.000893</td>\n",
       "      <td>0.002608</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>22</td>\n",
       "      <td>{'max_depth': 22}</td>\n",
       "      <td>0.736335</td>\n",
       "      <td>0.728571</td>\n",
       "      <td>0.731366</td>\n",
       "      <td>0.730662</td>\n",
       "      <td>0.726623</td>\n",
       "      <td>0.730712</td>\n",
       "      <td>0.003267</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.077315</td>\n",
       "      <td>0.002706</td>\n",
       "      <td>0.002704</td>\n",
       "      <td>0.000396</td>\n",
       "      <td>23</td>\n",
       "      <td>{'max_depth': 23}</td>\n",
       "      <td>0.744720</td>\n",
       "      <td>0.730745</td>\n",
       "      <td>0.727950</td>\n",
       "      <td>0.738428</td>\n",
       "      <td>0.726623</td>\n",
       "      <td>0.733693</td>\n",
       "      <td>0.006866</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.075880</td>\n",
       "      <td>0.003275</td>\n",
       "      <td>0.002704</td>\n",
       "      <td>0.000401</td>\n",
       "      <td>24</td>\n",
       "      <td>{'max_depth': 24}</td>\n",
       "      <td>0.743789</td>\n",
       "      <td>0.733540</td>\n",
       "      <td>0.734783</td>\n",
       "      <td>0.736253</td>\n",
       "      <td>0.732215</td>\n",
       "      <td>0.736116</td>\n",
       "      <td>0.004063</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.074519</td>\n",
       "      <td>0.001472</td>\n",
       "      <td>0.002199</td>\n",
       "      <td>0.000749</td>\n",
       "      <td>25</td>\n",
       "      <td>{'max_depth': 25}</td>\n",
       "      <td>0.744099</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.738199</td>\n",
       "      <td>0.745573</td>\n",
       "      <td>0.729730</td>\n",
       "      <td>0.739346</td>\n",
       "      <td>0.005571</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.075995</td>\n",
       "      <td>0.003325</td>\n",
       "      <td>0.002825</td>\n",
       "      <td>0.000733</td>\n",
       "      <td>26</td>\n",
       "      <td>{'max_depth': 26}</td>\n",
       "      <td>0.748137</td>\n",
       "      <td>0.735714</td>\n",
       "      <td>0.740062</td>\n",
       "      <td>0.740603</td>\n",
       "      <td>0.730351</td>\n",
       "      <td>0.738973</td>\n",
       "      <td>0.005878</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.055659      0.002224         0.002398        0.000373   \n",
       "1        0.060052      0.000907         0.001793        0.000737   \n",
       "2        0.061553      0.001092         0.002089        0.000136   \n",
       "3        0.065847      0.003863         0.001902        0.000491   \n",
       "4        0.064722      0.000749         0.001901        0.000198   \n",
       "5        0.069801      0.004666         0.002002        0.000632   \n",
       "6        0.072629      0.004881         0.002000        0.000002   \n",
       "7        0.075809      0.002222         0.002707        0.000684   \n",
       "8        0.074808      0.002949         0.003002        0.000546   \n",
       "9        0.078510      0.003090         0.002667        0.000533   \n",
       "10       0.074178      0.000893         0.002608        0.000852   \n",
       "11       0.077315      0.002706         0.002704        0.000396   \n",
       "12       0.075880      0.003275         0.002704        0.000401   \n",
       "13       0.074519      0.001472         0.002199        0.000749   \n",
       "14       0.075995      0.003325         0.002825        0.000733   \n",
       "\n",
       "   param_max_depth             params  split0_test_score  split1_test_score  \\\n",
       "0               12  {'max_depth': 12}           0.674534           0.662112   \n",
       "1               13  {'max_depth': 13}           0.678571           0.668634   \n",
       "2               14  {'max_depth': 14}           0.696584           0.679814   \n",
       "3               15  {'max_depth': 15}           0.704658           0.684472   \n",
       "4               16  {'max_depth': 16}           0.711801           0.696894   \n",
       "5               17  {'max_depth': 17}           0.718323           0.702484   \n",
       "6               18  {'max_depth': 18}           0.726087           0.712422   \n",
       "7               19  {'max_depth': 19}           0.725776           0.723292   \n",
       "8               20  {'max_depth': 20}           0.730124           0.726398   \n",
       "9               21  {'max_depth': 21}           0.732919           0.732919   \n",
       "10              22  {'max_depth': 22}           0.736335           0.728571   \n",
       "11              23  {'max_depth': 23}           0.744720           0.730745   \n",
       "12              24  {'max_depth': 24}           0.743789           0.733540   \n",
       "13              25  {'max_depth': 25}           0.744099           0.739130   \n",
       "14              26  {'max_depth': 26}           0.748137           0.735714   \n",
       "\n",
       "    split2_test_score  split3_test_score  split4_test_score  mean_test_score  \\\n",
       "0            0.653727           0.657036           0.672569         0.663996   \n",
       "1            0.662112           0.673812           0.679404         0.672506   \n",
       "2            0.674534           0.683442           0.686238         0.684122   \n",
       "3            0.683851           0.696179           0.690587         0.691949   \n",
       "4            0.693789           0.703324           0.703945         0.701951   \n",
       "5            0.701553           0.712022           0.713265         0.709530   \n",
       "6            0.708075           0.716682           0.713265         0.715306   \n",
       "7            0.713354           0.726934           0.717925         0.721456   \n",
       "8            0.718944           0.720721           0.720721         0.723381   \n",
       "9            0.727640           0.731904           0.730040         0.731085   \n",
       "10           0.731366           0.730662           0.726623         0.730712   \n",
       "11           0.727950           0.738428           0.726623         0.733693   \n",
       "12           0.734783           0.736253           0.732215         0.736116   \n",
       "13           0.738199           0.745573           0.729730         0.739346   \n",
       "14           0.740062           0.740603           0.730351         0.738973   \n",
       "\n",
       "    std_test_score  rank_test_score  \n",
       "0         0.008270               15  \n",
       "1         0.006467               14  \n",
       "2         0.007358               13  \n",
       "3         0.007782               12  \n",
       "4         0.006246               11  \n",
       "5         0.006492               10  \n",
       "6         0.006048                9  \n",
       "7         0.005102                8  \n",
       "8         0.004205                7  \n",
       "9         0.002018                5  \n",
       "10        0.003267                6  \n",
       "11        0.006866                4  \n",
       "12        0.004063                3  \n",
       "13        0.005571                1  \n",
       "14        0.005878                2  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "dt_clf = DecisionTreeClassifier(random_state=0)\n",
    "params = {\n",
    "    \"max_depth\": [12,13,14,15,16,17,18,19,20,21,22,23,24,25,26]\n",
    "}\n",
    "gscv_tree = GridSearchCV (dt_clf, params, scoring ='accuracy', cv = skf)\n",
    "gscv_tree.fit(X_train, y_train)\n",
    "print(gscv_tree.best_estimator_)\n",
    "# pd.DataFrame(gscv_tree.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.8583674990682072\n"
     ]
    }
   ],
   "source": [
    "# 랜덤 포레스트\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    random_state=42\n",
    ")\n",
    "rf_clf.fit(X_train, y_train)\n",
    "print(\"Random Forest Accuracy:\", rf_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForestClassifier(max_depth=34, n_estimators=550, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "rf_clf = RandomForestClassifier(random_state=0)\n",
    "# params = {\n",
    "#     \"max_depth\": [5,10,15,20,25,30,35],\n",
    "#     \"n_estimators\": [100,200,300,400,500,800,1000]\n",
    "# }\n",
    "params = {\n",
    "    \"max_depth\": [26,27,28,29,30,31,32,33,34],\n",
    "    \"n_estimators\": [450,500,550,600,650,700]\n",
    "}\n",
    "gscv_rf = GridSearchCV (rf_clf, params, scoring ='accuracy', cv = skf)\n",
    "gscv_rf.fit(X_train, y_train)\n",
    "print(gscv_rf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.411443</td>\n",
       "      <td>0.010121</td>\n",
       "      <td>0.020357</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 100}</td>\n",
       "      <td>0.648758</td>\n",
       "      <td>0.646894</td>\n",
       "      <td>0.643789</td>\n",
       "      <td>0.641504</td>\n",
       "      <td>0.631873</td>\n",
       "      <td>0.642564</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.792946</td>\n",
       "      <td>0.021246</td>\n",
       "      <td>0.036564</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 200}</td>\n",
       "      <td>0.648758</td>\n",
       "      <td>0.644410</td>\n",
       "      <td>0.648447</td>\n",
       "      <td>0.643057</td>\n",
       "      <td>0.634048</td>\n",
       "      <td>0.643744</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.167702</td>\n",
       "      <td>0.023728</td>\n",
       "      <td>0.054850</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 300}</td>\n",
       "      <td>0.653416</td>\n",
       "      <td>0.642547</td>\n",
       "      <td>0.648758</td>\n",
       "      <td>0.640572</td>\n",
       "      <td>0.634358</td>\n",
       "      <td>0.643930</td>\n",
       "      <td>0.006605</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.598454</td>\n",
       "      <td>0.036300</td>\n",
       "      <td>0.079807</td>\n",
       "      <td>0.010289</td>\n",
       "      <td>5</td>\n",
       "      <td>400</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 400}</td>\n",
       "      <td>0.651863</td>\n",
       "      <td>0.642547</td>\n",
       "      <td>0.646894</td>\n",
       "      <td>0.639640</td>\n",
       "      <td>0.634669</td>\n",
       "      <td>0.643123</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.003466</td>\n",
       "      <td>0.041828</td>\n",
       "      <td>0.092758</td>\n",
       "      <td>0.005912</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 500}</td>\n",
       "      <td>0.653106</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.648447</td>\n",
       "      <td>0.639329</td>\n",
       "      <td>0.632495</td>\n",
       "      <td>0.643247</td>\n",
       "      <td>0.007146</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.200616</td>\n",
       "      <td>0.073565</td>\n",
       "      <td>0.146300</td>\n",
       "      <td>0.002856</td>\n",
       "      <td>5</td>\n",
       "      <td>800</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 800}</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.643478</td>\n",
       "      <td>0.644720</td>\n",
       "      <td>0.641504</td>\n",
       "      <td>0.632495</td>\n",
       "      <td>0.642439</td>\n",
       "      <td>0.005714</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.961413</td>\n",
       "      <td>0.052801</td>\n",
       "      <td>0.178615</td>\n",
       "      <td>0.002159</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 1000}</td>\n",
       "      <td>0.648137</td>\n",
       "      <td>0.644099</td>\n",
       "      <td>0.645342</td>\n",
       "      <td>0.642746</td>\n",
       "      <td>0.631563</td>\n",
       "      <td>0.642377</td>\n",
       "      <td>0.005692</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.758636</td>\n",
       "      <td>0.036442</td>\n",
       "      <td>0.033649</td>\n",
       "      <td>0.002227</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 100}</td>\n",
       "      <td>0.708696</td>\n",
       "      <td>0.694410</td>\n",
       "      <td>0.707453</td>\n",
       "      <td>0.694315</td>\n",
       "      <td>0.685306</td>\n",
       "      <td>0.698036</td>\n",
       "      <td>0.008847</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.515033</td>\n",
       "      <td>0.138325</td>\n",
       "      <td>0.067491</td>\n",
       "      <td>0.003222</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 200}</td>\n",
       "      <td>0.704969</td>\n",
       "      <td>0.697516</td>\n",
       "      <td>0.711801</td>\n",
       "      <td>0.694936</td>\n",
       "      <td>0.694004</td>\n",
       "      <td>0.700645</td>\n",
       "      <td>0.006777</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.150137</td>\n",
       "      <td>0.072125</td>\n",
       "      <td>0.096304</td>\n",
       "      <td>0.005918</td>\n",
       "      <td>10</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 300}</td>\n",
       "      <td>0.709006</td>\n",
       "      <td>0.699068</td>\n",
       "      <td>0.713043</td>\n",
       "      <td>0.697111</td>\n",
       "      <td>0.691519</td>\n",
       "      <td>0.701950</td>\n",
       "      <td>0.007917</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.048869</td>\n",
       "      <td>0.263477</td>\n",
       "      <td>0.138487</td>\n",
       "      <td>0.006993</td>\n",
       "      <td>10</td>\n",
       "      <td>400</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 400}</td>\n",
       "      <td>0.711180</td>\n",
       "      <td>0.700932</td>\n",
       "      <td>0.708075</td>\n",
       "      <td>0.694004</td>\n",
       "      <td>0.693072</td>\n",
       "      <td>0.701453</td>\n",
       "      <td>0.007273</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.696555</td>\n",
       "      <td>0.241600</td>\n",
       "      <td>0.162404</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>10</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 500}</td>\n",
       "      <td>0.714907</td>\n",
       "      <td>0.696584</td>\n",
       "      <td>0.706522</td>\n",
       "      <td>0.696800</td>\n",
       "      <td>0.693383</td>\n",
       "      <td>0.701639</td>\n",
       "      <td>0.007962</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.440744</td>\n",
       "      <td>0.034421</td>\n",
       "      <td>0.248233</td>\n",
       "      <td>0.008386</td>\n",
       "      <td>10</td>\n",
       "      <td>800</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 800}</td>\n",
       "      <td>0.711801</td>\n",
       "      <td>0.696273</td>\n",
       "      <td>0.710248</td>\n",
       "      <td>0.697732</td>\n",
       "      <td>0.694004</td>\n",
       "      <td>0.702012</td>\n",
       "      <td>0.007470</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.673083</td>\n",
       "      <td>0.140280</td>\n",
       "      <td>0.303600</td>\n",
       "      <td>0.004587</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 1000}</td>\n",
       "      <td>0.711180</td>\n",
       "      <td>0.696894</td>\n",
       "      <td>0.710559</td>\n",
       "      <td>0.700528</td>\n",
       "      <td>0.693072</td>\n",
       "      <td>0.702447</td>\n",
       "      <td>0.007273</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.938587</td>\n",
       "      <td>0.024200</td>\n",
       "      <td>0.046389</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 15, 'n_estimators': 100}</td>\n",
       "      <td>0.790373</td>\n",
       "      <td>0.772671</td>\n",
       "      <td>0.774224</td>\n",
       "      <td>0.775707</td>\n",
       "      <td>0.782852</td>\n",
       "      <td>0.779165</td>\n",
       "      <td>0.006599</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.828865</td>\n",
       "      <td>0.008322</td>\n",
       "      <td>0.087851</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>15</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 15, 'n_estimators': 200}</td>\n",
       "      <td>0.787578</td>\n",
       "      <td>0.773292</td>\n",
       "      <td>0.776708</td>\n",
       "      <td>0.773532</td>\n",
       "      <td>0.778813</td>\n",
       "      <td>0.777985</td>\n",
       "      <td>0.005219</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.812448</td>\n",
       "      <td>0.026993</td>\n",
       "      <td>0.131371</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>15</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 15, 'n_estimators': 300}</td>\n",
       "      <td>0.788199</td>\n",
       "      <td>0.774534</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.776639</td>\n",
       "      <td>0.780056</td>\n",
       "      <td>0.780407</td>\n",
       "      <td>0.004783</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.759496</td>\n",
       "      <td>0.039398</td>\n",
       "      <td>0.183108</td>\n",
       "      <td>0.004382</td>\n",
       "      <td>15</td>\n",
       "      <td>400</td>\n",
       "      <td>{'max_depth': 15, 'n_estimators': 400}</td>\n",
       "      <td>0.788820</td>\n",
       "      <td>0.775466</td>\n",
       "      <td>0.780435</td>\n",
       "      <td>0.776639</td>\n",
       "      <td>0.778503</td>\n",
       "      <td>0.779972</td>\n",
       "      <td>0.004734</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.627345</td>\n",
       "      <td>0.063624</td>\n",
       "      <td>0.222309</td>\n",
       "      <td>0.009132</td>\n",
       "      <td>15</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 15, 'n_estimators': 500}</td>\n",
       "      <td>0.790994</td>\n",
       "      <td>0.776398</td>\n",
       "      <td>0.783230</td>\n",
       "      <td>0.775085</td>\n",
       "      <td>0.780056</td>\n",
       "      <td>0.781152</td>\n",
       "      <td>0.005688</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7.536066</td>\n",
       "      <td>0.095466</td>\n",
       "      <td>0.370397</td>\n",
       "      <td>0.006364</td>\n",
       "      <td>15</td>\n",
       "      <td>800</td>\n",
       "      <td>{'max_depth': 15, 'n_estimators': 800}</td>\n",
       "      <td>0.791304</td>\n",
       "      <td>0.777640</td>\n",
       "      <td>0.783851</td>\n",
       "      <td>0.777260</td>\n",
       "      <td>0.778192</td>\n",
       "      <td>0.781649</td>\n",
       "      <td>0.005392</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>9.456755</td>\n",
       "      <td>0.102391</td>\n",
       "      <td>0.471243</td>\n",
       "      <td>0.008593</td>\n",
       "      <td>15</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 15, 'n_estimators': 1000}</td>\n",
       "      <td>0.790062</td>\n",
       "      <td>0.776398</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.776328</td>\n",
       "      <td>0.775707</td>\n",
       "      <td>0.780842</td>\n",
       "      <td>0.005920</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.067395</td>\n",
       "      <td>0.011992</td>\n",
       "      <td>0.052982</td>\n",
       "      <td>0.002482</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 100}</td>\n",
       "      <td>0.815528</td>\n",
       "      <td>0.798758</td>\n",
       "      <td>0.807764</td>\n",
       "      <td>0.801802</td>\n",
       "      <td>0.795899</td>\n",
       "      <td>0.803950</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.114477</td>\n",
       "      <td>0.010998</td>\n",
       "      <td>0.100454</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>20</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 200}</td>\n",
       "      <td>0.819565</td>\n",
       "      <td>0.801553</td>\n",
       "      <td>0.808385</td>\n",
       "      <td>0.811432</td>\n",
       "      <td>0.801180</td>\n",
       "      <td>0.808423</td>\n",
       "      <td>0.006824</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.188295</td>\n",
       "      <td>0.027905</td>\n",
       "      <td>0.153440</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>20</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 300}</td>\n",
       "      <td>0.821118</td>\n",
       "      <td>0.802484</td>\n",
       "      <td>0.810870</td>\n",
       "      <td>0.817024</td>\n",
       "      <td>0.803976</td>\n",
       "      <td>0.811094</td>\n",
       "      <td>0.007218</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.280581</td>\n",
       "      <td>0.057216</td>\n",
       "      <td>0.202270</td>\n",
       "      <td>0.007443</td>\n",
       "      <td>20</td>\n",
       "      <td>400</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 400}</td>\n",
       "      <td>0.822360</td>\n",
       "      <td>0.806211</td>\n",
       "      <td>0.815528</td>\n",
       "      <td>0.815781</td>\n",
       "      <td>0.803044</td>\n",
       "      <td>0.812585</td>\n",
       "      <td>0.007015</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.394105</td>\n",
       "      <td>0.112039</td>\n",
       "      <td>0.251276</td>\n",
       "      <td>0.001950</td>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 500}</td>\n",
       "      <td>0.823292</td>\n",
       "      <td>0.809317</td>\n",
       "      <td>0.814596</td>\n",
       "      <td>0.814849</td>\n",
       "      <td>0.801180</td>\n",
       "      <td>0.812647</td>\n",
       "      <td>0.007274</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8.850856</td>\n",
       "      <td>0.054284</td>\n",
       "      <td>0.410372</td>\n",
       "      <td>0.008391</td>\n",
       "      <td>20</td>\n",
       "      <td>800</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 800}</td>\n",
       "      <td>0.822050</td>\n",
       "      <td>0.809938</td>\n",
       "      <td>0.818012</td>\n",
       "      <td>0.812364</td>\n",
       "      <td>0.802423</td>\n",
       "      <td>0.812957</td>\n",
       "      <td>0.006763</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>11.015884</td>\n",
       "      <td>0.124539</td>\n",
       "      <td>0.516344</td>\n",
       "      <td>0.014216</td>\n",
       "      <td>20</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 1000}</td>\n",
       "      <td>0.821739</td>\n",
       "      <td>0.808696</td>\n",
       "      <td>0.818323</td>\n",
       "      <td>0.811743</td>\n",
       "      <td>0.801802</td>\n",
       "      <td>0.812460</td>\n",
       "      <td>0.007053</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.100297</td>\n",
       "      <td>0.014386</td>\n",
       "      <td>0.056506</td>\n",
       "      <td>0.002138</td>\n",
       "      <td>25</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 25, 'n_estimators': 100}</td>\n",
       "      <td>0.816149</td>\n",
       "      <td>0.811491</td>\n",
       "      <td>0.825466</td>\n",
       "      <td>0.820441</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.816872</td>\n",
       "      <td>0.005528</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.240584</td>\n",
       "      <td>0.053113</td>\n",
       "      <td>0.108751</td>\n",
       "      <td>0.003234</td>\n",
       "      <td>25</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 25, 'n_estimators': 200}</td>\n",
       "      <td>0.820186</td>\n",
       "      <td>0.813043</td>\n",
       "      <td>0.822050</td>\n",
       "      <td>0.825412</td>\n",
       "      <td>0.811432</td>\n",
       "      <td>0.818425</td>\n",
       "      <td>0.005346</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3.374285</td>\n",
       "      <td>0.019244</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.002406</td>\n",
       "      <td>25</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 25, 'n_estimators': 300}</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.811801</td>\n",
       "      <td>0.824534</td>\n",
       "      <td>0.823237</td>\n",
       "      <td>0.811121</td>\n",
       "      <td>0.819356</td>\n",
       "      <td>0.006513</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4.516173</td>\n",
       "      <td>0.051738</td>\n",
       "      <td>0.220194</td>\n",
       "      <td>0.008681</td>\n",
       "      <td>25</td>\n",
       "      <td>400</td>\n",
       "      <td>{'max_depth': 25, 'n_estimators': 400}</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.813665</td>\n",
       "      <td>0.822050</td>\n",
       "      <td>0.821994</td>\n",
       "      <td>0.805530</td>\n",
       "      <td>0.817865</td>\n",
       "      <td>0.007375</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5.747314</td>\n",
       "      <td>0.071435</td>\n",
       "      <td>0.275002</td>\n",
       "      <td>0.002815</td>\n",
       "      <td>25</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 25, 'n_estimators': 500}</td>\n",
       "      <td>0.824845</td>\n",
       "      <td>0.811180</td>\n",
       "      <td>0.822360</td>\n",
       "      <td>0.817335</td>\n",
       "      <td>0.808015</td>\n",
       "      <td>0.816747</td>\n",
       "      <td>0.006398</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>9.305920</td>\n",
       "      <td>0.066621</td>\n",
       "      <td>0.436677</td>\n",
       "      <td>0.004358</td>\n",
       "      <td>25</td>\n",
       "      <td>800</td>\n",
       "      <td>{'max_depth': 25, 'n_estimators': 800}</td>\n",
       "      <td>0.825466</td>\n",
       "      <td>0.812422</td>\n",
       "      <td>0.822981</td>\n",
       "      <td>0.817956</td>\n",
       "      <td>0.804908</td>\n",
       "      <td>0.816747</td>\n",
       "      <td>0.007419</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>11.487637</td>\n",
       "      <td>0.154556</td>\n",
       "      <td>0.544431</td>\n",
       "      <td>0.012129</td>\n",
       "      <td>25</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 25, 'n_estimators': 1000}</td>\n",
       "      <td>0.826398</td>\n",
       "      <td>0.813975</td>\n",
       "      <td>0.822360</td>\n",
       "      <td>0.818888</td>\n",
       "      <td>0.806462</td>\n",
       "      <td>0.817616</td>\n",
       "      <td>0.006913</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.132784</td>\n",
       "      <td>0.018518</td>\n",
       "      <td>0.056671</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>30</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 100}</td>\n",
       "      <td>0.814596</td>\n",
       "      <td>0.805280</td>\n",
       "      <td>0.818323</td>\n",
       "      <td>0.819199</td>\n",
       "      <td>0.810500</td>\n",
       "      <td>0.813579</td>\n",
       "      <td>0.005167</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2.288341</td>\n",
       "      <td>0.024397</td>\n",
       "      <td>0.111456</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>30</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 200}</td>\n",
       "      <td>0.818944</td>\n",
       "      <td>0.816149</td>\n",
       "      <td>0.820497</td>\n",
       "      <td>0.822305</td>\n",
       "      <td>0.813607</td>\n",
       "      <td>0.818300</td>\n",
       "      <td>0.003096</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3.441381</td>\n",
       "      <td>0.073761</td>\n",
       "      <td>0.164334</td>\n",
       "      <td>0.003188</td>\n",
       "      <td>30</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 300}</td>\n",
       "      <td>0.827329</td>\n",
       "      <td>0.816149</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.822616</td>\n",
       "      <td>0.810500</td>\n",
       "      <td>0.819605</td>\n",
       "      <td>0.005777</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4.560962</td>\n",
       "      <td>0.068064</td>\n",
       "      <td>0.220080</td>\n",
       "      <td>0.002651</td>\n",
       "      <td>30</td>\n",
       "      <td>400</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 400}</td>\n",
       "      <td>0.826398</td>\n",
       "      <td>0.816770</td>\n",
       "      <td>0.822360</td>\n",
       "      <td>0.820441</td>\n",
       "      <td>0.812364</td>\n",
       "      <td>0.819667</td>\n",
       "      <td>0.004793</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>5.629390</td>\n",
       "      <td>0.022191</td>\n",
       "      <td>0.270758</td>\n",
       "      <td>0.005593</td>\n",
       "      <td>30</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 500}</td>\n",
       "      <td>0.827640</td>\n",
       "      <td>0.818012</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.823548</td>\n",
       "      <td>0.811432</td>\n",
       "      <td>0.820412</td>\n",
       "      <td>0.005468</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>9.229974</td>\n",
       "      <td>0.144019</td>\n",
       "      <td>0.458021</td>\n",
       "      <td>0.010275</td>\n",
       "      <td>30</td>\n",
       "      <td>800</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 800}</td>\n",
       "      <td>0.830124</td>\n",
       "      <td>0.813354</td>\n",
       "      <td>0.822981</td>\n",
       "      <td>0.824480</td>\n",
       "      <td>0.808947</td>\n",
       "      <td>0.819977</td>\n",
       "      <td>0.007717</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>11.806583</td>\n",
       "      <td>0.071645</td>\n",
       "      <td>0.574887</td>\n",
       "      <td>0.009825</td>\n",
       "      <td>30</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 1000}</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.815528</td>\n",
       "      <td>0.822981</td>\n",
       "      <td>0.823858</td>\n",
       "      <td>0.810500</td>\n",
       "      <td>0.820288</td>\n",
       "      <td>0.006435</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.157506</td>\n",
       "      <td>0.024636</td>\n",
       "      <td>0.057062</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>35</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 35, 'n_estimators': 100}</td>\n",
       "      <td>0.822360</td>\n",
       "      <td>0.810559</td>\n",
       "      <td>0.813975</td>\n",
       "      <td>0.817335</td>\n",
       "      <td>0.801491</td>\n",
       "      <td>0.813144</td>\n",
       "      <td>0.007009</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2.266299</td>\n",
       "      <td>0.015416</td>\n",
       "      <td>0.109291</td>\n",
       "      <td>0.002863</td>\n",
       "      <td>35</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 35, 'n_estimators': 200}</td>\n",
       "      <td>0.821739</td>\n",
       "      <td>0.812422</td>\n",
       "      <td>0.822360</td>\n",
       "      <td>0.821994</td>\n",
       "      <td>0.807704</td>\n",
       "      <td>0.817244</td>\n",
       "      <td>0.006053</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3.378601</td>\n",
       "      <td>0.023720</td>\n",
       "      <td>0.163736</td>\n",
       "      <td>0.003706</td>\n",
       "      <td>35</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 35, 'n_estimators': 300}</td>\n",
       "      <td>0.826398</td>\n",
       "      <td>0.812112</td>\n",
       "      <td>0.821118</td>\n",
       "      <td>0.824790</td>\n",
       "      <td>0.812675</td>\n",
       "      <td>0.819418</td>\n",
       "      <td>0.005989</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4.473022</td>\n",
       "      <td>0.012409</td>\n",
       "      <td>0.220085</td>\n",
       "      <td>0.009588</td>\n",
       "      <td>35</td>\n",
       "      <td>400</td>\n",
       "      <td>{'max_depth': 35, 'n_estimators': 400}</td>\n",
       "      <td>0.827329</td>\n",
       "      <td>0.813665</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.821994</td>\n",
       "      <td>0.814228</td>\n",
       "      <td>0.819729</td>\n",
       "      <td>0.005154</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5.645208</td>\n",
       "      <td>0.046911</td>\n",
       "      <td>0.279050</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>35</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 35, 'n_estimators': 500}</td>\n",
       "      <td>0.827640</td>\n",
       "      <td>0.814286</td>\n",
       "      <td>0.825155</td>\n",
       "      <td>0.821994</td>\n",
       "      <td>0.810500</td>\n",
       "      <td>0.819915</td>\n",
       "      <td>0.006508</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>9.163636</td>\n",
       "      <td>0.080356</td>\n",
       "      <td>0.444956</td>\n",
       "      <td>0.004991</td>\n",
       "      <td>35</td>\n",
       "      <td>800</td>\n",
       "      <td>{'max_depth': 35, 'n_estimators': 800}</td>\n",
       "      <td>0.828882</td>\n",
       "      <td>0.816149</td>\n",
       "      <td>0.823292</td>\n",
       "      <td>0.824169</td>\n",
       "      <td>0.805840</td>\n",
       "      <td>0.819666</td>\n",
       "      <td>0.008023</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>11.609316</td>\n",
       "      <td>0.147602</td>\n",
       "      <td>0.575605</td>\n",
       "      <td>0.009593</td>\n",
       "      <td>35</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 35, 'n_estimators': 1000}</td>\n",
       "      <td>0.827329</td>\n",
       "      <td>0.817081</td>\n",
       "      <td>0.823602</td>\n",
       "      <td>0.823858</td>\n",
       "      <td>0.806772</td>\n",
       "      <td>0.819729</td>\n",
       "      <td>0.007276</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.411443      0.010121         0.020357        0.000597   \n",
       "1        0.792946      0.021246         0.036564        0.002976   \n",
       "2        1.167702      0.023728         0.054850        0.001679   \n",
       "3        1.598454      0.036300         0.079807        0.010289   \n",
       "4        2.003466      0.041828         0.092758        0.005912   \n",
       "5        3.200616      0.073565         0.146300        0.002856   \n",
       "6        3.961413      0.052801         0.178615        0.002159   \n",
       "7        0.758636      0.036442         0.033649        0.002227   \n",
       "8        1.515033      0.138325         0.067491        0.003222   \n",
       "9        2.150137      0.072125         0.096304        0.005918   \n",
       "10       3.048869      0.263477         0.138487        0.006993   \n",
       "11       3.696555      0.241600         0.162404        0.014600   \n",
       "12       5.440744      0.034421         0.248233        0.008386   \n",
       "13       6.673083      0.140280         0.303600        0.004587   \n",
       "14       0.938587      0.024200         0.046389        0.001654   \n",
       "15       1.828865      0.008322         0.087851        0.002631   \n",
       "16       2.812448      0.026993         0.131371        0.001208   \n",
       "17       3.759496      0.039398         0.183108        0.004382   \n",
       "18       4.627345      0.063624         0.222309        0.009132   \n",
       "19       7.536066      0.095466         0.370397        0.006364   \n",
       "20       9.456755      0.102391         0.471243        0.008593   \n",
       "21       1.067395      0.011992         0.052982        0.002482   \n",
       "22       2.114477      0.010998         0.100454        0.000832   \n",
       "23       3.188295      0.027905         0.153440        0.002004   \n",
       "24       4.280581      0.057216         0.202270        0.007443   \n",
       "25       5.394105      0.112039         0.251276        0.001950   \n",
       "26       8.850856      0.054284         0.410372        0.008391   \n",
       "27      11.015884      0.124539         0.516344        0.014216   \n",
       "28       1.100297      0.014386         0.056506        0.002138   \n",
       "29       2.240584      0.053113         0.108751        0.003234   \n",
       "30       3.374285      0.019244         0.163400        0.002406   \n",
       "31       4.516173      0.051738         0.220194        0.008681   \n",
       "32       5.747314      0.071435         0.275002        0.002815   \n",
       "33       9.305920      0.066621         0.436677        0.004358   \n",
       "34      11.487637      0.154556         0.544431        0.012129   \n",
       "35       1.132784      0.018518         0.056671        0.000907   \n",
       "36       2.288341      0.024397         0.111456        0.001591   \n",
       "37       3.441381      0.073761         0.164334        0.003188   \n",
       "38       4.560962      0.068064         0.220080        0.002651   \n",
       "39       5.629390      0.022191         0.270758        0.005593   \n",
       "40       9.229974      0.144019         0.458021        0.010275   \n",
       "41      11.806583      0.071645         0.574887        0.009825   \n",
       "42       1.157506      0.024636         0.057062        0.001632   \n",
       "43       2.266299      0.015416         0.109291        0.002863   \n",
       "44       3.378601      0.023720         0.163736        0.003706   \n",
       "45       4.473022      0.012409         0.220085        0.009588   \n",
       "46       5.645208      0.046911         0.279050        0.002752   \n",
       "47       9.163636      0.080356         0.444956        0.004991   \n",
       "48      11.609316      0.147602         0.575605        0.009593   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "0                5                100   \n",
       "1                5                200   \n",
       "2                5                300   \n",
       "3                5                400   \n",
       "4                5                500   \n",
       "5                5                800   \n",
       "6                5               1000   \n",
       "7               10                100   \n",
       "8               10                200   \n",
       "9               10                300   \n",
       "10              10                400   \n",
       "11              10                500   \n",
       "12              10                800   \n",
       "13              10               1000   \n",
       "14              15                100   \n",
       "15              15                200   \n",
       "16              15                300   \n",
       "17              15                400   \n",
       "18              15                500   \n",
       "19              15                800   \n",
       "20              15               1000   \n",
       "21              20                100   \n",
       "22              20                200   \n",
       "23              20                300   \n",
       "24              20                400   \n",
       "25              20                500   \n",
       "26              20                800   \n",
       "27              20               1000   \n",
       "28              25                100   \n",
       "29              25                200   \n",
       "30              25                300   \n",
       "31              25                400   \n",
       "32              25                500   \n",
       "33              25                800   \n",
       "34              25               1000   \n",
       "35              30                100   \n",
       "36              30                200   \n",
       "37              30                300   \n",
       "38              30                400   \n",
       "39              30                500   \n",
       "40              30                800   \n",
       "41              30               1000   \n",
       "42              35                100   \n",
       "43              35                200   \n",
       "44              35                300   \n",
       "45              35                400   \n",
       "46              35                500   \n",
       "47              35                800   \n",
       "48              35               1000   \n",
       "\n",
       "                                     params  split0_test_score  \\\n",
       "0     {'max_depth': 5, 'n_estimators': 100}           0.648758   \n",
       "1     {'max_depth': 5, 'n_estimators': 200}           0.648758   \n",
       "2     {'max_depth': 5, 'n_estimators': 300}           0.653416   \n",
       "3     {'max_depth': 5, 'n_estimators': 400}           0.651863   \n",
       "4     {'max_depth': 5, 'n_estimators': 500}           0.653106   \n",
       "5     {'max_depth': 5, 'n_estimators': 800}           0.650000   \n",
       "6    {'max_depth': 5, 'n_estimators': 1000}           0.648137   \n",
       "7    {'max_depth': 10, 'n_estimators': 100}           0.708696   \n",
       "8    {'max_depth': 10, 'n_estimators': 200}           0.704969   \n",
       "9    {'max_depth': 10, 'n_estimators': 300}           0.709006   \n",
       "10   {'max_depth': 10, 'n_estimators': 400}           0.711180   \n",
       "11   {'max_depth': 10, 'n_estimators': 500}           0.714907   \n",
       "12   {'max_depth': 10, 'n_estimators': 800}           0.711801   \n",
       "13  {'max_depth': 10, 'n_estimators': 1000}           0.711180   \n",
       "14   {'max_depth': 15, 'n_estimators': 100}           0.790373   \n",
       "15   {'max_depth': 15, 'n_estimators': 200}           0.787578   \n",
       "16   {'max_depth': 15, 'n_estimators': 300}           0.788199   \n",
       "17   {'max_depth': 15, 'n_estimators': 400}           0.788820   \n",
       "18   {'max_depth': 15, 'n_estimators': 500}           0.790994   \n",
       "19   {'max_depth': 15, 'n_estimators': 800}           0.791304   \n",
       "20  {'max_depth': 15, 'n_estimators': 1000}           0.790062   \n",
       "21   {'max_depth': 20, 'n_estimators': 100}           0.815528   \n",
       "22   {'max_depth': 20, 'n_estimators': 200}           0.819565   \n",
       "23   {'max_depth': 20, 'n_estimators': 300}           0.821118   \n",
       "24   {'max_depth': 20, 'n_estimators': 400}           0.822360   \n",
       "25   {'max_depth': 20, 'n_estimators': 500}           0.823292   \n",
       "26   {'max_depth': 20, 'n_estimators': 800}           0.822050   \n",
       "27  {'max_depth': 20, 'n_estimators': 1000}           0.821739   \n",
       "28   {'max_depth': 25, 'n_estimators': 100}           0.816149   \n",
       "29   {'max_depth': 25, 'n_estimators': 200}           0.820186   \n",
       "30   {'max_depth': 25, 'n_estimators': 300}           0.826087   \n",
       "31   {'max_depth': 25, 'n_estimators': 400}           0.826087   \n",
       "32   {'max_depth': 25, 'n_estimators': 500}           0.824845   \n",
       "33   {'max_depth': 25, 'n_estimators': 800}           0.825466   \n",
       "34  {'max_depth': 25, 'n_estimators': 1000}           0.826398   \n",
       "35   {'max_depth': 30, 'n_estimators': 100}           0.814596   \n",
       "36   {'max_depth': 30, 'n_estimators': 200}           0.818944   \n",
       "37   {'max_depth': 30, 'n_estimators': 300}           0.827329   \n",
       "38   {'max_depth': 30, 'n_estimators': 400}           0.826398   \n",
       "39   {'max_depth': 30, 'n_estimators': 500}           0.827640   \n",
       "40   {'max_depth': 30, 'n_estimators': 800}           0.830124   \n",
       "41  {'max_depth': 30, 'n_estimators': 1000}           0.828571   \n",
       "42   {'max_depth': 35, 'n_estimators': 100}           0.822360   \n",
       "43   {'max_depth': 35, 'n_estimators': 200}           0.821739   \n",
       "44   {'max_depth': 35, 'n_estimators': 300}           0.826398   \n",
       "45   {'max_depth': 35, 'n_estimators': 400}           0.827329   \n",
       "46   {'max_depth': 35, 'n_estimators': 500}           0.827640   \n",
       "47   {'max_depth': 35, 'n_estimators': 800}           0.828882   \n",
       "48  {'max_depth': 35, 'n_estimators': 1000}           0.827329   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.646894           0.643789           0.641504   \n",
       "1            0.644410           0.648447           0.643057   \n",
       "2            0.642547           0.648758           0.640572   \n",
       "3            0.642547           0.646894           0.639640   \n",
       "4            0.642857           0.648447           0.639329   \n",
       "5            0.643478           0.644720           0.641504   \n",
       "6            0.644099           0.645342           0.642746   \n",
       "7            0.694410           0.707453           0.694315   \n",
       "8            0.697516           0.711801           0.694936   \n",
       "9            0.699068           0.713043           0.697111   \n",
       "10           0.700932           0.708075           0.694004   \n",
       "11           0.696584           0.706522           0.696800   \n",
       "12           0.696273           0.710248           0.697732   \n",
       "13           0.696894           0.710559           0.700528   \n",
       "14           0.772671           0.774224           0.775707   \n",
       "15           0.773292           0.776708           0.773532   \n",
       "16           0.774534           0.782609           0.776639   \n",
       "17           0.775466           0.780435           0.776639   \n",
       "18           0.776398           0.783230           0.775085   \n",
       "19           0.777640           0.783851           0.777260   \n",
       "20           0.776398           0.785714           0.776328   \n",
       "21           0.798758           0.807764           0.801802   \n",
       "22           0.801553           0.808385           0.811432   \n",
       "23           0.802484           0.810870           0.817024   \n",
       "24           0.806211           0.815528           0.815781   \n",
       "25           0.809317           0.814596           0.814849   \n",
       "26           0.809938           0.818012           0.812364   \n",
       "27           0.808696           0.818323           0.811743   \n",
       "28           0.811491           0.825466           0.820441   \n",
       "29           0.813043           0.822050           0.825412   \n",
       "30           0.811801           0.824534           0.823237   \n",
       "31           0.813665           0.822050           0.821994   \n",
       "32           0.811180           0.822360           0.817335   \n",
       "33           0.812422           0.822981           0.817956   \n",
       "34           0.813975           0.822360           0.818888   \n",
       "35           0.805280           0.818323           0.819199   \n",
       "36           0.816149           0.820497           0.822305   \n",
       "37           0.816149           0.821429           0.822616   \n",
       "38           0.816770           0.822360           0.820441   \n",
       "39           0.818012           0.821429           0.823548   \n",
       "40           0.813354           0.822981           0.824480   \n",
       "41           0.815528           0.822981           0.823858   \n",
       "42           0.810559           0.813975           0.817335   \n",
       "43           0.812422           0.822360           0.821994   \n",
       "44           0.812112           0.821118           0.824790   \n",
       "45           0.813665           0.821429           0.821994   \n",
       "46           0.814286           0.825155           0.821994   \n",
       "47           0.816149           0.823292           0.824169   \n",
       "48           0.817081           0.823602           0.823858   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.631873         0.642564        0.005900               47  \n",
       "1            0.634048         0.643744        0.005333               44  \n",
       "2            0.634358         0.643930        0.006605               43  \n",
       "3            0.634669         0.643123        0.005908               46  \n",
       "4            0.632495         0.643247        0.007146               45  \n",
       "5            0.632495         0.642439        0.005714               48  \n",
       "6            0.631563         0.642377        0.005692               49  \n",
       "7            0.685306         0.698036        0.008847               42  \n",
       "8            0.694004         0.700645        0.006777               41  \n",
       "9            0.691519         0.701950        0.007917               38  \n",
       "10           0.693072         0.701453        0.007273               40  \n",
       "11           0.693383         0.701639        0.007962               39  \n",
       "12           0.694004         0.702012        0.007470               37  \n",
       "13           0.693072         0.702447        0.007273               36  \n",
       "14           0.782852         0.779165        0.006599               34  \n",
       "15           0.778813         0.777985        0.005219               35  \n",
       "16           0.780056         0.780407        0.004783               32  \n",
       "17           0.778503         0.779972        0.004734               33  \n",
       "18           0.780056         0.781152        0.005688               30  \n",
       "19           0.778192         0.781649        0.005392               29  \n",
       "20           0.775707         0.780842        0.005920               31  \n",
       "21           0.795899         0.803950        0.007000               28  \n",
       "22           0.801180         0.808423        0.006824               27  \n",
       "23           0.803976         0.811094        0.007218               26  \n",
       "24           0.803044         0.812585        0.007015               24  \n",
       "25           0.801180         0.812647        0.007274               23  \n",
       "26           0.802423         0.812957        0.006763               22  \n",
       "27           0.801802         0.812460        0.007053               25  \n",
       "28           0.810811         0.816872        0.005528               17  \n",
       "29           0.811432         0.818425        0.005346               12  \n",
       "30           0.811121         0.819356        0.006513               11  \n",
       "31           0.805530         0.817865        0.007375               14  \n",
       "32           0.808015         0.816747        0.006398               18  \n",
       "33           0.804908         0.816747        0.007419               19  \n",
       "34           0.806462         0.817616        0.006913               15  \n",
       "35           0.810500         0.813579        0.005167               20  \n",
       "36           0.813607         0.818300        0.003096               13  \n",
       "37           0.810500         0.819605        0.005777                9  \n",
       "38           0.812364         0.819667        0.004793                7  \n",
       "39           0.811432         0.820412        0.005468                1  \n",
       "40           0.808947         0.819977        0.007717                3  \n",
       "41           0.810500         0.820288        0.006435                2  \n",
       "42           0.801491         0.813144        0.007009               21  \n",
       "43           0.807704         0.817244        0.006053               16  \n",
       "44           0.812675         0.819418        0.005989               10  \n",
       "45           0.814228         0.819729        0.005154                5  \n",
       "46           0.810500         0.819915        0.006508                4  \n",
       "47           0.805840         0.819666        0.008023                8  \n",
       "48           0.806772         0.819729        0.007276                6  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gscv_rf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6741210088209716\n",
      "0.6634364517331346\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.63      0.65      2715\n",
      "           1       0.65      0.70      0.67      2651\n",
      "\n",
      "    accuracy                           0.66      5366\n",
      "   macro avg       0.66      0.66      0.66      5366\n",
      "weighted avg       0.66      0.66      0.66      5366\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1707, 1008],\n",
       "       [ 798, 1853]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xgboost\n",
    "xgb_clf = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = xgb_clf.predict(X_train)\n",
    "y_pred_test = xgb_clf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_train, y_pred_train))\n",
    "print(accuracy_score(y_test, y_pred_test))\n",
    "# print(classification_report(y_train, y_pred_train))\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "confusion_matrix(y_test, y_pred_test, labels=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.05, max_bin=256, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=20, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=500,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, ...)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "xgb_clf = XGBClassifier(random_state=0)\n",
    "params = {\n",
    "    \"max_depth\": [3,5,7,10,15,20,25],\n",
    "    \"n_estimators\": [100, 300, 500],\n",
    "    'learning_rate' : [0.01,0.05,0.1]\n",
    "}\n",
    "gscv_xg = GridSearchCV (xgb_clf, params, scoring ='accuracy', cv = skf)\n",
    "gscv_xg.fit(X_train, y_train)\n",
    "print(gscv_xg.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.05, max_bin=256, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=23, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=700,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, ...)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "xgb_clf = XGBClassifier(random_state=0)\n",
    "params = {\n",
    "    \"max_depth\": [18,19,20,21,22,23],\n",
    "    \"n_estimators\": [400,450, 500,550,600,700],\n",
    "    'learning_rate' : [0.05]\n",
    "}\n",
    "gscv_xg = GridSearchCV (xgb_clf, params, scoring ='accuracy', cv = skf)\n",
    "gscv_xg.fit(X_train, y_train)\n",
    "print(gscv_xg.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.05, max_bin=256, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=23, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=800,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, ...)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "xgb_clf = XGBClassifier(random_state=0)\n",
    "params = {\n",
    "    \"max_depth\": [23,24,25],\n",
    "    \"n_estimators\": [650,700,800,1000],\n",
    "    'learning_rate' : [0.05]\n",
    "}\n",
    "gscv_xg = GridSearchCV (xgb_clf, params, scoring ='accuracy', cv = skf)\n",
    "gscv_xg.fit(X_train, y_train)\n",
    "print(gscv_xg.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.180436</td>\n",
       "      <td>0.006167</td>\n",
       "      <td>0.005074</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 3, 'n_est...</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.631677</td>\n",
       "      <td>0.622981</td>\n",
       "      <td>0.624107</td>\n",
       "      <td>0.622554</td>\n",
       "      <td>0.626351</td>\n",
       "      <td>0.003895</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.518502</td>\n",
       "      <td>0.022714</td>\n",
       "      <td>0.005011</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 3, 'n_est...</td>\n",
       "      <td>0.655901</td>\n",
       "      <td>0.644099</td>\n",
       "      <td>0.641925</td>\n",
       "      <td>0.644921</td>\n",
       "      <td>0.633737</td>\n",
       "      <td>0.644117</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.828115</td>\n",
       "      <td>0.065859</td>\n",
       "      <td>0.005504</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 3, 'n_est...</td>\n",
       "      <td>0.660870</td>\n",
       "      <td>0.653727</td>\n",
       "      <td>0.645963</td>\n",
       "      <td>0.652066</td>\n",
       "      <td>0.645853</td>\n",
       "      <td>0.651696</td>\n",
       "      <td>0.005575</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.283682</td>\n",
       "      <td>0.020203</td>\n",
       "      <td>0.005204</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 5, 'n_est...</td>\n",
       "      <td>0.655901</td>\n",
       "      <td>0.631988</td>\n",
       "      <td>0.650932</td>\n",
       "      <td>0.641504</td>\n",
       "      <td>0.637154</td>\n",
       "      <td>0.643496</td>\n",
       "      <td>0.008784</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.782597</td>\n",
       "      <td>0.035522</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 5, 'n_est...</td>\n",
       "      <td>0.666149</td>\n",
       "      <td>0.659006</td>\n",
       "      <td>0.663665</td>\n",
       "      <td>0.667599</td>\n",
       "      <td>0.659211</td>\n",
       "      <td>0.663126</td>\n",
       "      <td>0.003514</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>3.673918</td>\n",
       "      <td>0.069603</td>\n",
       "      <td>0.016322</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>300</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 20, 'n_est...</td>\n",
       "      <td>0.806522</td>\n",
       "      <td>0.806832</td>\n",
       "      <td>0.805590</td>\n",
       "      <td>0.807083</td>\n",
       "      <td>0.797142</td>\n",
       "      <td>0.804634</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5.443889</td>\n",
       "      <td>0.077368</td>\n",
       "      <td>0.024132</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 20, 'n_est...</td>\n",
       "      <td>0.807143</td>\n",
       "      <td>0.807453</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.797453</td>\n",
       "      <td>0.804199</td>\n",
       "      <td>0.003604</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1.964846</td>\n",
       "      <td>0.148565</td>\n",
       "      <td>0.008242</td>\n",
       "      <td>0.001166</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 25, 'n_est...</td>\n",
       "      <td>0.808696</td>\n",
       "      <td>0.804658</td>\n",
       "      <td>0.803106</td>\n",
       "      <td>0.800870</td>\n",
       "      <td>0.791550</td>\n",
       "      <td>0.801776</td>\n",
       "      <td>0.005716</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>4.189157</td>\n",
       "      <td>0.066287</td>\n",
       "      <td>0.018003</td>\n",
       "      <td>0.002367</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25</td>\n",
       "      <td>300</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 25, 'n_est...</td>\n",
       "      <td>0.806832</td>\n",
       "      <td>0.804037</td>\n",
       "      <td>0.801863</td>\n",
       "      <td>0.805530</td>\n",
       "      <td>0.799006</td>\n",
       "      <td>0.803454</td>\n",
       "      <td>0.002770</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>6.141532</td>\n",
       "      <td>0.097087</td>\n",
       "      <td>0.023187</td>\n",
       "      <td>0.002394</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25</td>\n",
       "      <td>500</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 25, 'n_est...</td>\n",
       "      <td>0.807453</td>\n",
       "      <td>0.802795</td>\n",
       "      <td>0.801242</td>\n",
       "      <td>0.806151</td>\n",
       "      <td>0.793725</td>\n",
       "      <td>0.802273</td>\n",
       "      <td>0.004823</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.180436      0.006167         0.005074        0.000435   \n",
       "1        0.518502      0.022714         0.005011        0.000315   \n",
       "2        0.828115      0.065859         0.005504        0.000552   \n",
       "3        0.283682      0.020203         0.005204        0.000675   \n",
       "4        0.782597      0.035522         0.005908        0.000585   \n",
       "..            ...           ...              ...             ...   \n",
       "58       3.673918      0.069603         0.016322        0.002087   \n",
       "59       5.443889      0.077368         0.024132        0.002773   \n",
       "60       1.964846      0.148565         0.008242        0.001166   \n",
       "61       4.189157      0.066287         0.018003        0.002367   \n",
       "62       6.141532      0.097087         0.023187        0.002394   \n",
       "\n",
       "   param_learning_rate param_max_depth param_n_estimators  \\\n",
       "0                 0.01               3                100   \n",
       "1                 0.01               3                300   \n",
       "2                 0.01               3                500   \n",
       "3                 0.01               5                100   \n",
       "4                 0.01               5                300   \n",
       "..                 ...             ...                ...   \n",
       "58                 0.1              20                300   \n",
       "59                 0.1              20                500   \n",
       "60                 0.1              25                100   \n",
       "61                 0.1              25                300   \n",
       "62                 0.1              25                500   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'learning_rate': 0.01, 'max_depth': 3, 'n_est...           0.630435   \n",
       "1   {'learning_rate': 0.01, 'max_depth': 3, 'n_est...           0.655901   \n",
       "2   {'learning_rate': 0.01, 'max_depth': 3, 'n_est...           0.660870   \n",
       "3   {'learning_rate': 0.01, 'max_depth': 5, 'n_est...           0.655901   \n",
       "4   {'learning_rate': 0.01, 'max_depth': 5, 'n_est...           0.666149   \n",
       "..                                                ...                ...   \n",
       "58  {'learning_rate': 0.1, 'max_depth': 20, 'n_est...           0.806522   \n",
       "59  {'learning_rate': 0.1, 'max_depth': 20, 'n_est...           0.807143   \n",
       "60  {'learning_rate': 0.1, 'max_depth': 25, 'n_est...           0.808696   \n",
       "61  {'learning_rate': 0.1, 'max_depth': 25, 'n_est...           0.806832   \n",
       "62  {'learning_rate': 0.1, 'max_depth': 25, 'n_est...           0.807453   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.631677           0.622981           0.624107   \n",
       "1            0.644099           0.641925           0.644921   \n",
       "2            0.653727           0.645963           0.652066   \n",
       "3            0.631988           0.650932           0.641504   \n",
       "4            0.659006           0.663665           0.667599   \n",
       "..                ...                ...                ...   \n",
       "58           0.806832           0.805590           0.807083   \n",
       "59           0.807453           0.804348           0.804598   \n",
       "60           0.804658           0.803106           0.800870   \n",
       "61           0.804037           0.801863           0.805530   \n",
       "62           0.802795           0.801242           0.806151   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.622554         0.626351        0.003895               63  \n",
       "1            0.633737         0.644117        0.007100               61  \n",
       "2            0.645853         0.651696        0.005575               59  \n",
       "3            0.637154         0.643496        0.008784               62  \n",
       "4            0.659211         0.663126        0.003514               54  \n",
       "..                ...              ...             ...              ...  \n",
       "58           0.797142         0.804634        0.003780                4  \n",
       "59           0.797453         0.804199        0.003604                6  \n",
       "60           0.791550         0.801776        0.005716               14  \n",
       "61           0.799006         0.803454        0.002770                8  \n",
       "62           0.793725         0.802273        0.004823               12  \n",
       "\n",
       "[63 rows x 16 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gscv_xg.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[996]\tvalid_0's binary_logloss: 0.471651\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002468 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[996]\tvalid_0's binary_logloss: 0.466149\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000766 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[979]\tvalid_0's binary_logloss: 0.455821\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000844 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[994]\tvalid_0's binary_logloss: 0.467889\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000759 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[999]\tvalid_0's binary_logloss: 0.46422\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000921 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1140]\tvalid_0's binary_logloss: 0.469779\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000808 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1306]\tvalid_0's binary_logloss: 0.460941\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000842 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1283]\tvalid_0's binary_logloss: 0.452066\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000752 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1054]\tvalid_0's binary_logloss: 0.467248\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000770 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1281]\tvalid_0's binary_logloss: 0.461524\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1140]\tvalid_0's binary_logloss: 0.469779\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002133 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1306]\tvalid_0's binary_logloss: 0.460941\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1283]\tvalid_0's binary_logloss: 0.452066\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000665 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1054]\tvalid_0's binary_logloss: 0.467248\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000813 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1281]\tvalid_0's binary_logloss: 0.461524\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000738 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[731]\tvalid_0's binary_logloss: 0.46689\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000774 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[801]\tvalid_0's binary_logloss: 0.466443\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000925 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[757]\tvalid_0's binary_logloss: 0.460734\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000674 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[739]\tvalid_0's binary_logloss: 0.469217\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000772 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[774]\tvalid_0's binary_logloss: 0.468469\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000821 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[731]\tvalid_0's binary_logloss: 0.46689\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000864 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[801]\tvalid_0's binary_logloss: 0.466443\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001075 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[757]\tvalid_0's binary_logloss: 0.460734\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001141 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[739]\tvalid_0's binary_logloss: 0.469217\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[774]\tvalid_0's binary_logloss: 0.468469\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000809 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[731]\tvalid_0's binary_logloss: 0.46689\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000935 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[801]\tvalid_0's binary_logloss: 0.466443\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000795 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[757]\tvalid_0's binary_logloss: 0.460734\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[739]\tvalid_0's binary_logloss: 0.469217\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[774]\tvalid_0's binary_logloss: 0.468469\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[626]\tvalid_0's binary_logloss: 0.473792\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001147 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[573]\tvalid_0's binary_logloss: 0.474005\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001057 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[662]\tvalid_0's binary_logloss: 0.465093\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000799 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[576]\tvalid_0's binary_logloss: 0.473279\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000759 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[557]\tvalid_0's binary_logloss: 0.47704\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000691 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[626]\tvalid_0's binary_logloss: 0.473792\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000873 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[573]\tvalid_0's binary_logloss: 0.474005\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000863 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[662]\tvalid_0's binary_logloss: 0.465093\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000845 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[576]\tvalid_0's binary_logloss: 0.473279\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001527 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[557]\tvalid_0's binary_logloss: 0.47704\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[626]\tvalid_0's binary_logloss: 0.473792\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000831 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[573]\tvalid_0's binary_logloss: 0.474005\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000933 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[662]\tvalid_0's binary_logloss: 0.465093\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000970 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[576]\tvalid_0's binary_logloss: 0.473279\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000766 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[557]\tvalid_0's binary_logloss: 0.47704\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000843 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[186]\tvalid_0's binary_logloss: 0.522903\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[254]\tvalid_0's binary_logloss: 0.523243\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[204]\tvalid_0's binary_logloss: 0.519538\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000775 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[137]\tvalid_0's binary_logloss: 0.533446\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000831 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[237]\tvalid_0's binary_logloss: 0.525805\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000851 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[186]\tvalid_0's binary_logloss: 0.522903\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002700 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[254]\tvalid_0's binary_logloss: 0.523243\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000887 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[204]\tvalid_0's binary_logloss: 0.519538\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000805 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[137]\tvalid_0's binary_logloss: 0.533446\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000790 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[237]\tvalid_0's binary_logloss: 0.525805\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000794 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[186]\tvalid_0's binary_logloss: 0.522903\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000797 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[254]\tvalid_0's binary_logloss: 0.523243\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[204]\tvalid_0's binary_logloss: 0.519538\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001917 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[137]\tvalid_0's binary_logloss: 0.533446\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002365 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[237]\tvalid_0's binary_logloss: 0.525805\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 8081, number of negative: 8017\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000965 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 867\n",
      "[LightGBM] [Info] Number of data points in the train set: 16098, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501988 -> initscore=0.007951\n",
      "[LightGBM] [Info] Start training from score 0.007951\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1499]\tvalid_0's binary_logloss: 0.405889\n",
      "LGBMClassifier(early_stopping_rounds=100, n_estimators=1500, random_state=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.765491</td>\n",
       "      <td>0.021304</td>\n",
       "      <td>0.031073</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 1000}</td>\n",
       "      <td>0.786646</td>\n",
       "      <td>0.786025</td>\n",
       "      <td>0.790373</td>\n",
       "      <td>0.792793</td>\n",
       "      <td>0.787512</td>\n",
       "      <td>0.788670</td>\n",
       "      <td>0.002543</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.991288</td>\n",
       "      <td>0.086752</td>\n",
       "      <td>0.039980</td>\n",
       "      <td>0.005023</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1500</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 1500}</td>\n",
       "      <td>0.787888</td>\n",
       "      <td>0.789130</td>\n",
       "      <td>0.791615</td>\n",
       "      <td>0.789376</td>\n",
       "      <td>0.792171</td>\n",
       "      <td>0.790036</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.989509</td>\n",
       "      <td>0.081493</td>\n",
       "      <td>0.039817</td>\n",
       "      <td>0.004039</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2000</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 2000}</td>\n",
       "      <td>0.787888</td>\n",
       "      <td>0.789130</td>\n",
       "      <td>0.791615</td>\n",
       "      <td>0.789376</td>\n",
       "      <td>0.792171</td>\n",
       "      <td>0.790036</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.630683</td>\n",
       "      <td>0.019075</td>\n",
       "      <td>0.022881</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'learning_rate': 0.15, 'n_estimators': 1000}</td>\n",
       "      <td>0.786957</td>\n",
       "      <td>0.789441</td>\n",
       "      <td>0.792547</td>\n",
       "      <td>0.794967</td>\n",
       "      <td>0.783162</td>\n",
       "      <td>0.789415</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.646376</td>\n",
       "      <td>0.018817</td>\n",
       "      <td>0.023720</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1500</td>\n",
       "      <td>{'learning_rate': 0.15, 'n_estimators': 1500}</td>\n",
       "      <td>0.786957</td>\n",
       "      <td>0.789441</td>\n",
       "      <td>0.792547</td>\n",
       "      <td>0.794967</td>\n",
       "      <td>0.783162</td>\n",
       "      <td>0.789415</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.685742</td>\n",
       "      <td>0.052163</td>\n",
       "      <td>0.024275</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2000</td>\n",
       "      <td>{'learning_rate': 0.15, 'n_estimators': 2000}</td>\n",
       "      <td>0.786957</td>\n",
       "      <td>0.789441</td>\n",
       "      <td>0.792547</td>\n",
       "      <td>0.794967</td>\n",
       "      <td>0.783162</td>\n",
       "      <td>0.789415</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.532945</td>\n",
       "      <td>0.027392</td>\n",
       "      <td>0.019730</td>\n",
       "      <td>0.003197</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'learning_rate': 0.2, 'n_estimators': 1000}</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.783851</td>\n",
       "      <td>0.788820</td>\n",
       "      <td>0.790308</td>\n",
       "      <td>0.775707</td>\n",
       "      <td>0.784880</td>\n",
       "      <td>0.005116</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.529747</td>\n",
       "      <td>0.029764</td>\n",
       "      <td>0.017322</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1500</td>\n",
       "      <td>{'learning_rate': 0.2, 'n_estimators': 1500}</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.783851</td>\n",
       "      <td>0.788820</td>\n",
       "      <td>0.790308</td>\n",
       "      <td>0.775707</td>\n",
       "      <td>0.784880</td>\n",
       "      <td>0.005116</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.524308</td>\n",
       "      <td>0.022411</td>\n",
       "      <td>0.020594</td>\n",
       "      <td>0.005851</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2000</td>\n",
       "      <td>{'learning_rate': 0.2, 'n_estimators': 2000}</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.783851</td>\n",
       "      <td>0.788820</td>\n",
       "      <td>0.790308</td>\n",
       "      <td>0.775707</td>\n",
       "      <td>0.784880</td>\n",
       "      <td>0.005116</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.236598</td>\n",
       "      <td>0.029743</td>\n",
       "      <td>0.007111</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'learning_rate': 0.5, 'n_estimators': 1000}</td>\n",
       "      <td>0.779503</td>\n",
       "      <td>0.783851</td>\n",
       "      <td>0.771118</td>\n",
       "      <td>0.752097</td>\n",
       "      <td>0.767940</td>\n",
       "      <td>0.770902</td>\n",
       "      <td>0.010992</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.239081</td>\n",
       "      <td>0.027058</td>\n",
       "      <td>0.006714</td>\n",
       "      <td>0.001962</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1500</td>\n",
       "      <td>{'learning_rate': 0.5, 'n_estimators': 1500}</td>\n",
       "      <td>0.779503</td>\n",
       "      <td>0.783851</td>\n",
       "      <td>0.771118</td>\n",
       "      <td>0.752097</td>\n",
       "      <td>0.767940</td>\n",
       "      <td>0.770902</td>\n",
       "      <td>0.010992</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.245046</td>\n",
       "      <td>0.027113</td>\n",
       "      <td>0.007468</td>\n",
       "      <td>0.001694</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2000</td>\n",
       "      <td>{'learning_rate': 0.5, 'n_estimators': 2000}</td>\n",
       "      <td>0.779503</td>\n",
       "      <td>0.783851</td>\n",
       "      <td>0.771118</td>\n",
       "      <td>0.752097</td>\n",
       "      <td>0.767940</td>\n",
       "      <td>0.770902</td>\n",
       "      <td>0.010992</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.765491      0.021304         0.031073        0.001533   \n",
       "1        0.991288      0.086752         0.039980        0.005023   \n",
       "2        0.989509      0.081493         0.039817        0.004039   \n",
       "3        0.630683      0.019075         0.022881        0.001632   \n",
       "4        0.646376      0.018817         0.023720        0.002500   \n",
       "5        0.685742      0.052163         0.024275        0.001837   \n",
       "6        0.532945      0.027392         0.019730        0.003197   \n",
       "7        0.529747      0.029764         0.017322        0.001141   \n",
       "8        0.524308      0.022411         0.020594        0.005851   \n",
       "9        0.236598      0.029743         0.007111        0.001011   \n",
       "10       0.239081      0.027058         0.006714        0.001962   \n",
       "11       0.245046      0.027113         0.007468        0.001694   \n",
       "\n",
       "   param_learning_rate param_n_estimators  \\\n",
       "0                  0.1               1000   \n",
       "1                  0.1               1500   \n",
       "2                  0.1               2000   \n",
       "3                 0.15               1000   \n",
       "4                 0.15               1500   \n",
       "5                 0.15               2000   \n",
       "6                  0.2               1000   \n",
       "7                  0.2               1500   \n",
       "8                  0.2               2000   \n",
       "9                  0.5               1000   \n",
       "10                 0.5               1500   \n",
       "11                 0.5               2000   \n",
       "\n",
       "                                           params  split0_test_score  \\\n",
       "0    {'learning_rate': 0.1, 'n_estimators': 1000}           0.786646   \n",
       "1    {'learning_rate': 0.1, 'n_estimators': 1500}           0.787888   \n",
       "2    {'learning_rate': 0.1, 'n_estimators': 2000}           0.787888   \n",
       "3   {'learning_rate': 0.15, 'n_estimators': 1000}           0.786957   \n",
       "4   {'learning_rate': 0.15, 'n_estimators': 1500}           0.786957   \n",
       "5   {'learning_rate': 0.15, 'n_estimators': 2000}           0.786957   \n",
       "6    {'learning_rate': 0.2, 'n_estimators': 1000}           0.785714   \n",
       "7    {'learning_rate': 0.2, 'n_estimators': 1500}           0.785714   \n",
       "8    {'learning_rate': 0.2, 'n_estimators': 2000}           0.785714   \n",
       "9    {'learning_rate': 0.5, 'n_estimators': 1000}           0.779503   \n",
       "10   {'learning_rate': 0.5, 'n_estimators': 1500}           0.779503   \n",
       "11   {'learning_rate': 0.5, 'n_estimators': 2000}           0.779503   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.786025           0.790373           0.792793   \n",
       "1            0.789130           0.791615           0.789376   \n",
       "2            0.789130           0.791615           0.789376   \n",
       "3            0.789441           0.792547           0.794967   \n",
       "4            0.789441           0.792547           0.794967   \n",
       "5            0.789441           0.792547           0.794967   \n",
       "6            0.783851           0.788820           0.790308   \n",
       "7            0.783851           0.788820           0.790308   \n",
       "8            0.783851           0.788820           0.790308   \n",
       "9            0.783851           0.771118           0.752097   \n",
       "10           0.783851           0.771118           0.752097   \n",
       "11           0.783851           0.771118           0.752097   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.787512         0.788670        0.002543                6  \n",
       "1            0.792171         0.790036        0.001608                1  \n",
       "2            0.792171         0.790036        0.001608                1  \n",
       "3            0.783162         0.789415        0.004142                3  \n",
       "4            0.783162         0.789415        0.004142                3  \n",
       "5            0.783162         0.789415        0.004142                3  \n",
       "6            0.775707         0.784880        0.005116                7  \n",
       "7            0.775707         0.784880        0.005116                7  \n",
       "8            0.775707         0.784880        0.005116                7  \n",
       "9            0.767940         0.770902        0.010992               10  \n",
       "10           0.767940         0.770902        0.010992               10  \n",
       "11           0.767940         0.770902        0.010992               10  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LightGBM\n",
    "from lightgbm import LGBMClassifier\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "lgbm_clf = LGBMClassifier(random_state=0, early_stopping_rounds=100)\n",
    "params = {\n",
    "    \"n_estimators\": [1000,1500,2000],\n",
    "    'learning_rate' : [0.1,0.15,0.2,0.5]\n",
    "}\n",
    "gscv_lgbm = GridSearchCV (lgbm_clf, params, scoring ='accuracy', cv = skf)\n",
    "gscv_lgbm.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
    "print(gscv_lgbm.best_estimator_)\n",
    "pd.DataFrame(gscv_lgbm.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier(early_stopping_rounds=100, n_estimators=1500, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(gscv_lgbm.best_estimator_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 4가지 모델의 최적 파라미터를 사용하여 5-foldvalidation을 이용하여 정확도, 정밀도, 재현율을 비교하여 최종 모델을 선정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8586, number of negative: 8585\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4431\n",
      "[LightGBM] [Info] Number of data points in the train set: 17171, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500029 -> initscore=0.000116\n",
      "[LightGBM] [Info] Start training from score 0.000116\n",
      "[LightGBM] [Info] Number of positive: 8586, number of negative: 8585\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002336 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4412\n",
      "[LightGBM] [Info] Number of data points in the train set: 17171, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500029 -> initscore=0.000116\n",
      "[LightGBM] [Info] Start training from score 0.000116\n",
      "[LightGBM] [Info] Number of positive: 8585, number of negative: 8586\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001978 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4199\n",
      "[LightGBM] [Info] Number of data points in the train set: 17171, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499971 -> initscore=-0.000116\n",
      "[LightGBM] [Info] Start training from score -0.000116\n",
      "[LightGBM] [Info] Number of positive: 8585, number of negative: 8586\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002023 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4166\n",
      "[LightGBM] [Info] Number of data points in the train set: 17171, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499971 -> initscore=-0.000116\n",
      "[LightGBM] [Info] Start training from score -0.000116\n",
      "[LightGBM] [Info] Number of positive: 8586, number of negative: 8586\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003462 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4336\n",
      "[LightGBM] [Info] Number of data points in the train set: 17172, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "의사결정나무 정확도: 0.6995951709679751\n",
      "의사결정나무 정밀도: 0.7512877969279211\n",
      "의사결정나무 재현율: 0.6994063543009145\n",
      "랜덤포레스트 정확도: 0.7757249550569872\n",
      "랜덤포레스트 정밀도: 0.7861506110046139\n",
      "랜덤포레스트 재현율: 0.88166643588162\n",
      "xgboost 정확도: 0.756156308119006\n",
      "xgboost 정밀도: 0.7844645569174908\n",
      "xgboost 재현율: 0.8393627120527526\n",
      "lightGBM 정확도: 0.7478630766963016\n",
      "lightGBM 정밀도: 0.7819028697159789\n",
      "lightGBM 재현율: 0.8310703810470927\n"
     ]
    }
   ],
   "source": [
    "# 최종 모델 비교\n",
    "dt_clf = DecisionTreeClassifier(max_depth=25, random_state=0)\n",
    "rf_clf = RandomForestClassifier(max_depth=34, n_estimators=550, random_state=0)\n",
    "xgb_clf = XGBClassifier(learning_rate=0.05, max_depth=23, n_estimators=800, random_state=0)\n",
    "lgbm_clf = LGBMClassifier(n_estimators=1500, learning_rate=0.1, random_state=0)\n",
    "\n",
    "stratified_kfold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# k번 반복하면서 평가한 정확도를 저장할 배열\n",
    "dt_accuracy = []\n",
    "dt_precision = []\n",
    "dt_recall = []\n",
    "rf_accuracy = []\n",
    "rf_precision = []\n",
    "rf_recall = []\n",
    "xgb_accuracy = []\n",
    "xgb_precision = []\n",
    "xgb_recall = []\n",
    "lgbm_accuracy = []\n",
    "lgbm_precision = []\n",
    "lgbm_recall = []\n",
    "\n",
    "for train_index, val_index in stratified_kfold.split(X_new, y_new):\n",
    "    X_train, y_train = X_new.iloc[train_index], y_new.iloc[train_index]\n",
    "    X_val, y_val = X_new.iloc[val_index], y_new.iloc[val_index]\n",
    "\n",
    "    dt_clf.fit(X_train, y_train)\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "    xgb_clf.fit(X_train, y_train)\n",
    "    lgbm_clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_dt = dt_clf.predict(X_val)   # 검증 데이터로 예측\n",
    "    y_pred_rf = rf_clf.predict(X_val)   # 검증 데이터로 예측\n",
    "    y_pred_xgb = xgb_clf.predict(X_val)   # 검증 데이터로 예측\n",
    "    y_pred_lgbm = lgbm_clf.predict(X_val)   # 검증 데이터로 예측\n",
    "\n",
    "    dt_accuracy.append(accuracy_score(y_val, y_pred_dt)) \n",
    "    rf_accuracy.append(accuracy_score(y_val, y_pred_rf)) \n",
    "    xgb_accuracy.append(accuracy_score(y_val, y_pred_xgb)) \n",
    "    lgbm_accuracy.append(accuracy_score(y_val, y_pred_lgbm)) \n",
    "    \n",
    "    dt_precision.append(precision_score(y_val, y_pred_dt)) \n",
    "    rf_precision.append(precision_score(y_val, y_pred_rf)) \n",
    "    xgb_precision.append(precision_score(y_val, y_pred_xgb)) \n",
    "    lgbm_precision.append(precision_score(y_val, y_pred_lgbm)) \n",
    "    \n",
    "    dt_recall.append(recall_score(y_val, y_pred_dt)) \n",
    "    rf_recall.append(recall_score(y_val, y_pred_rf)) \n",
    "    xgb_recall.append(recall_score(y_val, y_pred_xgb)) \n",
    "    lgbm_recall.append(recall_score(y_val, y_pred_lgbm)) \n",
    "\n",
    "\n",
    "print(\"의사결정나무 정확도:\", np.mean(dt_accuracy))\n",
    "print(\"의사결정나무 정밀도:\", np.mean(dt_precision))\n",
    "print(\"의사결정나무 재현율:\", np.mean(dt_recall))\n",
    "\n",
    "print(\"랜덤포레스트 정확도:\", np.mean(rf_accuracy))\n",
    "print(\"랜덤포레스트 정밀도:\", np.mean(rf_precision))\n",
    "print(\"랜덤포레스트 재현율:\", np.mean(rf_recall))\n",
    "\n",
    "print(\"xgboost 정확도:\", np.mean(xgb_accuracy))\n",
    "print(\"xgboost 정밀도:\", np.mean(xgb_precision))\n",
    "print(\"xgboost 재현율:\", np.mean(xgb_recall))\n",
    "\n",
    "print(\"lightGBM 정확도:\", np.mean(lgbm_accuracy))\n",
    "print(\"lightGBM 정밀도:\", np.mean(lgbm_precision))\n",
    "print(\"lightGBM 재현율:\", np.mean(lgbm_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "랜덤포레스트 최종모델로 선정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_prob_data = pd.DataFrame({\n",
    "    'branch': [1, 1],\n",
    "    'found': [1, 1],\n",
    "    'course': [1, 1],\n",
    "    'daynight': [0, 0],\n",
    "    'major': [1, 1],\n",
    "    'school': [1, 1],\n",
    "    'school_area': [1, 1],\n",
    "    'sex': [0, 0],\n",
    "    'age': [24, 24],\n",
    "    'f009': [1, 1],\n",
    "    'i001': [1, 1],\n",
    "    'i033': [850, 900],\n",
    "    'i037': [140, 140],\n",
    "    'i042': [100, 100],\n",
    "    'i046': [400, 400],\n",
    "    'i066': [800, 0],\n",
    "    'i076': [800, 800],\n",
    "    'l001': [1, 1],\n",
    "    'l009': [1, 1],\n",
    "    'l016': [500, 500],\n",
    "    'm002': [5, 5],\n",
    "    'k110': [1, 1],\n",
    "    'k004': [1, 1],\n",
    "    'k007': [1, 1],\n",
    "    'k009': [1, 1],\n",
    "    'k011': [1, 1],\n",
    "    'q001': [3, 3],\n",
    "    'q002': [1, 1],\n",
    "    'q003': [1, 1],\n",
    "    'q004': [1, 1],\n",
    "    'q006': [0, 0],\n",
    "    'p001': [1, 1],\n",
    "    'p026': [1, 1],\n",
    "    'p029': [1, 1],\n",
    "    'p036': [1, 1],\n",
    "    'p045': [1, 1]\n",
    "})\n",
    "rf_clf = RandomForestClassifier(max_depth=34, n_estimators=550, random_state=0)\n",
    "rf_clf.fit(X_res, y_res)\n",
    "\n",
    "test_pred = rf_clf.predict(high_prob_data)\n",
    "\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_prob_data = pd.DataFrame({\n",
    "    'branch': [1, 1],     # 본분교\n",
    "    'found': [1, 1],      # 국공사립\n",
    "    'course': [1, 1],     # 학위과정\n",
    "    'daynight': [0, 0],   # 주야간\n",
    "    'major': [7, 7],      # 전공계열\n",
    "    'school': [1, 1],     # 학교유형\n",
    "    'school_area': [0, 0], #학교소재지역\n",
    "    'sex': [0, 0],        # 성별\n",
    "    'age': [23, 23],      # 연령\n",
    "    'f009': [4, 4],       # 고등학교 계열\n",
    "    'i001': [1, 1],       # 어학연수 경험\n",
    "    'i033': [650, 650],   # 토익 점수\n",
    "    'i037': [0, 0],   # 토익 스피킹 점수\n",
    "    'i042': [0, 0],   # 토플 점수\n",
    "    'i046': [0, 0],   # 텝스 점수\n",
    "    'i066': [0, 0],     # 일본어 JLPT 점수\n",
    "    'i076': [0, 0],   # 중국어 HSK 점수\n",
    "    'l001': [1, 1],       # 직업 훈련 여부\n",
    "    'l009': [0, 0],       # 직업 훈련 종료 여부\n",
    "    'l016': [20, 20],   # 직업 훈련 총 시간\n",
    "    'm002': [1, 1],       # 자격증 개수\n",
    "    'k110': [0, 0],       # NCS 준비 여부\n",
    "    'k004': [0, 0],       # 직무적성검사 여부\n",
    "    'k007': [0, 0],       # 공모전 수상\n",
    "    'k009': [0, 0],       # 대외 활동 여부\n",
    "    'k011': [0, 0],       # 이력서 작성 및 면접훈련 교육 여부\n",
    "    'q001': [1, 1],       # 현재 건강 상태\n",
    "    'q002': [1, 1],       # 일주일 평균 운동 시간\n",
    "    'q003': [10, 10],       # 하루 평균 수면시간\n",
    "    'q004': [1, 1],       # 흡연 여부\n",
    "    'q006': [0, 0],       # 음주 빈도\n",
    "    'p001': [1, 1],       # 혼인 여부\n",
    "    'p026': [1, 1],       # 아버님의 최종 학력\n",
    "    'p029': [1, 1],       # 어머님의 최종 학력\n",
    "    'p036': [1, 1],       # 부모님의 자산 규모\n",
    "    'p045': [0, 0]        # 군 복무 경혐\n",
    "})\n",
    "rf_clf = RandomForestClassifier(max_depth=34, n_estimators=550, random_state=0)\n",
    "rf_clf.fit(X_res, y_res)\n",
    "\n",
    "test_pred = rf_clf.predict(high_prob_data)\n",
    "\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "high_prob_data = pd.DataFrame({\n",
    "    'branch': [1, 1],     # 본분교\n",
    "    'found': [1, 1],      # 국공사립\n",
    "    'course': [1, 1],     # 학위과정\n",
    "    'daynight': [0, 0],   # 주야간\n",
    "    'major': [7, 7],      # 전공계열\n",
    "    'school': [1, 1],     # 학교유형\n",
    "    'school_area': [0, 0], #학교소재지역\n",
    "    'sex': [0, 0],        # 성별\n",
    "    'age': [23, 23],      # 연령\n",
    "    'f009': [4, 4],       # 고등학교 계열\n",
    "    'i001': [1, 1],       # 어학연수 경험\n",
    "    'i033': [800, 700],   # 토익 점수\n",
    "    'i037': [0, 0],   # 토익 스피킹 점수\n",
    "    'i042': [0, 0],   # 토플 점수\n",
    "    'i046': [0, 0],   # 텝스 점수\n",
    "    'i066': [0, 0],     # 일본어 JLPT 점수\n",
    "    'i076': [0, 0],   # 중국어 HSK 점수\n",
    "    'l001': [1, 1],       # 직업 훈련 여부\n",
    "    'l009': [0, 0],       # 직업 훈련 종료 여부\n",
    "    'l016': [20, 20],   # 직업 훈련 총 시간\n",
    "    'm002': [1, 3],       # 자격증 개수\n",
    "    'k110': [0, 0],       # NCS 준비 여부\n",
    "    'k004': [0, 0],       # 직무적성검사 여부\n",
    "    'k007': [0, 1],       # 공모전 수상\n",
    "    'k009': [0, 0],       # 대외 활동 여부\n",
    "    'k011': [0, 1],       # 이력서 작성 및 면접훈련 교육 여부\n",
    "    'q001': [1, 1],       # 현재 건강 상태\n",
    "    'q002': [1, 1],       # 일주일 평균 운동 시간\n",
    "    'q003': [10, 10],       # 하루 평균 수면시간\n",
    "    'q004': [1, 1],       # 흡연 여부\n",
    "    'q006': [0, 0],       # 음주 빈도\n",
    "    'p001': [1, 1],       # 혼인 여부\n",
    "    'p026': [1, 1],       # 아버님의 최종 학력\n",
    "    'p029': [1, 1],       # 어머님의 최종 학력\n",
    "    'p036': [1, 1],       # 부모님의 자산 규모\n",
    "    'p045': [0, 0]        # 군 복무 경혐\n",
    "})\n",
    "rf_clf = RandomForestClassifier(max_depth=34, n_estimators=550, random_state=0)\n",
    "rf_clf.fit(X_res, y_res)\n",
    "\n",
    "test_pred = rf_clf.predict(high_prob_data)\n",
    "\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------\n",
    "추가적인 모델 사용\n",
    "\n",
    "- 범주형 변수를 라벨인코딩을 하면 로지스틱과 svm과 같은 수리적인 모델에서는 사용하면 안되지만 그냥 한번 해봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7064231581562928 0.689340290719344\n"
     ]
    }
   ],
   "source": [
    "# 로지스틱\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_new, y_new, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "lr_clf = LogisticRegression()\n",
    "lr_clf.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(lr_clf.score(X_train_scaled, y_train), lr_clf.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5663436451733135, 0.5566530003727171)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SVM\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SVC(kernel=\"rbf\")   # kernel 기본값 \"rbf\"\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.score(X_train, y_train), model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7083488632128214\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1873,  779],\n",
       "       [ 786, 1928]], dtype=int64)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MLP\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(100,100,100,100,100),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    max_iter=5000,\n",
    "    random_state=42\n",
    ")\n",
    "mlp.fit(X_train, y_train)\n",
    "\n",
    "pred = mlp.predict(X_test)\n",
    "print(accuracy_score(y_test, pred))\n",
    "confusion_matrix(y_test, pred, labels=[0, 1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
