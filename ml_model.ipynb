{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from pandas) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from matplotlib) (2.2.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from seaborn) (2.2.1)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from seaborn) (2.2.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from seaborn) (3.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from pandas>=1.2->seaborn) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: imbalanced-learn in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy<3,>=1.24.3 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from imbalanced-learn) (2.2.1)\n",
      "Requirement already satisfied: scipy<2,>=1.10.1 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from imbalanced-learn) (1.15.1)\n",
      "Requirement already satisfied: scikit-learn<2,>=1.3.2 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from imbalanced-learn) (1.6.1)\n",
      "Requirement already satisfied: sklearn-compat<1,>=0.1 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from imbalanced-learn) (0.1.3)\n",
      "Requirement already satisfied: joblib<2,>=1.1.1 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from imbalanced-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from imbalanced-learn) (3.5.0)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (4.5.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from lightgbm) (2.2.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\playdata\\appdata\\local\\anaconda3\\envs\\pystudy_env\\lib\\site-packages (from lightgbm) (1.15.1)\n"
     ]
    }
   ],
   "source": [
    "# !pip install sklearn\n",
    "!pip install imbalanced-learn\n",
    "!pip install lightgbm\n",
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>branch</th>\n",
       "      <th>found</th>\n",
       "      <th>course</th>\n",
       "      <th>daynight</th>\n",
       "      <th>major</th>\n",
       "      <th>school</th>\n",
       "      <th>school_area</th>\n",
       "      <th>sex</th>\n",
       "      <th>...</th>\n",
       "      <th>q001</th>\n",
       "      <th>q002</th>\n",
       "      <th>q003</th>\n",
       "      <th>q004</th>\n",
       "      <th>q006</th>\n",
       "      <th>p001</th>\n",
       "      <th>p026</th>\n",
       "      <th>p029</th>\n",
       "      <th>p036</th>\n",
       "      <th>p045</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>21</td>\n",
       "      <td>167.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27</td>\n",
       "      <td>187.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>34</td>\n",
       "      <td>237.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35</td>\n",
       "      <td>260.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0     id  branch  found  course  daynight  major  school  \\\n",
       "0           0    5.0     1.0    3.0     3.0         0    1.0     1.0   \n",
       "1          21  167.0     1.0    3.0     5.0         1    1.0     1.0   \n",
       "2          27  187.0     1.0    3.0     3.0         0    1.0     1.0   \n",
       "3          34  237.0     1.0    3.0     3.0         1    1.0     1.0   \n",
       "4          35  260.0     1.0    3.0     5.0         1    1.0     1.0   \n",
       "\n",
       "   school_area  sex  ...  q001  q002  q003  q004  q006  p001  p026  p029  \\\n",
       "0            1    1  ...   3.0   2.0   8.0     0   1.0   1.0   5.0   5.0   \n",
       "1            1    1  ...   2.0   3.0   5.0     1   3.0   1.0   5.0   5.0   \n",
       "2            0    1  ...   5.0  10.0   7.0     0   3.0   1.0   5.0   6.0   \n",
       "3            1    1  ...   4.0   2.0   7.0     0   3.0   1.0   3.0   4.0   \n",
       "4            0    1  ...   1.0   5.0   6.0     0   1.0   1.0   3.0   3.0   \n",
       "\n",
       "   p036  p045  \n",
       "0  6.00     0  \n",
       "1  2.00     0  \n",
       "2  3.01     0  \n",
       "3  3.00     0  \n",
       "4  2.00     0  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/data.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.7688227241615332, 0.7785476007185014)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(['hire_state'],axis=1), df['hire_state'], random_state=0)\n",
    "dt_clf = DecisionTreeClassifier(random_state=0, max_depth=3)   # max_depth : 가지치기 (최대 깊이 지정)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "dt_clf.score(X_train, y_train), dt_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b로 시작하는 변수 삭제\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "\n",
    "X = df.drop(['id','hire_state','b022', 'b023', 'b036', 'b038', 'b039', 'b040'],axis=1)\n",
    "y = df['hire_state']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### oversampleing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({1: 10732, 0: 4853})\n",
      "Counter({0: 10732, 1: 10732})\n",
      "Counter({0: 10732, 1: 10732})\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from collections import Counter   \n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "\n",
    "counter = Counter(y)\n",
    "print(counter)\n",
    "\n",
    "smt = SMOTE(random_state=42)\n",
    "X_new, y_new = smt.fit_resample(X, y)\n",
    "ros = RandomOverSampler(random_state=42)\n",
    "X_res, y_res = ros.fit_resample(X, y)\n",
    "\n",
    "counter = Counter(y_new)\n",
    "print(counter)\n",
    "counter = Counter(y_res)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6087712759348988 0.5972791651136787\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, random_state=0)\n",
    "\n",
    "dt_clf = DecisionTreeClassifier(random_state=0, max_depth=3)   # max_depth : 가지치기 (최대 깊이 지정)\n",
    "dt_clf.fit(X_train, y_train)\n",
    "\n",
    "print(dt_clf.score(X_train, y_train), dt_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1348, 1367],\n",
       "       [ 794, 1857]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_pred = dt_clf.predict(X_test)\n",
    "confusion_matrix(y_test, y_pred, labels=[0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그리드서치"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=26, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "dt_clf = DecisionTreeClassifier(random_state=0)\n",
    "params = {\n",
    "    \"max_depth\": [12,13,14,15,16,17,18,19,20,21,22,23,24,25,26]\n",
    "}\n",
    "gscv_tree = GridSearchCV (dt_clf, params, scoring ='accuracy', cv = skf)\n",
    "gscv_tree.fit(X_train, y_train)\n",
    "print(gscv_tree.best_estimator_)\n",
    "# pd.DataFrame(gscv_tree.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 0.8591129332836377\n"
     ]
    }
   ],
   "source": [
    "# 랜덤 포레스트\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf_clf = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    random_state=42\n",
    ")\n",
    "rf_clf.fit(X_train, y_train)\n",
    "print(\"Random Forest Accuracy:\", rf_clf.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "rf_clf = RandomForestClassifier(random_state=0)\n",
    "# params = {\n",
    "#     \"max_depth\": [5,10,15,20,25,30,35],\n",
    "#     \"n_estimators\": [100,200,300,400,500,800,1000]\n",
    "# }\n",
    "params = {\n",
    "    \"max_depth\": [26,27,28,29,30,31,32,33,34],\n",
    "    \"n_estimators\": [450,500,550,600,650,700]\n",
    "}\n",
    "gscv_rf = GridSearchCV (rf_clf, params, scoring ='accuracy', cv = skf)\n",
    "gscv_rf.fit(X_train, y_train)\n",
    "print(gscv_rf.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.411443</td>\n",
       "      <td>0.010121</td>\n",
       "      <td>0.020357</td>\n",
       "      <td>0.000597</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 100}</td>\n",
       "      <td>0.648758</td>\n",
       "      <td>0.646894</td>\n",
       "      <td>0.643789</td>\n",
       "      <td>0.641504</td>\n",
       "      <td>0.631873</td>\n",
       "      <td>0.642564</td>\n",
       "      <td>0.005900</td>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.792946</td>\n",
       "      <td>0.021246</td>\n",
       "      <td>0.036564</td>\n",
       "      <td>0.002976</td>\n",
       "      <td>5</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 200}</td>\n",
       "      <td>0.648758</td>\n",
       "      <td>0.644410</td>\n",
       "      <td>0.648447</td>\n",
       "      <td>0.643057</td>\n",
       "      <td>0.634048</td>\n",
       "      <td>0.643744</td>\n",
       "      <td>0.005333</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.167702</td>\n",
       "      <td>0.023728</td>\n",
       "      <td>0.054850</td>\n",
       "      <td>0.001679</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 300}</td>\n",
       "      <td>0.653416</td>\n",
       "      <td>0.642547</td>\n",
       "      <td>0.648758</td>\n",
       "      <td>0.640572</td>\n",
       "      <td>0.634358</td>\n",
       "      <td>0.643930</td>\n",
       "      <td>0.006605</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.598454</td>\n",
       "      <td>0.036300</td>\n",
       "      <td>0.079807</td>\n",
       "      <td>0.010289</td>\n",
       "      <td>5</td>\n",
       "      <td>400</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 400}</td>\n",
       "      <td>0.651863</td>\n",
       "      <td>0.642547</td>\n",
       "      <td>0.646894</td>\n",
       "      <td>0.639640</td>\n",
       "      <td>0.634669</td>\n",
       "      <td>0.643123</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>46</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.003466</td>\n",
       "      <td>0.041828</td>\n",
       "      <td>0.092758</td>\n",
       "      <td>0.005912</td>\n",
       "      <td>5</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 500}</td>\n",
       "      <td>0.653106</td>\n",
       "      <td>0.642857</td>\n",
       "      <td>0.648447</td>\n",
       "      <td>0.639329</td>\n",
       "      <td>0.632495</td>\n",
       "      <td>0.643247</td>\n",
       "      <td>0.007146</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3.200616</td>\n",
       "      <td>0.073565</td>\n",
       "      <td>0.146300</td>\n",
       "      <td>0.002856</td>\n",
       "      <td>5</td>\n",
       "      <td>800</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 800}</td>\n",
       "      <td>0.650000</td>\n",
       "      <td>0.643478</td>\n",
       "      <td>0.644720</td>\n",
       "      <td>0.641504</td>\n",
       "      <td>0.632495</td>\n",
       "      <td>0.642439</td>\n",
       "      <td>0.005714</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.961413</td>\n",
       "      <td>0.052801</td>\n",
       "      <td>0.178615</td>\n",
       "      <td>0.002159</td>\n",
       "      <td>5</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 5, 'n_estimators': 1000}</td>\n",
       "      <td>0.648137</td>\n",
       "      <td>0.644099</td>\n",
       "      <td>0.645342</td>\n",
       "      <td>0.642746</td>\n",
       "      <td>0.631563</td>\n",
       "      <td>0.642377</td>\n",
       "      <td>0.005692</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.758636</td>\n",
       "      <td>0.036442</td>\n",
       "      <td>0.033649</td>\n",
       "      <td>0.002227</td>\n",
       "      <td>10</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 100}</td>\n",
       "      <td>0.708696</td>\n",
       "      <td>0.694410</td>\n",
       "      <td>0.707453</td>\n",
       "      <td>0.694315</td>\n",
       "      <td>0.685306</td>\n",
       "      <td>0.698036</td>\n",
       "      <td>0.008847</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.515033</td>\n",
       "      <td>0.138325</td>\n",
       "      <td>0.067491</td>\n",
       "      <td>0.003222</td>\n",
       "      <td>10</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 200}</td>\n",
       "      <td>0.704969</td>\n",
       "      <td>0.697516</td>\n",
       "      <td>0.711801</td>\n",
       "      <td>0.694936</td>\n",
       "      <td>0.694004</td>\n",
       "      <td>0.700645</td>\n",
       "      <td>0.006777</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2.150137</td>\n",
       "      <td>0.072125</td>\n",
       "      <td>0.096304</td>\n",
       "      <td>0.005918</td>\n",
       "      <td>10</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 300}</td>\n",
       "      <td>0.709006</td>\n",
       "      <td>0.699068</td>\n",
       "      <td>0.713043</td>\n",
       "      <td>0.697111</td>\n",
       "      <td>0.691519</td>\n",
       "      <td>0.701950</td>\n",
       "      <td>0.007917</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3.048869</td>\n",
       "      <td>0.263477</td>\n",
       "      <td>0.138487</td>\n",
       "      <td>0.006993</td>\n",
       "      <td>10</td>\n",
       "      <td>400</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 400}</td>\n",
       "      <td>0.711180</td>\n",
       "      <td>0.700932</td>\n",
       "      <td>0.708075</td>\n",
       "      <td>0.694004</td>\n",
       "      <td>0.693072</td>\n",
       "      <td>0.701453</td>\n",
       "      <td>0.007273</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3.696555</td>\n",
       "      <td>0.241600</td>\n",
       "      <td>0.162404</td>\n",
       "      <td>0.014600</td>\n",
       "      <td>10</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 500}</td>\n",
       "      <td>0.714907</td>\n",
       "      <td>0.696584</td>\n",
       "      <td>0.706522</td>\n",
       "      <td>0.696800</td>\n",
       "      <td>0.693383</td>\n",
       "      <td>0.701639</td>\n",
       "      <td>0.007962</td>\n",
       "      <td>39</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>5.440744</td>\n",
       "      <td>0.034421</td>\n",
       "      <td>0.248233</td>\n",
       "      <td>0.008386</td>\n",
       "      <td>10</td>\n",
       "      <td>800</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 800}</td>\n",
       "      <td>0.711801</td>\n",
       "      <td>0.696273</td>\n",
       "      <td>0.710248</td>\n",
       "      <td>0.697732</td>\n",
       "      <td>0.694004</td>\n",
       "      <td>0.702012</td>\n",
       "      <td>0.007470</td>\n",
       "      <td>37</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>6.673083</td>\n",
       "      <td>0.140280</td>\n",
       "      <td>0.303600</td>\n",
       "      <td>0.004587</td>\n",
       "      <td>10</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 10, 'n_estimators': 1000}</td>\n",
       "      <td>0.711180</td>\n",
       "      <td>0.696894</td>\n",
       "      <td>0.710559</td>\n",
       "      <td>0.700528</td>\n",
       "      <td>0.693072</td>\n",
       "      <td>0.702447</td>\n",
       "      <td>0.007273</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.938587</td>\n",
       "      <td>0.024200</td>\n",
       "      <td>0.046389</td>\n",
       "      <td>0.001654</td>\n",
       "      <td>15</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 15, 'n_estimators': 100}</td>\n",
       "      <td>0.790373</td>\n",
       "      <td>0.772671</td>\n",
       "      <td>0.774224</td>\n",
       "      <td>0.775707</td>\n",
       "      <td>0.782852</td>\n",
       "      <td>0.779165</td>\n",
       "      <td>0.006599</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1.828865</td>\n",
       "      <td>0.008322</td>\n",
       "      <td>0.087851</td>\n",
       "      <td>0.002631</td>\n",
       "      <td>15</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 15, 'n_estimators': 200}</td>\n",
       "      <td>0.787578</td>\n",
       "      <td>0.773292</td>\n",
       "      <td>0.776708</td>\n",
       "      <td>0.773532</td>\n",
       "      <td>0.778813</td>\n",
       "      <td>0.777985</td>\n",
       "      <td>0.005219</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2.812448</td>\n",
       "      <td>0.026993</td>\n",
       "      <td>0.131371</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>15</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 15, 'n_estimators': 300}</td>\n",
       "      <td>0.788199</td>\n",
       "      <td>0.774534</td>\n",
       "      <td>0.782609</td>\n",
       "      <td>0.776639</td>\n",
       "      <td>0.780056</td>\n",
       "      <td>0.780407</td>\n",
       "      <td>0.004783</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>3.759496</td>\n",
       "      <td>0.039398</td>\n",
       "      <td>0.183108</td>\n",
       "      <td>0.004382</td>\n",
       "      <td>15</td>\n",
       "      <td>400</td>\n",
       "      <td>{'max_depth': 15, 'n_estimators': 400}</td>\n",
       "      <td>0.788820</td>\n",
       "      <td>0.775466</td>\n",
       "      <td>0.780435</td>\n",
       "      <td>0.776639</td>\n",
       "      <td>0.778503</td>\n",
       "      <td>0.779972</td>\n",
       "      <td>0.004734</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>4.627345</td>\n",
       "      <td>0.063624</td>\n",
       "      <td>0.222309</td>\n",
       "      <td>0.009132</td>\n",
       "      <td>15</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 15, 'n_estimators': 500}</td>\n",
       "      <td>0.790994</td>\n",
       "      <td>0.776398</td>\n",
       "      <td>0.783230</td>\n",
       "      <td>0.775085</td>\n",
       "      <td>0.780056</td>\n",
       "      <td>0.781152</td>\n",
       "      <td>0.005688</td>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>7.536066</td>\n",
       "      <td>0.095466</td>\n",
       "      <td>0.370397</td>\n",
       "      <td>0.006364</td>\n",
       "      <td>15</td>\n",
       "      <td>800</td>\n",
       "      <td>{'max_depth': 15, 'n_estimators': 800}</td>\n",
       "      <td>0.791304</td>\n",
       "      <td>0.777640</td>\n",
       "      <td>0.783851</td>\n",
       "      <td>0.777260</td>\n",
       "      <td>0.778192</td>\n",
       "      <td>0.781649</td>\n",
       "      <td>0.005392</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>9.456755</td>\n",
       "      <td>0.102391</td>\n",
       "      <td>0.471243</td>\n",
       "      <td>0.008593</td>\n",
       "      <td>15</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 15, 'n_estimators': 1000}</td>\n",
       "      <td>0.790062</td>\n",
       "      <td>0.776398</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.776328</td>\n",
       "      <td>0.775707</td>\n",
       "      <td>0.780842</td>\n",
       "      <td>0.005920</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1.067395</td>\n",
       "      <td>0.011992</td>\n",
       "      <td>0.052982</td>\n",
       "      <td>0.002482</td>\n",
       "      <td>20</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 100}</td>\n",
       "      <td>0.815528</td>\n",
       "      <td>0.798758</td>\n",
       "      <td>0.807764</td>\n",
       "      <td>0.801802</td>\n",
       "      <td>0.795899</td>\n",
       "      <td>0.803950</td>\n",
       "      <td>0.007000</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2.114477</td>\n",
       "      <td>0.010998</td>\n",
       "      <td>0.100454</td>\n",
       "      <td>0.000832</td>\n",
       "      <td>20</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 200}</td>\n",
       "      <td>0.819565</td>\n",
       "      <td>0.801553</td>\n",
       "      <td>0.808385</td>\n",
       "      <td>0.811432</td>\n",
       "      <td>0.801180</td>\n",
       "      <td>0.808423</td>\n",
       "      <td>0.006824</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3.188295</td>\n",
       "      <td>0.027905</td>\n",
       "      <td>0.153440</td>\n",
       "      <td>0.002004</td>\n",
       "      <td>20</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 300}</td>\n",
       "      <td>0.821118</td>\n",
       "      <td>0.802484</td>\n",
       "      <td>0.810870</td>\n",
       "      <td>0.817024</td>\n",
       "      <td>0.803976</td>\n",
       "      <td>0.811094</td>\n",
       "      <td>0.007218</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.280581</td>\n",
       "      <td>0.057216</td>\n",
       "      <td>0.202270</td>\n",
       "      <td>0.007443</td>\n",
       "      <td>20</td>\n",
       "      <td>400</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 400}</td>\n",
       "      <td>0.822360</td>\n",
       "      <td>0.806211</td>\n",
       "      <td>0.815528</td>\n",
       "      <td>0.815781</td>\n",
       "      <td>0.803044</td>\n",
       "      <td>0.812585</td>\n",
       "      <td>0.007015</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.394105</td>\n",
       "      <td>0.112039</td>\n",
       "      <td>0.251276</td>\n",
       "      <td>0.001950</td>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 500}</td>\n",
       "      <td>0.823292</td>\n",
       "      <td>0.809317</td>\n",
       "      <td>0.814596</td>\n",
       "      <td>0.814849</td>\n",
       "      <td>0.801180</td>\n",
       "      <td>0.812647</td>\n",
       "      <td>0.007274</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8.850856</td>\n",
       "      <td>0.054284</td>\n",
       "      <td>0.410372</td>\n",
       "      <td>0.008391</td>\n",
       "      <td>20</td>\n",
       "      <td>800</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 800}</td>\n",
       "      <td>0.822050</td>\n",
       "      <td>0.809938</td>\n",
       "      <td>0.818012</td>\n",
       "      <td>0.812364</td>\n",
       "      <td>0.802423</td>\n",
       "      <td>0.812957</td>\n",
       "      <td>0.006763</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>11.015884</td>\n",
       "      <td>0.124539</td>\n",
       "      <td>0.516344</td>\n",
       "      <td>0.014216</td>\n",
       "      <td>20</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 20, 'n_estimators': 1000}</td>\n",
       "      <td>0.821739</td>\n",
       "      <td>0.808696</td>\n",
       "      <td>0.818323</td>\n",
       "      <td>0.811743</td>\n",
       "      <td>0.801802</td>\n",
       "      <td>0.812460</td>\n",
       "      <td>0.007053</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.100297</td>\n",
       "      <td>0.014386</td>\n",
       "      <td>0.056506</td>\n",
       "      <td>0.002138</td>\n",
       "      <td>25</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 25, 'n_estimators': 100}</td>\n",
       "      <td>0.816149</td>\n",
       "      <td>0.811491</td>\n",
       "      <td>0.825466</td>\n",
       "      <td>0.820441</td>\n",
       "      <td>0.810811</td>\n",
       "      <td>0.816872</td>\n",
       "      <td>0.005528</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>2.240584</td>\n",
       "      <td>0.053113</td>\n",
       "      <td>0.108751</td>\n",
       "      <td>0.003234</td>\n",
       "      <td>25</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 25, 'n_estimators': 200}</td>\n",
       "      <td>0.820186</td>\n",
       "      <td>0.813043</td>\n",
       "      <td>0.822050</td>\n",
       "      <td>0.825412</td>\n",
       "      <td>0.811432</td>\n",
       "      <td>0.818425</td>\n",
       "      <td>0.005346</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>3.374285</td>\n",
       "      <td>0.019244</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.002406</td>\n",
       "      <td>25</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 25, 'n_estimators': 300}</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.811801</td>\n",
       "      <td>0.824534</td>\n",
       "      <td>0.823237</td>\n",
       "      <td>0.811121</td>\n",
       "      <td>0.819356</td>\n",
       "      <td>0.006513</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>4.516173</td>\n",
       "      <td>0.051738</td>\n",
       "      <td>0.220194</td>\n",
       "      <td>0.008681</td>\n",
       "      <td>25</td>\n",
       "      <td>400</td>\n",
       "      <td>{'max_depth': 25, 'n_estimators': 400}</td>\n",
       "      <td>0.826087</td>\n",
       "      <td>0.813665</td>\n",
       "      <td>0.822050</td>\n",
       "      <td>0.821994</td>\n",
       "      <td>0.805530</td>\n",
       "      <td>0.817865</td>\n",
       "      <td>0.007375</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>5.747314</td>\n",
       "      <td>0.071435</td>\n",
       "      <td>0.275002</td>\n",
       "      <td>0.002815</td>\n",
       "      <td>25</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 25, 'n_estimators': 500}</td>\n",
       "      <td>0.824845</td>\n",
       "      <td>0.811180</td>\n",
       "      <td>0.822360</td>\n",
       "      <td>0.817335</td>\n",
       "      <td>0.808015</td>\n",
       "      <td>0.816747</td>\n",
       "      <td>0.006398</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>9.305920</td>\n",
       "      <td>0.066621</td>\n",
       "      <td>0.436677</td>\n",
       "      <td>0.004358</td>\n",
       "      <td>25</td>\n",
       "      <td>800</td>\n",
       "      <td>{'max_depth': 25, 'n_estimators': 800}</td>\n",
       "      <td>0.825466</td>\n",
       "      <td>0.812422</td>\n",
       "      <td>0.822981</td>\n",
       "      <td>0.817956</td>\n",
       "      <td>0.804908</td>\n",
       "      <td>0.816747</td>\n",
       "      <td>0.007419</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>11.487637</td>\n",
       "      <td>0.154556</td>\n",
       "      <td>0.544431</td>\n",
       "      <td>0.012129</td>\n",
       "      <td>25</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 25, 'n_estimators': 1000}</td>\n",
       "      <td>0.826398</td>\n",
       "      <td>0.813975</td>\n",
       "      <td>0.822360</td>\n",
       "      <td>0.818888</td>\n",
       "      <td>0.806462</td>\n",
       "      <td>0.817616</td>\n",
       "      <td>0.006913</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1.132784</td>\n",
       "      <td>0.018518</td>\n",
       "      <td>0.056671</td>\n",
       "      <td>0.000907</td>\n",
       "      <td>30</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 100}</td>\n",
       "      <td>0.814596</td>\n",
       "      <td>0.805280</td>\n",
       "      <td>0.818323</td>\n",
       "      <td>0.819199</td>\n",
       "      <td>0.810500</td>\n",
       "      <td>0.813579</td>\n",
       "      <td>0.005167</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>2.288341</td>\n",
       "      <td>0.024397</td>\n",
       "      <td>0.111456</td>\n",
       "      <td>0.001591</td>\n",
       "      <td>30</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 200}</td>\n",
       "      <td>0.818944</td>\n",
       "      <td>0.816149</td>\n",
       "      <td>0.820497</td>\n",
       "      <td>0.822305</td>\n",
       "      <td>0.813607</td>\n",
       "      <td>0.818300</td>\n",
       "      <td>0.003096</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>3.441381</td>\n",
       "      <td>0.073761</td>\n",
       "      <td>0.164334</td>\n",
       "      <td>0.003188</td>\n",
       "      <td>30</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 300}</td>\n",
       "      <td>0.827329</td>\n",
       "      <td>0.816149</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.822616</td>\n",
       "      <td>0.810500</td>\n",
       "      <td>0.819605</td>\n",
       "      <td>0.005777</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>4.560962</td>\n",
       "      <td>0.068064</td>\n",
       "      <td>0.220080</td>\n",
       "      <td>0.002651</td>\n",
       "      <td>30</td>\n",
       "      <td>400</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 400}</td>\n",
       "      <td>0.826398</td>\n",
       "      <td>0.816770</td>\n",
       "      <td>0.822360</td>\n",
       "      <td>0.820441</td>\n",
       "      <td>0.812364</td>\n",
       "      <td>0.819667</td>\n",
       "      <td>0.004793</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>5.629390</td>\n",
       "      <td>0.022191</td>\n",
       "      <td>0.270758</td>\n",
       "      <td>0.005593</td>\n",
       "      <td>30</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 500}</td>\n",
       "      <td>0.827640</td>\n",
       "      <td>0.818012</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.823548</td>\n",
       "      <td>0.811432</td>\n",
       "      <td>0.820412</td>\n",
       "      <td>0.005468</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>9.229974</td>\n",
       "      <td>0.144019</td>\n",
       "      <td>0.458021</td>\n",
       "      <td>0.010275</td>\n",
       "      <td>30</td>\n",
       "      <td>800</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 800}</td>\n",
       "      <td>0.830124</td>\n",
       "      <td>0.813354</td>\n",
       "      <td>0.822981</td>\n",
       "      <td>0.824480</td>\n",
       "      <td>0.808947</td>\n",
       "      <td>0.819977</td>\n",
       "      <td>0.007717</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>11.806583</td>\n",
       "      <td>0.071645</td>\n",
       "      <td>0.574887</td>\n",
       "      <td>0.009825</td>\n",
       "      <td>30</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 30, 'n_estimators': 1000}</td>\n",
       "      <td>0.828571</td>\n",
       "      <td>0.815528</td>\n",
       "      <td>0.822981</td>\n",
       "      <td>0.823858</td>\n",
       "      <td>0.810500</td>\n",
       "      <td>0.820288</td>\n",
       "      <td>0.006435</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1.157506</td>\n",
       "      <td>0.024636</td>\n",
       "      <td>0.057062</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>35</td>\n",
       "      <td>100</td>\n",
       "      <td>{'max_depth': 35, 'n_estimators': 100}</td>\n",
       "      <td>0.822360</td>\n",
       "      <td>0.810559</td>\n",
       "      <td>0.813975</td>\n",
       "      <td>0.817335</td>\n",
       "      <td>0.801491</td>\n",
       "      <td>0.813144</td>\n",
       "      <td>0.007009</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2.266299</td>\n",
       "      <td>0.015416</td>\n",
       "      <td>0.109291</td>\n",
       "      <td>0.002863</td>\n",
       "      <td>35</td>\n",
       "      <td>200</td>\n",
       "      <td>{'max_depth': 35, 'n_estimators': 200}</td>\n",
       "      <td>0.821739</td>\n",
       "      <td>0.812422</td>\n",
       "      <td>0.822360</td>\n",
       "      <td>0.821994</td>\n",
       "      <td>0.807704</td>\n",
       "      <td>0.817244</td>\n",
       "      <td>0.006053</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>3.378601</td>\n",
       "      <td>0.023720</td>\n",
       "      <td>0.163736</td>\n",
       "      <td>0.003706</td>\n",
       "      <td>35</td>\n",
       "      <td>300</td>\n",
       "      <td>{'max_depth': 35, 'n_estimators': 300}</td>\n",
       "      <td>0.826398</td>\n",
       "      <td>0.812112</td>\n",
       "      <td>0.821118</td>\n",
       "      <td>0.824790</td>\n",
       "      <td>0.812675</td>\n",
       "      <td>0.819418</td>\n",
       "      <td>0.005989</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>4.473022</td>\n",
       "      <td>0.012409</td>\n",
       "      <td>0.220085</td>\n",
       "      <td>0.009588</td>\n",
       "      <td>35</td>\n",
       "      <td>400</td>\n",
       "      <td>{'max_depth': 35, 'n_estimators': 400}</td>\n",
       "      <td>0.827329</td>\n",
       "      <td>0.813665</td>\n",
       "      <td>0.821429</td>\n",
       "      <td>0.821994</td>\n",
       "      <td>0.814228</td>\n",
       "      <td>0.819729</td>\n",
       "      <td>0.005154</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>5.645208</td>\n",
       "      <td>0.046911</td>\n",
       "      <td>0.279050</td>\n",
       "      <td>0.002752</td>\n",
       "      <td>35</td>\n",
       "      <td>500</td>\n",
       "      <td>{'max_depth': 35, 'n_estimators': 500}</td>\n",
       "      <td>0.827640</td>\n",
       "      <td>0.814286</td>\n",
       "      <td>0.825155</td>\n",
       "      <td>0.821994</td>\n",
       "      <td>0.810500</td>\n",
       "      <td>0.819915</td>\n",
       "      <td>0.006508</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>9.163636</td>\n",
       "      <td>0.080356</td>\n",
       "      <td>0.444956</td>\n",
       "      <td>0.004991</td>\n",
       "      <td>35</td>\n",
       "      <td>800</td>\n",
       "      <td>{'max_depth': 35, 'n_estimators': 800}</td>\n",
       "      <td>0.828882</td>\n",
       "      <td>0.816149</td>\n",
       "      <td>0.823292</td>\n",
       "      <td>0.824169</td>\n",
       "      <td>0.805840</td>\n",
       "      <td>0.819666</td>\n",
       "      <td>0.008023</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>11.609316</td>\n",
       "      <td>0.147602</td>\n",
       "      <td>0.575605</td>\n",
       "      <td>0.009593</td>\n",
       "      <td>35</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'max_depth': 35, 'n_estimators': 1000}</td>\n",
       "      <td>0.827329</td>\n",
       "      <td>0.817081</td>\n",
       "      <td>0.823602</td>\n",
       "      <td>0.823858</td>\n",
       "      <td>0.806772</td>\n",
       "      <td>0.819729</td>\n",
       "      <td>0.007276</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.411443      0.010121         0.020357        0.000597   \n",
       "1        0.792946      0.021246         0.036564        0.002976   \n",
       "2        1.167702      0.023728         0.054850        0.001679   \n",
       "3        1.598454      0.036300         0.079807        0.010289   \n",
       "4        2.003466      0.041828         0.092758        0.005912   \n",
       "5        3.200616      0.073565         0.146300        0.002856   \n",
       "6        3.961413      0.052801         0.178615        0.002159   \n",
       "7        0.758636      0.036442         0.033649        0.002227   \n",
       "8        1.515033      0.138325         0.067491        0.003222   \n",
       "9        2.150137      0.072125         0.096304        0.005918   \n",
       "10       3.048869      0.263477         0.138487        0.006993   \n",
       "11       3.696555      0.241600         0.162404        0.014600   \n",
       "12       5.440744      0.034421         0.248233        0.008386   \n",
       "13       6.673083      0.140280         0.303600        0.004587   \n",
       "14       0.938587      0.024200         0.046389        0.001654   \n",
       "15       1.828865      0.008322         0.087851        0.002631   \n",
       "16       2.812448      0.026993         0.131371        0.001208   \n",
       "17       3.759496      0.039398         0.183108        0.004382   \n",
       "18       4.627345      0.063624         0.222309        0.009132   \n",
       "19       7.536066      0.095466         0.370397        0.006364   \n",
       "20       9.456755      0.102391         0.471243        0.008593   \n",
       "21       1.067395      0.011992         0.052982        0.002482   \n",
       "22       2.114477      0.010998         0.100454        0.000832   \n",
       "23       3.188295      0.027905         0.153440        0.002004   \n",
       "24       4.280581      0.057216         0.202270        0.007443   \n",
       "25       5.394105      0.112039         0.251276        0.001950   \n",
       "26       8.850856      0.054284         0.410372        0.008391   \n",
       "27      11.015884      0.124539         0.516344        0.014216   \n",
       "28       1.100297      0.014386         0.056506        0.002138   \n",
       "29       2.240584      0.053113         0.108751        0.003234   \n",
       "30       3.374285      0.019244         0.163400        0.002406   \n",
       "31       4.516173      0.051738         0.220194        0.008681   \n",
       "32       5.747314      0.071435         0.275002        0.002815   \n",
       "33       9.305920      0.066621         0.436677        0.004358   \n",
       "34      11.487637      0.154556         0.544431        0.012129   \n",
       "35       1.132784      0.018518         0.056671        0.000907   \n",
       "36       2.288341      0.024397         0.111456        0.001591   \n",
       "37       3.441381      0.073761         0.164334        0.003188   \n",
       "38       4.560962      0.068064         0.220080        0.002651   \n",
       "39       5.629390      0.022191         0.270758        0.005593   \n",
       "40       9.229974      0.144019         0.458021        0.010275   \n",
       "41      11.806583      0.071645         0.574887        0.009825   \n",
       "42       1.157506      0.024636         0.057062        0.001632   \n",
       "43       2.266299      0.015416         0.109291        0.002863   \n",
       "44       3.378601      0.023720         0.163736        0.003706   \n",
       "45       4.473022      0.012409         0.220085        0.009588   \n",
       "46       5.645208      0.046911         0.279050        0.002752   \n",
       "47       9.163636      0.080356         0.444956        0.004991   \n",
       "48      11.609316      0.147602         0.575605        0.009593   \n",
       "\n",
       "   param_max_depth param_n_estimators  \\\n",
       "0                5                100   \n",
       "1                5                200   \n",
       "2                5                300   \n",
       "3                5                400   \n",
       "4                5                500   \n",
       "5                5                800   \n",
       "6                5               1000   \n",
       "7               10                100   \n",
       "8               10                200   \n",
       "9               10                300   \n",
       "10              10                400   \n",
       "11              10                500   \n",
       "12              10                800   \n",
       "13              10               1000   \n",
       "14              15                100   \n",
       "15              15                200   \n",
       "16              15                300   \n",
       "17              15                400   \n",
       "18              15                500   \n",
       "19              15                800   \n",
       "20              15               1000   \n",
       "21              20                100   \n",
       "22              20                200   \n",
       "23              20                300   \n",
       "24              20                400   \n",
       "25              20                500   \n",
       "26              20                800   \n",
       "27              20               1000   \n",
       "28              25                100   \n",
       "29              25                200   \n",
       "30              25                300   \n",
       "31              25                400   \n",
       "32              25                500   \n",
       "33              25                800   \n",
       "34              25               1000   \n",
       "35              30                100   \n",
       "36              30                200   \n",
       "37              30                300   \n",
       "38              30                400   \n",
       "39              30                500   \n",
       "40              30                800   \n",
       "41              30               1000   \n",
       "42              35                100   \n",
       "43              35                200   \n",
       "44              35                300   \n",
       "45              35                400   \n",
       "46              35                500   \n",
       "47              35                800   \n",
       "48              35               1000   \n",
       "\n",
       "                                     params  split0_test_score  \\\n",
       "0     {'max_depth': 5, 'n_estimators': 100}           0.648758   \n",
       "1     {'max_depth': 5, 'n_estimators': 200}           0.648758   \n",
       "2     {'max_depth': 5, 'n_estimators': 300}           0.653416   \n",
       "3     {'max_depth': 5, 'n_estimators': 400}           0.651863   \n",
       "4     {'max_depth': 5, 'n_estimators': 500}           0.653106   \n",
       "5     {'max_depth': 5, 'n_estimators': 800}           0.650000   \n",
       "6    {'max_depth': 5, 'n_estimators': 1000}           0.648137   \n",
       "7    {'max_depth': 10, 'n_estimators': 100}           0.708696   \n",
       "8    {'max_depth': 10, 'n_estimators': 200}           0.704969   \n",
       "9    {'max_depth': 10, 'n_estimators': 300}           0.709006   \n",
       "10   {'max_depth': 10, 'n_estimators': 400}           0.711180   \n",
       "11   {'max_depth': 10, 'n_estimators': 500}           0.714907   \n",
       "12   {'max_depth': 10, 'n_estimators': 800}           0.711801   \n",
       "13  {'max_depth': 10, 'n_estimators': 1000}           0.711180   \n",
       "14   {'max_depth': 15, 'n_estimators': 100}           0.790373   \n",
       "15   {'max_depth': 15, 'n_estimators': 200}           0.787578   \n",
       "16   {'max_depth': 15, 'n_estimators': 300}           0.788199   \n",
       "17   {'max_depth': 15, 'n_estimators': 400}           0.788820   \n",
       "18   {'max_depth': 15, 'n_estimators': 500}           0.790994   \n",
       "19   {'max_depth': 15, 'n_estimators': 800}           0.791304   \n",
       "20  {'max_depth': 15, 'n_estimators': 1000}           0.790062   \n",
       "21   {'max_depth': 20, 'n_estimators': 100}           0.815528   \n",
       "22   {'max_depth': 20, 'n_estimators': 200}           0.819565   \n",
       "23   {'max_depth': 20, 'n_estimators': 300}           0.821118   \n",
       "24   {'max_depth': 20, 'n_estimators': 400}           0.822360   \n",
       "25   {'max_depth': 20, 'n_estimators': 500}           0.823292   \n",
       "26   {'max_depth': 20, 'n_estimators': 800}           0.822050   \n",
       "27  {'max_depth': 20, 'n_estimators': 1000}           0.821739   \n",
       "28   {'max_depth': 25, 'n_estimators': 100}           0.816149   \n",
       "29   {'max_depth': 25, 'n_estimators': 200}           0.820186   \n",
       "30   {'max_depth': 25, 'n_estimators': 300}           0.826087   \n",
       "31   {'max_depth': 25, 'n_estimators': 400}           0.826087   \n",
       "32   {'max_depth': 25, 'n_estimators': 500}           0.824845   \n",
       "33   {'max_depth': 25, 'n_estimators': 800}           0.825466   \n",
       "34  {'max_depth': 25, 'n_estimators': 1000}           0.826398   \n",
       "35   {'max_depth': 30, 'n_estimators': 100}           0.814596   \n",
       "36   {'max_depth': 30, 'n_estimators': 200}           0.818944   \n",
       "37   {'max_depth': 30, 'n_estimators': 300}           0.827329   \n",
       "38   {'max_depth': 30, 'n_estimators': 400}           0.826398   \n",
       "39   {'max_depth': 30, 'n_estimators': 500}           0.827640   \n",
       "40   {'max_depth': 30, 'n_estimators': 800}           0.830124   \n",
       "41  {'max_depth': 30, 'n_estimators': 1000}           0.828571   \n",
       "42   {'max_depth': 35, 'n_estimators': 100}           0.822360   \n",
       "43   {'max_depth': 35, 'n_estimators': 200}           0.821739   \n",
       "44   {'max_depth': 35, 'n_estimators': 300}           0.826398   \n",
       "45   {'max_depth': 35, 'n_estimators': 400}           0.827329   \n",
       "46   {'max_depth': 35, 'n_estimators': 500}           0.827640   \n",
       "47   {'max_depth': 35, 'n_estimators': 800}           0.828882   \n",
       "48  {'max_depth': 35, 'n_estimators': 1000}           0.827329   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.646894           0.643789           0.641504   \n",
       "1            0.644410           0.648447           0.643057   \n",
       "2            0.642547           0.648758           0.640572   \n",
       "3            0.642547           0.646894           0.639640   \n",
       "4            0.642857           0.648447           0.639329   \n",
       "5            0.643478           0.644720           0.641504   \n",
       "6            0.644099           0.645342           0.642746   \n",
       "7            0.694410           0.707453           0.694315   \n",
       "8            0.697516           0.711801           0.694936   \n",
       "9            0.699068           0.713043           0.697111   \n",
       "10           0.700932           0.708075           0.694004   \n",
       "11           0.696584           0.706522           0.696800   \n",
       "12           0.696273           0.710248           0.697732   \n",
       "13           0.696894           0.710559           0.700528   \n",
       "14           0.772671           0.774224           0.775707   \n",
       "15           0.773292           0.776708           0.773532   \n",
       "16           0.774534           0.782609           0.776639   \n",
       "17           0.775466           0.780435           0.776639   \n",
       "18           0.776398           0.783230           0.775085   \n",
       "19           0.777640           0.783851           0.777260   \n",
       "20           0.776398           0.785714           0.776328   \n",
       "21           0.798758           0.807764           0.801802   \n",
       "22           0.801553           0.808385           0.811432   \n",
       "23           0.802484           0.810870           0.817024   \n",
       "24           0.806211           0.815528           0.815781   \n",
       "25           0.809317           0.814596           0.814849   \n",
       "26           0.809938           0.818012           0.812364   \n",
       "27           0.808696           0.818323           0.811743   \n",
       "28           0.811491           0.825466           0.820441   \n",
       "29           0.813043           0.822050           0.825412   \n",
       "30           0.811801           0.824534           0.823237   \n",
       "31           0.813665           0.822050           0.821994   \n",
       "32           0.811180           0.822360           0.817335   \n",
       "33           0.812422           0.822981           0.817956   \n",
       "34           0.813975           0.822360           0.818888   \n",
       "35           0.805280           0.818323           0.819199   \n",
       "36           0.816149           0.820497           0.822305   \n",
       "37           0.816149           0.821429           0.822616   \n",
       "38           0.816770           0.822360           0.820441   \n",
       "39           0.818012           0.821429           0.823548   \n",
       "40           0.813354           0.822981           0.824480   \n",
       "41           0.815528           0.822981           0.823858   \n",
       "42           0.810559           0.813975           0.817335   \n",
       "43           0.812422           0.822360           0.821994   \n",
       "44           0.812112           0.821118           0.824790   \n",
       "45           0.813665           0.821429           0.821994   \n",
       "46           0.814286           0.825155           0.821994   \n",
       "47           0.816149           0.823292           0.824169   \n",
       "48           0.817081           0.823602           0.823858   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.631873         0.642564        0.005900               47  \n",
       "1            0.634048         0.643744        0.005333               44  \n",
       "2            0.634358         0.643930        0.006605               43  \n",
       "3            0.634669         0.643123        0.005908               46  \n",
       "4            0.632495         0.643247        0.007146               45  \n",
       "5            0.632495         0.642439        0.005714               48  \n",
       "6            0.631563         0.642377        0.005692               49  \n",
       "7            0.685306         0.698036        0.008847               42  \n",
       "8            0.694004         0.700645        0.006777               41  \n",
       "9            0.691519         0.701950        0.007917               38  \n",
       "10           0.693072         0.701453        0.007273               40  \n",
       "11           0.693383         0.701639        0.007962               39  \n",
       "12           0.694004         0.702012        0.007470               37  \n",
       "13           0.693072         0.702447        0.007273               36  \n",
       "14           0.782852         0.779165        0.006599               34  \n",
       "15           0.778813         0.777985        0.005219               35  \n",
       "16           0.780056         0.780407        0.004783               32  \n",
       "17           0.778503         0.779972        0.004734               33  \n",
       "18           0.780056         0.781152        0.005688               30  \n",
       "19           0.778192         0.781649        0.005392               29  \n",
       "20           0.775707         0.780842        0.005920               31  \n",
       "21           0.795899         0.803950        0.007000               28  \n",
       "22           0.801180         0.808423        0.006824               27  \n",
       "23           0.803976         0.811094        0.007218               26  \n",
       "24           0.803044         0.812585        0.007015               24  \n",
       "25           0.801180         0.812647        0.007274               23  \n",
       "26           0.802423         0.812957        0.006763               22  \n",
       "27           0.801802         0.812460        0.007053               25  \n",
       "28           0.810811         0.816872        0.005528               17  \n",
       "29           0.811432         0.818425        0.005346               12  \n",
       "30           0.811121         0.819356        0.006513               11  \n",
       "31           0.805530         0.817865        0.007375               14  \n",
       "32           0.808015         0.816747        0.006398               18  \n",
       "33           0.804908         0.816747        0.007419               19  \n",
       "34           0.806462         0.817616        0.006913               15  \n",
       "35           0.810500         0.813579        0.005167               20  \n",
       "36           0.813607         0.818300        0.003096               13  \n",
       "37           0.810500         0.819605        0.005777                9  \n",
       "38           0.812364         0.819667        0.004793                7  \n",
       "39           0.811432         0.820412        0.005468                1  \n",
       "40           0.808947         0.819977        0.007717                3  \n",
       "41           0.810500         0.820288        0.006435                2  \n",
       "42           0.801491         0.813144        0.007009               21  \n",
       "43           0.807704         0.817244        0.006053               16  \n",
       "44           0.812675         0.819418        0.005989               10  \n",
       "45           0.814228         0.819729        0.005154                5  \n",
       "46           0.810500         0.819915        0.006508                4  \n",
       "47           0.805840         0.819666        0.008023                8  \n",
       "48           0.806772         0.819729        0.007276                6  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gscv_rf.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6741210088209716\n",
      "0.6634364517331346\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.68      0.63      0.65      2715\n",
      "           1       0.65      0.70      0.67      2651\n",
      "\n",
      "    accuracy                           0.66      5366\n",
      "   macro avg       0.66      0.66      0.66      5366\n",
      "weighted avg       0.66      0.66      0.66      5366\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1707, 1008],\n",
       "       [ 798, 1853]], dtype=int64)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# xgboost\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "xgb_clf = XGBClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=0\n",
    ")\n",
    "\n",
    "xgb_clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred_train = xgb_clf.predict(X_train)\n",
    "y_pred_test = xgb_clf.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_train, y_pred_train))\n",
    "print(accuracy_score(y_test, y_pred_test))\n",
    "# print(classification_report(y_train, y_pred_train))\n",
    "print(classification_report(y_test, y_pred_test))\n",
    "confusion_matrix(y_test, y_pred_test, labels=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.05, max_bin=256, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=20, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=500,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, ...)\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBModel\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "xgb_clf = XGBClassifier(random_state=0)\n",
    "params = {\n",
    "    \"max_depth\": [3,5,7,10,15,20,25],\n",
    "    \"n_estimators\": [100, 300, 500],\n",
    "    'learning_rate' : [0.01,0.05,0.1]\n",
    "}\n",
    "gscv_xg = GridSearchCV (xgb_clf, params, scoring ='accuracy', cv = skf)\n",
    "gscv_xg.fit(X_train, y_train)\n",
    "print(gscv_xg.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.05, max_bin=256, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=23, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=700,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, ...)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "xgb_clf = XGBClassifier(random_state=0)\n",
    "params = {\n",
    "    \"max_depth\": [18,19,20,21,22,23],\n",
    "    \"n_estimators\": [400,450, 500,550,600,700],\n",
    "    'learning_rate' : [0.05]\n",
    "}\n",
    "gscv_xg = GridSearchCV (xgb_clf, params, scoring ='accuracy', cv = skf)\n",
    "gscv_xg.fit(X_train, y_train)\n",
    "print(gscv_xg.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
      "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=1,\n",
      "              early_stopping_rounds=None, enable_categorical=False,\n",
      "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
      "              importance_type=None, interaction_constraints='',\n",
      "              learning_rate=0.05, max_bin=256, max_cat_to_onehot=4,\n",
      "              max_delta_step=0, max_depth=23, max_leaves=0, min_child_weight=1,\n",
      "              missing=nan, monotone_constraints='()', n_estimators=800,\n",
      "              n_jobs=0, num_parallel_tree=1, predictor='auto', random_state=0,\n",
      "              reg_alpha=0, reg_lambda=1, ...)\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5)\n",
    "xgb_clf = XGBClassifier(random_state=0)\n",
    "params = {\n",
    "    \"max_depth\": [23,24,25],\n",
    "    \"n_estimators\": [650,700,800,1000],\n",
    "    'learning_rate' : [0.05]\n",
    "}\n",
    "gscv_xg = GridSearchCV (xgb_clf, params, scoring ='accuracy', cv = skf)\n",
    "gscv_xg.fit(X_train, y_train)\n",
    "print(gscv_xg.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_max_depth</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.180436</td>\n",
       "      <td>0.006167</td>\n",
       "      <td>0.005074</td>\n",
       "      <td>0.000435</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 3, 'n_est...</td>\n",
       "      <td>0.630435</td>\n",
       "      <td>0.631677</td>\n",
       "      <td>0.622981</td>\n",
       "      <td>0.624107</td>\n",
       "      <td>0.622554</td>\n",
       "      <td>0.626351</td>\n",
       "      <td>0.003895</td>\n",
       "      <td>63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.518502</td>\n",
       "      <td>0.022714</td>\n",
       "      <td>0.005011</td>\n",
       "      <td>0.000315</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>300</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 3, 'n_est...</td>\n",
       "      <td>0.655901</td>\n",
       "      <td>0.644099</td>\n",
       "      <td>0.641925</td>\n",
       "      <td>0.644921</td>\n",
       "      <td>0.633737</td>\n",
       "      <td>0.644117</td>\n",
       "      <td>0.007100</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.828115</td>\n",
       "      <td>0.065859</td>\n",
       "      <td>0.005504</td>\n",
       "      <td>0.000552</td>\n",
       "      <td>0.01</td>\n",
       "      <td>3</td>\n",
       "      <td>500</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 3, 'n_est...</td>\n",
       "      <td>0.660870</td>\n",
       "      <td>0.653727</td>\n",
       "      <td>0.645963</td>\n",
       "      <td>0.652066</td>\n",
       "      <td>0.645853</td>\n",
       "      <td>0.651696</td>\n",
       "      <td>0.005575</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.283682</td>\n",
       "      <td>0.020203</td>\n",
       "      <td>0.005204</td>\n",
       "      <td>0.000675</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 5, 'n_est...</td>\n",
       "      <td>0.655901</td>\n",
       "      <td>0.631988</td>\n",
       "      <td>0.650932</td>\n",
       "      <td>0.641504</td>\n",
       "      <td>0.637154</td>\n",
       "      <td>0.643496</td>\n",
       "      <td>0.008784</td>\n",
       "      <td>62</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.782597</td>\n",
       "      <td>0.035522</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>0.000585</td>\n",
       "      <td>0.01</td>\n",
       "      <td>5</td>\n",
       "      <td>300</td>\n",
       "      <td>{'learning_rate': 0.01, 'max_depth': 5, 'n_est...</td>\n",
       "      <td>0.666149</td>\n",
       "      <td>0.659006</td>\n",
       "      <td>0.663665</td>\n",
       "      <td>0.667599</td>\n",
       "      <td>0.659211</td>\n",
       "      <td>0.663126</td>\n",
       "      <td>0.003514</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>3.673918</td>\n",
       "      <td>0.069603</td>\n",
       "      <td>0.016322</td>\n",
       "      <td>0.002087</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>300</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 20, 'n_est...</td>\n",
       "      <td>0.806522</td>\n",
       "      <td>0.806832</td>\n",
       "      <td>0.805590</td>\n",
       "      <td>0.807083</td>\n",
       "      <td>0.797142</td>\n",
       "      <td>0.804634</td>\n",
       "      <td>0.003780</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>5.443889</td>\n",
       "      <td>0.077368</td>\n",
       "      <td>0.024132</td>\n",
       "      <td>0.002773</td>\n",
       "      <td>0.1</td>\n",
       "      <td>20</td>\n",
       "      <td>500</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 20, 'n_est...</td>\n",
       "      <td>0.807143</td>\n",
       "      <td>0.807453</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.804598</td>\n",
       "      <td>0.797453</td>\n",
       "      <td>0.804199</td>\n",
       "      <td>0.003604</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1.964846</td>\n",
       "      <td>0.148565</td>\n",
       "      <td>0.008242</td>\n",
       "      <td>0.001166</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25</td>\n",
       "      <td>100</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 25, 'n_est...</td>\n",
       "      <td>0.808696</td>\n",
       "      <td>0.804658</td>\n",
       "      <td>0.803106</td>\n",
       "      <td>0.800870</td>\n",
       "      <td>0.791550</td>\n",
       "      <td>0.801776</td>\n",
       "      <td>0.005716</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>4.189157</td>\n",
       "      <td>0.066287</td>\n",
       "      <td>0.018003</td>\n",
       "      <td>0.002367</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25</td>\n",
       "      <td>300</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 25, 'n_est...</td>\n",
       "      <td>0.806832</td>\n",
       "      <td>0.804037</td>\n",
       "      <td>0.801863</td>\n",
       "      <td>0.805530</td>\n",
       "      <td>0.799006</td>\n",
       "      <td>0.803454</td>\n",
       "      <td>0.002770</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>6.141532</td>\n",
       "      <td>0.097087</td>\n",
       "      <td>0.023187</td>\n",
       "      <td>0.002394</td>\n",
       "      <td>0.1</td>\n",
       "      <td>25</td>\n",
       "      <td>500</td>\n",
       "      <td>{'learning_rate': 0.1, 'max_depth': 25, 'n_est...</td>\n",
       "      <td>0.807453</td>\n",
       "      <td>0.802795</td>\n",
       "      <td>0.801242</td>\n",
       "      <td>0.806151</td>\n",
       "      <td>0.793725</td>\n",
       "      <td>0.802273</td>\n",
       "      <td>0.004823</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>63 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.180436      0.006167         0.005074        0.000435   \n",
       "1        0.518502      0.022714         0.005011        0.000315   \n",
       "2        0.828115      0.065859         0.005504        0.000552   \n",
       "3        0.283682      0.020203         0.005204        0.000675   \n",
       "4        0.782597      0.035522         0.005908        0.000585   \n",
       "..            ...           ...              ...             ...   \n",
       "58       3.673918      0.069603         0.016322        0.002087   \n",
       "59       5.443889      0.077368         0.024132        0.002773   \n",
       "60       1.964846      0.148565         0.008242        0.001166   \n",
       "61       4.189157      0.066287         0.018003        0.002367   \n",
       "62       6.141532      0.097087         0.023187        0.002394   \n",
       "\n",
       "   param_learning_rate param_max_depth param_n_estimators  \\\n",
       "0                 0.01               3                100   \n",
       "1                 0.01               3                300   \n",
       "2                 0.01               3                500   \n",
       "3                 0.01               5                100   \n",
       "4                 0.01               5                300   \n",
       "..                 ...             ...                ...   \n",
       "58                 0.1              20                300   \n",
       "59                 0.1              20                500   \n",
       "60                 0.1              25                100   \n",
       "61                 0.1              25                300   \n",
       "62                 0.1              25                500   \n",
       "\n",
       "                                               params  split0_test_score  \\\n",
       "0   {'learning_rate': 0.01, 'max_depth': 3, 'n_est...           0.630435   \n",
       "1   {'learning_rate': 0.01, 'max_depth': 3, 'n_est...           0.655901   \n",
       "2   {'learning_rate': 0.01, 'max_depth': 3, 'n_est...           0.660870   \n",
       "3   {'learning_rate': 0.01, 'max_depth': 5, 'n_est...           0.655901   \n",
       "4   {'learning_rate': 0.01, 'max_depth': 5, 'n_est...           0.666149   \n",
       "..                                                ...                ...   \n",
       "58  {'learning_rate': 0.1, 'max_depth': 20, 'n_est...           0.806522   \n",
       "59  {'learning_rate': 0.1, 'max_depth': 20, 'n_est...           0.807143   \n",
       "60  {'learning_rate': 0.1, 'max_depth': 25, 'n_est...           0.808696   \n",
       "61  {'learning_rate': 0.1, 'max_depth': 25, 'n_est...           0.806832   \n",
       "62  {'learning_rate': 0.1, 'max_depth': 25, 'n_est...           0.807453   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.631677           0.622981           0.624107   \n",
       "1            0.644099           0.641925           0.644921   \n",
       "2            0.653727           0.645963           0.652066   \n",
       "3            0.631988           0.650932           0.641504   \n",
       "4            0.659006           0.663665           0.667599   \n",
       "..                ...                ...                ...   \n",
       "58           0.806832           0.805590           0.807083   \n",
       "59           0.807453           0.804348           0.804598   \n",
       "60           0.804658           0.803106           0.800870   \n",
       "61           0.804037           0.801863           0.805530   \n",
       "62           0.802795           0.801242           0.806151   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.622554         0.626351        0.003895               63  \n",
       "1            0.633737         0.644117        0.007100               61  \n",
       "2            0.645853         0.651696        0.005575               59  \n",
       "3            0.637154         0.643496        0.008784               62  \n",
       "4            0.659211         0.663126        0.003514               54  \n",
       "..                ...              ...             ...              ...  \n",
       "58           0.797142         0.804634        0.003780                4  \n",
       "59           0.797453         0.804199        0.003604                6  \n",
       "60           0.791550         0.801776        0.005716               14  \n",
       "61           0.799006         0.803454        0.002770                8  \n",
       "62           0.793725         0.802273        0.004823               12  \n",
       "\n",
       "[63 rows x 16 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(gscv_xg.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000814 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[996]\tvalid_0's binary_logloss: 0.471651\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002468 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[996]\tvalid_0's binary_logloss: 0.466149\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000766 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[979]\tvalid_0's binary_logloss: 0.455821\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000844 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[994]\tvalid_0's binary_logloss: 0.467889\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000759 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[999]\tvalid_0's binary_logloss: 0.46422\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000921 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1140]\tvalid_0's binary_logloss: 0.469779\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000808 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1306]\tvalid_0's binary_logloss: 0.460941\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000842 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1283]\tvalid_0's binary_logloss: 0.452066\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000752 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1054]\tvalid_0's binary_logloss: 0.467248\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000770 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1281]\tvalid_0's binary_logloss: 0.461524\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000905 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1140]\tvalid_0's binary_logloss: 0.469779\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002133 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1306]\tvalid_0's binary_logloss: 0.460941\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000907 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1283]\tvalid_0's binary_logloss: 0.452066\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000665 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1054]\tvalid_0's binary_logloss: 0.467248\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000813 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[1281]\tvalid_0's binary_logloss: 0.461524\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000738 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[731]\tvalid_0's binary_logloss: 0.46689\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000774 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[801]\tvalid_0's binary_logloss: 0.466443\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000925 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[757]\tvalid_0's binary_logloss: 0.460734\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000674 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[739]\tvalid_0's binary_logloss: 0.469217\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000772 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[774]\tvalid_0's binary_logloss: 0.468469\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000821 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[731]\tvalid_0's binary_logloss: 0.46689\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000864 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[801]\tvalid_0's binary_logloss: 0.466443\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001075 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[757]\tvalid_0's binary_logloss: 0.460734\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001141 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[739]\tvalid_0's binary_logloss: 0.469217\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[774]\tvalid_0's binary_logloss: 0.468469\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000809 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[731]\tvalid_0's binary_logloss: 0.46689\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000935 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[801]\tvalid_0's binary_logloss: 0.466443\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000795 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[757]\tvalid_0's binary_logloss: 0.460734\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000721 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[739]\tvalid_0's binary_logloss: 0.469217\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[774]\tvalid_0's binary_logloss: 0.468469\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000819 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[626]\tvalid_0's binary_logloss: 0.473792\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001147 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[573]\tvalid_0's binary_logloss: 0.474005\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001057 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[662]\tvalid_0's binary_logloss: 0.465093\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000799 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[576]\tvalid_0's binary_logloss: 0.473279\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000759 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[557]\tvalid_0's binary_logloss: 0.47704\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000691 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[626]\tvalid_0's binary_logloss: 0.473792\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000873 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[573]\tvalid_0's binary_logloss: 0.474005\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000863 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[662]\tvalid_0's binary_logloss: 0.465093\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000845 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[576]\tvalid_0's binary_logloss: 0.473279\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001527 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[557]\tvalid_0's binary_logloss: 0.47704\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000780 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[626]\tvalid_0's binary_logloss: 0.473792\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000831 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[573]\tvalid_0's binary_logloss: 0.474005\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000933 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[662]\tvalid_0's binary_logloss: 0.465093\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000970 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[576]\tvalid_0's binary_logloss: 0.473279\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000766 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[557]\tvalid_0's binary_logloss: 0.47704\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000843 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[186]\tvalid_0's binary_logloss: 0.522903\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001310 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[254]\tvalid_0's binary_logloss: 0.523243\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000662 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[204]\tvalid_0's binary_logloss: 0.519538\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000775 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[137]\tvalid_0's binary_logloss: 0.533446\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000831 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[237]\tvalid_0's binary_logloss: 0.525805\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000851 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[186]\tvalid_0's binary_logloss: 0.522903\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002700 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[254]\tvalid_0's binary_logloss: 0.523243\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000887 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[204]\tvalid_0's binary_logloss: 0.519538\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000805 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[137]\tvalid_0's binary_logloss: 0.533446\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000790 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[237]\tvalid_0's binary_logloss: 0.525805\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000794 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 841\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[186]\tvalid_0's binary_logloss: 0.522903\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6413\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000797 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.502019 -> initscore=0.008076\n",
      "[LightGBM] [Info] Start training from score 0.008076\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[254]\tvalid_0's binary_logloss: 0.523243\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6464, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000792 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 839\n",
      "[LightGBM] [Info] Number of data points in the train set: 12878, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501941 -> initscore=0.007765\n",
      "[LightGBM] [Info] Start training from score 0.007765\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[204]\tvalid_0's binary_logloss: 0.519538\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001917 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 845\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[137]\tvalid_0's binary_logloss: 0.533446\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 6465, number of negative: 6414\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002365 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 836\n",
      "[LightGBM] [Info] Number of data points in the train set: 12879, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501980 -> initscore=0.007920\n",
      "[LightGBM] [Info] Start training from score 0.007920\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Early stopping, best iteration is:\n",
      "[237]\tvalid_0's binary_logloss: 0.525805\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] Number of positive: 8081, number of negative: 8017\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000965 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 867\n",
      "[LightGBM] [Info] Number of data points in the train set: 16098, number of used features: 35\n",
      "[LightGBM] [Warning] early_stopping_round is set=100, early_stopping_rounds=100 will be ignored. Current value: early_stopping_round=100\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.501988 -> initscore=0.007951\n",
      "[LightGBM] [Info] Start training from score 0.007951\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "Did not meet early stopping. Best iteration is:\n",
      "[1499]\tvalid_0's binary_logloss: 0.405889\n",
      "LGBMClassifier(early_stopping_rounds=100, n_estimators=1500, random_state=0)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_learning_rate</th>\n",
       "      <th>param_n_estimators</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>split3_test_score</th>\n",
       "      <th>split4_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.765491</td>\n",
       "      <td>0.021304</td>\n",
       "      <td>0.031073</td>\n",
       "      <td>0.001533</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 1000}</td>\n",
       "      <td>0.786646</td>\n",
       "      <td>0.786025</td>\n",
       "      <td>0.790373</td>\n",
       "      <td>0.792793</td>\n",
       "      <td>0.787512</td>\n",
       "      <td>0.788670</td>\n",
       "      <td>0.002543</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.991288</td>\n",
       "      <td>0.086752</td>\n",
       "      <td>0.039980</td>\n",
       "      <td>0.005023</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1500</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 1500}</td>\n",
       "      <td>0.787888</td>\n",
       "      <td>0.789130</td>\n",
       "      <td>0.791615</td>\n",
       "      <td>0.789376</td>\n",
       "      <td>0.792171</td>\n",
       "      <td>0.790036</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.989509</td>\n",
       "      <td>0.081493</td>\n",
       "      <td>0.039817</td>\n",
       "      <td>0.004039</td>\n",
       "      <td>0.1</td>\n",
       "      <td>2000</td>\n",
       "      <td>{'learning_rate': 0.1, 'n_estimators': 2000}</td>\n",
       "      <td>0.787888</td>\n",
       "      <td>0.789130</td>\n",
       "      <td>0.791615</td>\n",
       "      <td>0.789376</td>\n",
       "      <td>0.792171</td>\n",
       "      <td>0.790036</td>\n",
       "      <td>0.001608</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.630683</td>\n",
       "      <td>0.019075</td>\n",
       "      <td>0.022881</td>\n",
       "      <td>0.001632</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'learning_rate': 0.15, 'n_estimators': 1000}</td>\n",
       "      <td>0.786957</td>\n",
       "      <td>0.789441</td>\n",
       "      <td>0.792547</td>\n",
       "      <td>0.794967</td>\n",
       "      <td>0.783162</td>\n",
       "      <td>0.789415</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.646376</td>\n",
       "      <td>0.018817</td>\n",
       "      <td>0.023720</td>\n",
       "      <td>0.002500</td>\n",
       "      <td>0.15</td>\n",
       "      <td>1500</td>\n",
       "      <td>{'learning_rate': 0.15, 'n_estimators': 1500}</td>\n",
       "      <td>0.786957</td>\n",
       "      <td>0.789441</td>\n",
       "      <td>0.792547</td>\n",
       "      <td>0.794967</td>\n",
       "      <td>0.783162</td>\n",
       "      <td>0.789415</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.685742</td>\n",
       "      <td>0.052163</td>\n",
       "      <td>0.024275</td>\n",
       "      <td>0.001837</td>\n",
       "      <td>0.15</td>\n",
       "      <td>2000</td>\n",
       "      <td>{'learning_rate': 0.15, 'n_estimators': 2000}</td>\n",
       "      <td>0.786957</td>\n",
       "      <td>0.789441</td>\n",
       "      <td>0.792547</td>\n",
       "      <td>0.794967</td>\n",
       "      <td>0.783162</td>\n",
       "      <td>0.789415</td>\n",
       "      <td>0.004142</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.532945</td>\n",
       "      <td>0.027392</td>\n",
       "      <td>0.019730</td>\n",
       "      <td>0.003197</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'learning_rate': 0.2, 'n_estimators': 1000}</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.783851</td>\n",
       "      <td>0.788820</td>\n",
       "      <td>0.790308</td>\n",
       "      <td>0.775707</td>\n",
       "      <td>0.784880</td>\n",
       "      <td>0.005116</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.529747</td>\n",
       "      <td>0.029764</td>\n",
       "      <td>0.017322</td>\n",
       "      <td>0.001141</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1500</td>\n",
       "      <td>{'learning_rate': 0.2, 'n_estimators': 1500}</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.783851</td>\n",
       "      <td>0.788820</td>\n",
       "      <td>0.790308</td>\n",
       "      <td>0.775707</td>\n",
       "      <td>0.784880</td>\n",
       "      <td>0.005116</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.524308</td>\n",
       "      <td>0.022411</td>\n",
       "      <td>0.020594</td>\n",
       "      <td>0.005851</td>\n",
       "      <td>0.2</td>\n",
       "      <td>2000</td>\n",
       "      <td>{'learning_rate': 0.2, 'n_estimators': 2000}</td>\n",
       "      <td>0.785714</td>\n",
       "      <td>0.783851</td>\n",
       "      <td>0.788820</td>\n",
       "      <td>0.790308</td>\n",
       "      <td>0.775707</td>\n",
       "      <td>0.784880</td>\n",
       "      <td>0.005116</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.236598</td>\n",
       "      <td>0.029743</td>\n",
       "      <td>0.007111</td>\n",
       "      <td>0.001011</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1000</td>\n",
       "      <td>{'learning_rate': 0.5, 'n_estimators': 1000}</td>\n",
       "      <td>0.779503</td>\n",
       "      <td>0.783851</td>\n",
       "      <td>0.771118</td>\n",
       "      <td>0.752097</td>\n",
       "      <td>0.767940</td>\n",
       "      <td>0.770902</td>\n",
       "      <td>0.010992</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.239081</td>\n",
       "      <td>0.027058</td>\n",
       "      <td>0.006714</td>\n",
       "      <td>0.001962</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1500</td>\n",
       "      <td>{'learning_rate': 0.5, 'n_estimators': 1500}</td>\n",
       "      <td>0.779503</td>\n",
       "      <td>0.783851</td>\n",
       "      <td>0.771118</td>\n",
       "      <td>0.752097</td>\n",
       "      <td>0.767940</td>\n",
       "      <td>0.770902</td>\n",
       "      <td>0.010992</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.245046</td>\n",
       "      <td>0.027113</td>\n",
       "      <td>0.007468</td>\n",
       "      <td>0.001694</td>\n",
       "      <td>0.5</td>\n",
       "      <td>2000</td>\n",
       "      <td>{'learning_rate': 0.5, 'n_estimators': 2000}</td>\n",
       "      <td>0.779503</td>\n",
       "      <td>0.783851</td>\n",
       "      <td>0.771118</td>\n",
       "      <td>0.752097</td>\n",
       "      <td>0.767940</td>\n",
       "      <td>0.770902</td>\n",
       "      <td>0.010992</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0        0.765491      0.021304         0.031073        0.001533   \n",
       "1        0.991288      0.086752         0.039980        0.005023   \n",
       "2        0.989509      0.081493         0.039817        0.004039   \n",
       "3        0.630683      0.019075         0.022881        0.001632   \n",
       "4        0.646376      0.018817         0.023720        0.002500   \n",
       "5        0.685742      0.052163         0.024275        0.001837   \n",
       "6        0.532945      0.027392         0.019730        0.003197   \n",
       "7        0.529747      0.029764         0.017322        0.001141   \n",
       "8        0.524308      0.022411         0.020594        0.005851   \n",
       "9        0.236598      0.029743         0.007111        0.001011   \n",
       "10       0.239081      0.027058         0.006714        0.001962   \n",
       "11       0.245046      0.027113         0.007468        0.001694   \n",
       "\n",
       "   param_learning_rate param_n_estimators  \\\n",
       "0                  0.1               1000   \n",
       "1                  0.1               1500   \n",
       "2                  0.1               2000   \n",
       "3                 0.15               1000   \n",
       "4                 0.15               1500   \n",
       "5                 0.15               2000   \n",
       "6                  0.2               1000   \n",
       "7                  0.2               1500   \n",
       "8                  0.2               2000   \n",
       "9                  0.5               1000   \n",
       "10                 0.5               1500   \n",
       "11                 0.5               2000   \n",
       "\n",
       "                                           params  split0_test_score  \\\n",
       "0    {'learning_rate': 0.1, 'n_estimators': 1000}           0.786646   \n",
       "1    {'learning_rate': 0.1, 'n_estimators': 1500}           0.787888   \n",
       "2    {'learning_rate': 0.1, 'n_estimators': 2000}           0.787888   \n",
       "3   {'learning_rate': 0.15, 'n_estimators': 1000}           0.786957   \n",
       "4   {'learning_rate': 0.15, 'n_estimators': 1500}           0.786957   \n",
       "5   {'learning_rate': 0.15, 'n_estimators': 2000}           0.786957   \n",
       "6    {'learning_rate': 0.2, 'n_estimators': 1000}           0.785714   \n",
       "7    {'learning_rate': 0.2, 'n_estimators': 1500}           0.785714   \n",
       "8    {'learning_rate': 0.2, 'n_estimators': 2000}           0.785714   \n",
       "9    {'learning_rate': 0.5, 'n_estimators': 1000}           0.779503   \n",
       "10   {'learning_rate': 0.5, 'n_estimators': 1500}           0.779503   \n",
       "11   {'learning_rate': 0.5, 'n_estimators': 2000}           0.779503   \n",
       "\n",
       "    split1_test_score  split2_test_score  split3_test_score  \\\n",
       "0            0.786025           0.790373           0.792793   \n",
       "1            0.789130           0.791615           0.789376   \n",
       "2            0.789130           0.791615           0.789376   \n",
       "3            0.789441           0.792547           0.794967   \n",
       "4            0.789441           0.792547           0.794967   \n",
       "5            0.789441           0.792547           0.794967   \n",
       "6            0.783851           0.788820           0.790308   \n",
       "7            0.783851           0.788820           0.790308   \n",
       "8            0.783851           0.788820           0.790308   \n",
       "9            0.783851           0.771118           0.752097   \n",
       "10           0.783851           0.771118           0.752097   \n",
       "11           0.783851           0.771118           0.752097   \n",
       "\n",
       "    split4_test_score  mean_test_score  std_test_score  rank_test_score  \n",
       "0            0.787512         0.788670        0.002543                6  \n",
       "1            0.792171         0.790036        0.001608                1  \n",
       "2            0.792171         0.790036        0.001608                1  \n",
       "3            0.783162         0.789415        0.004142                3  \n",
       "4            0.783162         0.789415        0.004142                3  \n",
       "5            0.783162         0.789415        0.004142                3  \n",
       "6            0.775707         0.784880        0.005116                7  \n",
       "7            0.775707         0.784880        0.005116                7  \n",
       "8            0.775707         0.784880        0.005116                7  \n",
       "9            0.767940         0.770902        0.010992               10  \n",
       "10           0.767940         0.770902        0.010992               10  \n",
       "11           0.767940         0.770902        0.010992               10  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LightGBM\n",
    "from lightgbm import LGBMClassifier\n",
    "skf = StratifiedKFold(n_splits=5)\n",
    "lgbm_clf = LGBMClassifier(random_state=0, early_stopping_rounds=100)\n",
    "params = {\n",
    "    \"n_estimators\": [1000,1500,2000],\n",
    "    'learning_rate' : [0.1,0.15,0.2,0.5]\n",
    "}\n",
    "gscv_lgbm = GridSearchCV (lgbm_clf, params, scoring ='accuracy', cv = skf)\n",
    "gscv_lgbm.fit(X_train, y_train, eval_set=(X_test, y_test))\n",
    "print(gscv_lgbm.best_estimator_)\n",
    "pd.DataFrame(gscv_lgbm.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LGBMClassifier(early_stopping_rounds=100, n_estimators=1500, random_state=0)\n"
     ]
    }
   ],
   "source": [
    "print(gscv_lgbm.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 8586, number of negative: 8585\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002269 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4431\n",
      "[LightGBM] [Info] Number of data points in the train set: 17171, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500029 -> initscore=0.000116\n",
      "[LightGBM] [Info] Start training from score 0.000116\n",
      "[LightGBM] [Info] Number of positive: 8586, number of negative: 8585\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002336 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4412\n",
      "[LightGBM] [Info] Number of data points in the train set: 17171, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500029 -> initscore=0.000116\n",
      "[LightGBM] [Info] Start training from score 0.000116\n",
      "[LightGBM] [Info] Number of positive: 8585, number of negative: 8586\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001978 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4199\n",
      "[LightGBM] [Info] Number of data points in the train set: 17171, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499971 -> initscore=-0.000116\n",
      "[LightGBM] [Info] Start training from score -0.000116\n",
      "[LightGBM] [Info] Number of positive: 8585, number of negative: 8586\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002023 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4166\n",
      "[LightGBM] [Info] Number of data points in the train set: 17171, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.499971 -> initscore=-0.000116\n",
      "[LightGBM] [Info] Start training from score -0.000116\n",
      "[LightGBM] [Info] Number of positive: 8586, number of negative: 8586\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.003462 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4336\n",
      "[LightGBM] [Info] Number of data points in the train set: 17172, number of used features: 35\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.500000 -> initscore=0.000000\n",
      "의사결정나무 정확도: 0.6995951709679751\n",
      "의사결정나무 정밀도: 0.7512877969279211\n",
      "의사결정나무 재현율: 0.6994063543009145\n",
      "랜덤포레스트 정확도: 0.7757249550569872\n",
      "랜덤포레스트 정밀도: 0.7861506110046139\n",
      "랜덤포레스트 재현율: 0.88166643588162\n",
      "xgboost 정확도: 0.756156308119006\n",
      "xgboost 정밀도: 0.7844645569174908\n",
      "xgboost 재현율: 0.8393627120527526\n",
      "lightGBM 정확도: 0.7478630766963016\n",
      "lightGBM 정밀도: 0.7819028697159789\n",
      "lightGBM 재현율: 0.8310703810470927\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "# 최종 모델 비교\n",
    "dt_clf = DecisionTreeClassifier(max_depth=25, random_state=0)\n",
    "rf_clf = RandomForestClassifier(max_depth=34, n_estimators=550, random_state=0)\n",
    "xgb_clf = XGBClassifier(learning_rate=0.05, max_depth=23, n_estimators=800, random_state=0)\n",
    "lgbm_clf = LGBMClassifier(n_estimators=1500, learning_rate=0.1, random_state=0)\n",
    "\n",
    "stratified_kfold = StratifiedKFold(n_splits=5)\n",
    "\n",
    "# k번 반복하면서 평가한 정확도를 저장할 배열\n",
    "dt_accuracy = []\n",
    "dt_precision = []\n",
    "dt_recall = []\n",
    "rf_accuracy = []\n",
    "rf_precision = []\n",
    "rf_recall = []\n",
    "xgb_accuracy = []\n",
    "xgb_precision = []\n",
    "xgb_recall = []\n",
    "lgbm_accuracy = []\n",
    "lgbm_precision = []\n",
    "lgbm_recall = []\n",
    "\n",
    "for train_index, val_index in stratified_kfold.split(X_new, y_new):\n",
    "    X_train, y_train = X_new.iloc[train_index], y_new.iloc[train_index]\n",
    "    X_val, y_val = X_new.iloc[val_index], y_new.iloc[val_index]\n",
    "\n",
    "    dt_clf.fit(X_train, y_train)\n",
    "    rf_clf.fit(X_train, y_train)\n",
    "    xgb_clf.fit(X_train, y_train)\n",
    "    lgbm_clf.fit(X_train, y_train)\n",
    "\n",
    "    y_pred_dt = dt_clf.predict(X_val)   # 검증 데이터로 예측\n",
    "    y_pred_rf = rf_clf.predict(X_val)   # 검증 데이터로 예측\n",
    "    y_pred_xgb = xgb_clf.predict(X_val)   # 검증 데이터로 예측\n",
    "    y_pred_lgbm = lgbm_clf.predict(X_val)   # 검증 데이터로 예측\n",
    "\n",
    "    dt_accuracy.append(accuracy_score(y_val, y_pred_dt)) \n",
    "    rf_accuracy.append(accuracy_score(y_val, y_pred_rf)) \n",
    "    xgb_accuracy.append(accuracy_score(y_val, y_pred_xgb)) \n",
    "    lgbm_accuracy.append(accuracy_score(y_val, y_pred_lgbm)) \n",
    "    \n",
    "    dt_precision.append(precision_score(y_val, y_pred_dt)) \n",
    "    rf_precision.append(precision_score(y_val, y_pred_rf)) \n",
    "    xgb_precision.append(precision_score(y_val, y_pred_xgb)) \n",
    "    lgbm_precision.append(precision_score(y_val, y_pred_lgbm)) \n",
    "    \n",
    "    dt_recall.append(recall_score(y_val, y_pred_dt)) \n",
    "    rf_recall.append(recall_score(y_val, y_pred_rf)) \n",
    "    xgb_recall.append(recall_score(y_val, y_pred_xgb)) \n",
    "    lgbm_recall.append(recall_score(y_val, y_pred_lgbm)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"의사결정나무 정확도:\", np.mean(dt_accuracy))\n",
    "print(\"의사결정나무 정밀도:\", np.mean(dt_precision))\n",
    "print(\"의사결정나무 재현율:\", np.mean(dt_recall))\n",
    "\n",
    "print(\"랜덤포레스트 정확도:\", np.mean(rf_accuracy))\n",
    "print(\"랜덤포레스트 정밀도:\", np.mean(rf_precision))\n",
    "print(\"랜덤포레스트 재현율:\", np.mean(rf_recall))\n",
    "\n",
    "print(\"xgboost 정확도:\", np.mean(xgb_accuracy))\n",
    "print(\"xgboost 정밀도:\", np.mean(xgb_precision))\n",
    "print(\"xgboost 재현율:\", np.mean(xgb_recall))\n",
    "\n",
    "print(\"lightGBM 정확도:\", np.mean(lgbm_accuracy))\n",
    "print(\"lightGBM 정밀도:\", np.mean(lgbm_precision))\n",
    "print(\"lightGBM 재현율:\", np.mean(lgbm_recall))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "랜덤포레스트 최종모델로 선정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 모델 및 정확도 데이터\n",
    "models = ['DecisionTree', 'RandomForest', 'XGBoost']\n",
    "train_acc = [0.7016, 1.0000, 0.7375]  # Train Accuracy\n",
    "test_acc = [0.6918, 0.7208, 0.7275]   # Test Accuracy\n",
    "\n",
    "# x축 위치 설정\n",
    "x = np.arange(len(models))\n",
    "width = 0.3  # 막대 너비\n",
    "\n",
    "# 그래프 크기 설정\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# 바 그래프 추가\n",
    "plt.bar(x - width/2, train_acc, width, label='Train Accuracy', color='skyblue')\n",
    "plt.bar(x + width/2, test_acc, width, label='Test Accuracy', color='salmon')\n",
    "\n",
    "# 그래프 제목 및 라벨\n",
    "plt.title('Train vs Test Accuracy for Models')\n",
    "plt.xlabel('Models')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xticks(x, models)  # x축 눈금 설정\n",
    "plt.ylim(0, 1.05)  # y축 범위 설정\n",
    "plt.legend()\n",
    "\n",
    "# 그래프 출력\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystudy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
